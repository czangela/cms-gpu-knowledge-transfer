{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CMS Tracker DPG Knowledge Transfer Material","title":"Home"},{"location":"#cms-tracker-dpg-knowledge-transfer-material","text":"","title":"CMS Tracker DPG Knowledge Transfer Material"},{"location":"getting-started/","text":"Access to machines A step-by-step guide on how to access GPU equipped machines at CERN, CMS or how to develop on your machine. Prerequisites CERN computing account Access machines at CERN See the CERN cloud insfrastructure resources guide on how to request GPU resources. lxplus The lxplus service offers lxplus-gpu.cern.ch for shared GPU instances - with limited isolation and performance. One can connect similary as would do to the lxplus.cern.ch host domain. ssh <username>@lxplus-gpu.cern.ch [-X] Access machines at CMS P5 This section is taken from the CMS TWiki TriggerDevelopmentWithGPUs page. Dedicated machines for the development of the online reconstruction There are 6 machines available for general development and validation of the online reconstruction on GPUs: gpu-c2a02-37-03.cms gpu-c2a02-37-04.cms gpu-c2a02-39-01.cms gpu-c2a02-39-02.cms gpu-c2a02-39-03.cms gpu-c2a02-39-04.cms All machines are equipped with two Intel \"Skylake\" Xeon Gold 6130 processors (for a total of 2x16=32 physical cores and 2x2x16 = 64 logical cores or hardware threads); 96 GB of RAM; one NVIDIA Tesla T4 GPU. How to connect To connect to these machines you need to have an online account and be in the gpudev group. To request access, please subscribe to the cms-hlt-gpu@cern.ch e-group and send an email to andrea.bocci@cern.ch , indicating whether you already have an online account; your online or lxplus username; your full name and email. Miscellaneous - or special GPU nodes This section is more or less taken from the Patatrack website systems subpage. cmg-gpu1080 System information Topology of the machine Getting access to the machine In order to get access to the machine you should send a request to subscribe to the CERN e-group: cms-gpu You should also send an email to Felice Pantaleo motivating the reason for the requested access. Usage Policy Normally, no more than 1 GPU per users should be used. To limit visible devices use export CUDA_VISIBLE_DEVICES=<list of numbers> Where <list of numbers> can be e.g. 0 , 0,4 , 1,2,3 . Use nvidia-smi to check available resources. Usage for ML studies If you need to use the machine for training DNNs you could accidentally occupy all the GPUs, making them unavailable for other users. For this reason you're kindly asked to use import setGPU before any import that will use a GPU (e.g. tensorflow). This will assign to you the least loaded GPU on the system. It is strictly forbidden to use GPUs from within your jupyter notebook. Please export your notebook to a python program and execute it. The access to the machine will be revoked when failing to comply to this rule.","title":"Access to machines"},{"location":"getting-started/#access-to-machines","text":"A step-by-step guide on how to access GPU equipped machines at CERN, CMS or how to develop on your machine.","title":"Access to machines"},{"location":"getting-started/#prerequisites","text":"CERN computing account","title":"Prerequisites"},{"location":"getting-started/#access-machines-at-cern","text":"See the CERN cloud insfrastructure resources guide on how to request GPU resources. lxplus The lxplus service offers lxplus-gpu.cern.ch for shared GPU instances - with limited isolation and performance. One can connect similary as would do to the lxplus.cern.ch host domain. ssh <username>@lxplus-gpu.cern.ch [-X]","title":"Access machines at CERN"},{"location":"getting-started/#access-machines-at-cms-p5","text":"This section is taken from the CMS TWiki TriggerDevelopmentWithGPUs page.","title":"Access machines at CMS P5"},{"location":"getting-started/#dedicated-machines-for-the-development-of-the-online-reconstruction","text":"There are 6 machines available for general development and validation of the online reconstruction on GPUs: gpu-c2a02-37-03.cms gpu-c2a02-37-04.cms gpu-c2a02-39-01.cms gpu-c2a02-39-02.cms gpu-c2a02-39-03.cms gpu-c2a02-39-04.cms","title":"Dedicated machines for the development of the online reconstruction"},{"location":"getting-started/#all-machines-are-equipped-with","text":"two Intel \"Skylake\" Xeon Gold 6130 processors (for a total of 2x16=32 physical cores and 2x2x16 = 64 logical cores or hardware threads); 96 GB of RAM; one NVIDIA Tesla T4 GPU.","title":"All machines are equipped with"},{"location":"getting-started/#how-to-connect","text":"To connect to these machines you need to have an online account and be in the gpudev group. To request access, please subscribe to the cms-hlt-gpu@cern.ch e-group and send an email to andrea.bocci@cern.ch , indicating whether you already have an online account; your online or lxplus username; your full name and email.","title":"How to connect"},{"location":"getting-started/#miscellaneous-or-special-gpu-nodes","text":"This section is more or less taken from the Patatrack website systems subpage.","title":"Miscellaneous - or special GPU nodes"},{"location":"getting-started/#cmg-gpu1080","text":"","title":"cmg-gpu1080"},{"location":"getting-started/#system-information","text":"Topology of the machine","title":"System information"},{"location":"getting-started/#getting-access-to-the-machine","text":"In order to get access to the machine you should send a request to subscribe to the CERN e-group: cms-gpu You should also send an email to Felice Pantaleo motivating the reason for the requested access.","title":"Getting access to the machine"},{"location":"getting-started/#usage-policy","text":"Normally, no more than 1 GPU per users should be used. To limit visible devices use export CUDA_VISIBLE_DEVICES=<list of numbers> Where <list of numbers> can be e.g. 0 , 0,4 , 1,2,3 . Use nvidia-smi to check available resources.","title":"Usage Policy"},{"location":"getting-started/#usage-for-ml-studies","text":"If you need to use the machine for training DNNs you could accidentally occupy all the GPUs, making them unavailable for other users. For this reason you're kindly asked to use import setGPU before any import that will use a GPU (e.g. tensorflow). This will assign to you the least loaded GPU on the system. It is strictly forbidden to use GPUs from within your jupyter notebook. Please export your notebook to a python program and execute it. The access to the machine will be revoked when failing to comply to this rule.","title":"Usage for ML studies"},{"location":"weeks/examples/","text":"Practice Example Unordered List Ordered List Example : * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Example : 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Practice"},{"location":"weeks/examples/#practice","text":"Example Unordered List Ordered List Example : * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Example : 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Practice"},{"location":"weeks/further_reading/","text":"Further reading 1. Performance measurement Memory bandwidth (sometimes also referred to as memory throughput) Processing throughput To calculate the memory bandwidth we need to understand two concepts related to it. Memory interface width: It is the physical bit-width of the memory bus. Every clock-cycle data is transferred along a memory bus to and from the on-card memory. The memory interface width is the physical count of the bits of how many can fit down the bus per clock cycle. Memory clock rate: (or memory clock cycle) Measured in the unit of Hz it is the rate of memory bus clock. For example a memory bus operating on a 1GHz rate is capable of issuing memory tranfers with the bus 10^9 times per second. Talking about clock rate it is important to take a detour to SDR , DDR and QDR . Example A simple analogy is of course public transport, where memory interface width would be the number of seats on a bus and the memory clock rate is the frequency of buses in unit time (hour let's say). Multiplying these two numbers we can calculate how many people can be transported along one bus line in a certain time period. 2. Parallel Programming models Suggested reading: SIMD < SIMT < SMT: parallelism in NVIDIA GPUs by yosefk Quote NVIDIA call their parallel programming model SIMT - \"Single Instruction, Multiple Threads\". Two other different, but related parallel programming models are SIMD - \"Single Instruction, Multiple Data\", and SMT - \"Simultaneous Multithreading\". Each model exploits a different source of parallelism: In SIMD, elements of short vectors are processed in parallel. In SMT, instructions of several threads are run in parallel. SIMT is somewhere in between \u2013 an interesting hybrid between vector processing and hardware threading. Other resources https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html https://towardsdatascience.com/the-ai-illustrated-guide-why-are-gpus-so-powerful-99f4ae85a5c3 https://streamhpc.com/blog/2016-07-19/performance-can-be-measured-as-throughput-latency-or-processor-utilisation/ https://www.gamersnexus.net/dictionary/3-memory-interface","title":"Further reading"},{"location":"weeks/further_reading/#further-reading","text":"","title":"Further reading"},{"location":"weeks/further_reading/#1-performance-measurement","text":"Memory bandwidth (sometimes also referred to as memory throughput) Processing throughput To calculate the memory bandwidth we need to understand two concepts related to it. Memory interface width: It is the physical bit-width of the memory bus. Every clock-cycle data is transferred along a memory bus to and from the on-card memory. The memory interface width is the physical count of the bits of how many can fit down the bus per clock cycle. Memory clock rate: (or memory clock cycle) Measured in the unit of Hz it is the rate of memory bus clock. For example a memory bus operating on a 1GHz rate is capable of issuing memory tranfers with the bus 10^9 times per second. Talking about clock rate it is important to take a detour to SDR , DDR and QDR . Example A simple analogy is of course public transport, where memory interface width would be the number of seats on a bus and the memory clock rate is the frequency of buses in unit time (hour let's say). Multiplying these two numbers we can calculate how many people can be transported along one bus line in a certain time period.","title":"1. Performance measurement"},{"location":"weeks/further_reading/#2-parallel-programming-models","text":"Suggested reading: SIMD < SIMT < SMT: parallelism in NVIDIA GPUs by yosefk Quote NVIDIA call their parallel programming model SIMT - \"Single Instruction, Multiple Threads\". Two other different, but related parallel programming models are SIMD - \"Single Instruction, Multiple Data\", and SMT - \"Simultaneous Multithreading\". Each model exploits a different source of parallelism: In SIMD, elements of short vectors are processed in parallel. In SMT, instructions of several threads are run in parallel. SIMT is somewhere in between \u2013 an interesting hybrid between vector processing and hardware threading. Other resources https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html https://towardsdatascience.com/the-ai-illustrated-guide-why-are-gpus-so-powerful-99f4ae85a5c3 https://streamhpc.com/blog/2016-07-19/performance-can-be-measured-as-throughput-latency-or-processor-utilisation/ https://www.gamersnexus.net/dictionary/3-memory-interface","title":"2. Parallel Programming models"},{"location":"weeks/week01/","text":"Week 01 - 2021.12.06-10. Overview What is the CUDA programming model? Hierarchy of thread groups Kernels and other language extensions Resources This material heavily borrows from the following sources: CUDA C++ Programming Guide Introduction to GPUs by New York University Introduction to parallel programming and CUDA by Felice Pantaleo Introduction 1. CUDA\u00ae: A General-Purpose Parallel Computing Platform and Programming Model In November 2006, NVIDIA\u00ae introduced CUDA\u00ae , which originally stood for \u201cCompute Unified Device Architecture\u201d, a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU. CUDA comes with a software environment that allows developers to use C++ as a high-level programming language. Other languages, application programming interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC. 2. A Scalable Programming Model At the core of the CUDA parallel programming model there are three key abstractions: a hierarchy of thread groups shared memories barrier synchronization They are exposed to the programmer as a minimal set of language extensions . These abstractions provide fine-grained data parallelism and thread parallelism , nested within coarse-grained data parallelism and task parallelism . Further reading and material Optional reading and exercise on this topic at abstractions: granularity . Programming model 3. Kernels CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels , that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new <<<...>>> execution configuration syntax. Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables. Kernel and execution configuration example // Kernel definition __global__ void VecAdd ( float * A , float * B , float * C ) { int i = threadIdx . x ; C [ i ] = A [ i ] + B [ i ]; } int main () { ... // Kernel invocation with N threads VecAdd <<< 1 , N >>> ( A , B , C ); ... } 01. Question: raw to digi conversion kernel Find the kernel that converts raw data to digis. Where is it launched? What is the execution configuration? How do we access individual threads in the kernel? Go to exercise 01_01 4. Thread hierarchy A kernel is executed in parallel by an array of threads: All threads run the same code. Each thread has an ID that it uses to compute memory addresses and make control decisions. Threads are arranged as a grid of thread blocks: Different kernels can have different grid/block configuration Threads from the same block have access to a shared memory and their execution can be synchronized Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series. This independence requirement allows thread blocks to be scheduled in any order across any number of cores, enabling programmers to write code that scales with the number of cores. Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. The grid of blocks and the thread blocks can be 1, 2, or 3-dimensional. The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs) as shown below. Each SM has a set of execution units, a set of registers and a chunk of shared memory. 5. Language extensions From CUDA Toolkit Documentation: Language Extensions : __global__ The global execution space specifier declares a function as being a kernel. Such a function is: Executed on the device, Callable from the host, Callable from the device for devices of compute capability 3.2 or higher (see CUDA Dynamic Parallelism for more details). A global function must have void return type, and cannot be a member of a class. Any call to a global function must specify its execution configuration as described in Execution . A call to a global function is asynchronous, meaning it returns before the device has completed its execution. __device__ The device execution space specifier declares a function that is: Executed on the device, Callable from the device only. The global and device execution space specifiers cannot be used together. __host__ The host execution space specifier declares a function that is: Executed on the host, Callable from the host only. It is equivalent to declare a function with only the host execution space specifier or to declare it without any of the host , device , or global execution space specifier; in either case the function is compiled for the host only. The global and host execution space specifiers cannot be used together. The device and host execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. 02. Question: host and device functions Give an example of global , device and host-device functions in CMSSW . Can you find an example where host and device code diverge? How is this achieved? Go to exercise 01_02 03. Exercise: Write a kernel in which if we're running on the device each thread prints which block and thread it is associated with, for example block 1 thread 3 if we're running on the host each thread just prints host . Go to exercise 01_03 6. Execution Configuration From CUDA Toolkit Documentation: Execution Configuration Any call to a global function must specify the execution configuration for that call. The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams). The execution configuration is specified by inserting an expression of the form <<< Dg, Db, Ns, S >>> between the function name and the parenthesized argument list, where: Dg is of type dim3 (see dim3) and specifies the dimension and size of the grid, such that Dg.x * Dg.y * Dg.z equals the number of blocks being launched; Db is of type dim3 (see dim3) and specifies the dimension and size of each block, such that Db.x * Db.y * Db.z equals the number of threads per block; Ns is of type size_t and specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory; this dynamically allocated memory is used by any of the variables declared as an external array as mentioned in shared ; Ns is an optional argument which defaults to 0; S is of type cudaStream_t and specifies the associated stream; S is an optional argument which defaults to 0. Abstractions: Granularity Granularity If is the computation time and denotes the communication time, then the Granularity G of a task can be calculated as Granularity is usually measured in terms of the number of instructions executed in a particular task. Fine-grained parallelism Fine-grained parallelism means individual tasks are relatively small in terms of code size and execution time. The data is transferred among processors frequently in amounts of one or a few memory words. Coarse-grained parallelism Coarse-grained is the opposite in the sense that data is communicated infrequently, after larger amounts of computation. The CUDA abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block . This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors. The following exercise requires knowledge about barrier synchronization and shared memory . Follow-up on __syncthreads() and shared memory . 04. Exercise: Fine-grained vs coarse-grained parallelism Give examples in the MatMulKernel kernel of coarse-grained and fine-grained data parallelism (as defined in CUDA abstraction model) as well as sequential execution. Go to exercise 01_04","title":"Week 01 - 2021.12.06-10."},{"location":"weeks/week01/#week-01-20211206-10","text":"Overview What is the CUDA programming model? Hierarchy of thread groups Kernels and other language extensions Resources This material heavily borrows from the following sources: CUDA C++ Programming Guide Introduction to GPUs by New York University Introduction to parallel programming and CUDA by Felice Pantaleo","title":"Week 01 - 2021.12.06-10."},{"location":"weeks/week01/#introduction","text":"","title":"Introduction"},{"location":"weeks/week01/#1-cuda-a-general-purpose-parallel-computing-platform-and-programming-model","text":"In November 2006, NVIDIA\u00ae introduced CUDA\u00ae , which originally stood for \u201cCompute Unified Device Architecture\u201d, a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU. CUDA comes with a software environment that allows developers to use C++ as a high-level programming language. Other languages, application programming interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC.","title":"1. CUDA\u00ae: A General-Purpose Parallel Computing Platform and Programming Model"},{"location":"weeks/week01/#2-a-scalable-programming-model","text":"At the core of the CUDA parallel programming model there are three key abstractions: a hierarchy of thread groups shared memories barrier synchronization They are exposed to the programmer as a minimal set of language extensions . These abstractions provide fine-grained data parallelism and thread parallelism , nested within coarse-grained data parallelism and task parallelism . Further reading and material Optional reading and exercise on this topic at abstractions: granularity .","title":"2. A Scalable Programming Model"},{"location":"weeks/week01/#programming-model","text":"","title":"Programming model"},{"location":"weeks/week01/#3-kernels","text":"CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels , that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new <<<...>>> execution configuration syntax. Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables. Kernel and execution configuration example // Kernel definition __global__ void VecAdd ( float * A , float * B , float * C ) { int i = threadIdx . x ; C [ i ] = A [ i ] + B [ i ]; } int main () { ... // Kernel invocation with N threads VecAdd <<< 1 , N >>> ( A , B , C ); ... } 01. Question: raw to digi conversion kernel Find the kernel that converts raw data to digis. Where is it launched? What is the execution configuration? How do we access individual threads in the kernel? Go to exercise 01_01","title":"3. Kernels"},{"location":"weeks/week01/#4-thread-hierarchy","text":"A kernel is executed in parallel by an array of threads: All threads run the same code. Each thread has an ID that it uses to compute memory addresses and make control decisions. Threads are arranged as a grid of thread blocks: Different kernels can have different grid/block configuration Threads from the same block have access to a shared memory and their execution can be synchronized Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series. This independence requirement allows thread blocks to be scheduled in any order across any number of cores, enabling programmers to write code that scales with the number of cores. Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. The grid of blocks and the thread blocks can be 1, 2, or 3-dimensional. The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs) as shown below. Each SM has a set of execution units, a set of registers and a chunk of shared memory.","title":"4. Thread hierarchy"},{"location":"weeks/week01/#5-language-extensions","text":"From CUDA Toolkit Documentation: Language Extensions :","title":"5. Language extensions"},{"location":"weeks/week01/#__global__","text":"The global execution space specifier declares a function as being a kernel. Such a function is: Executed on the device, Callable from the host, Callable from the device for devices of compute capability 3.2 or higher (see CUDA Dynamic Parallelism for more details). A global function must have void return type, and cannot be a member of a class. Any call to a global function must specify its execution configuration as described in Execution . A call to a global function is asynchronous, meaning it returns before the device has completed its execution.","title":"__global__"},{"location":"weeks/week01/#__device__","text":"The device execution space specifier declares a function that is: Executed on the device, Callable from the device only. The global and device execution space specifiers cannot be used together.","title":"__device__"},{"location":"weeks/week01/#__host__","text":"The host execution space specifier declares a function that is: Executed on the host, Callable from the host only. It is equivalent to declare a function with only the host execution space specifier or to declare it without any of the host , device , or global execution space specifier; in either case the function is compiled for the host only. The global and host execution space specifiers cannot be used together. The device and host execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. 02. Question: host and device functions Give an example of global , device and host-device functions in CMSSW . Can you find an example where host and device code diverge? How is this achieved? Go to exercise 01_02 03. Exercise: Write a kernel in which if we're running on the device each thread prints which block and thread it is associated with, for example block 1 thread 3 if we're running on the host each thread just prints host . Go to exercise 01_03","title":"__host__"},{"location":"weeks/week01/#6-execution-configuration","text":"From CUDA Toolkit Documentation: Execution Configuration Any call to a global function must specify the execution configuration for that call. The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams). The execution configuration is specified by inserting an expression of the form <<< Dg, Db, Ns, S >>> between the function name and the parenthesized argument list, where: Dg is of type dim3 (see dim3) and specifies the dimension and size of the grid, such that Dg.x * Dg.y * Dg.z equals the number of blocks being launched; Db is of type dim3 (see dim3) and specifies the dimension and size of each block, such that Db.x * Db.y * Db.z equals the number of threads per block; Ns is of type size_t and specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory; this dynamically allocated memory is used by any of the variables declared as an external array as mentioned in shared ; Ns is an optional argument which defaults to 0; S is of type cudaStream_t and specifies the associated stream; S is an optional argument which defaults to 0.","title":"6. Execution Configuration"},{"location":"weeks/week01/#abstractions-granularity","text":"Granularity If is the computation time and denotes the communication time, then the Granularity G of a task can be calculated as Granularity is usually measured in terms of the number of instructions executed in a particular task. Fine-grained parallelism Fine-grained parallelism means individual tasks are relatively small in terms of code size and execution time. The data is transferred among processors frequently in amounts of one or a few memory words. Coarse-grained parallelism Coarse-grained is the opposite in the sense that data is communicated infrequently, after larger amounts of computation. The CUDA abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block . This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors. The following exercise requires knowledge about barrier synchronization and shared memory . Follow-up on __syncthreads() and shared memory . 04. Exercise: Fine-grained vs coarse-grained parallelism Give examples in the MatMulKernel kernel of coarse-grained and fine-grained data parallelism (as defined in CUDA abstraction model) as well as sequential execution. Go to exercise 01_04","title":"Abstractions: Granularity"},{"location":"weeks/week01_exercises/","text":"Week 01 - Exercises Exercise_01_01 Question 01. Question: raw to digi conversion kernel Find the kernel that converts raw data to digis. Where is it launched? What is the execution configuration? How do we access individual threads in the kernel? To search the source code use CMSSW dxr - software cross-reference . Solution 01.a. Find the kernel that converts raw data to digis. // Kernel to perform Raw to Digi conversion __global__ void RawToDigi_kernel(const SiPixelROCsStatusAndMapping *cablingMap, const unsigned char *modToUnp, const uint32_t wordCounter, const uint32_t *word, const uint8_t *fedIds, uint16_t *xx, uint16_t *yy, uint16_t *adc, uint32_t *pdigi, ... 01.b. Where is it launched? It is launched in the makeClustersAsync function: void SiPixelRawToClusterGPUKernel :: makeClustersAsync ( bool isRun2 , const SiPixelClusterThresholds clusterThresholds , const SiPixelROCsStatusAndMapping * cablingMap , const unsigned char * modToUnp , const SiPixelGainForHLTonGPU * gains , const WordFedAppender & wordFed , SiPixelFormatterErrors && errors , const uint32_t wordCounter , ... // Launch rawToDigi kernel RawToDigi_kernel <<< blocks , threadsPerBlock , 0 , stream >>> ( cablingMap , modToUnp , wordCounter , word_d . get (), fedId_d . get (), digis_d . view (). xx (), digis_d . view (). yy (), digis_d . view (). adc (), ... 01.c. What is the execution configuration? For the RawToDigi_kernel he execution configuration is defined as <<< blocks , threadsPerBlock , 0 , stream >>> Where const int threadsPerBlock = 512 ; const int blocks = ( wordCounter + threadsPerBlock - 1 ) / threadsPerBlock ; // fill it all In this case 01.d. How do we access individual threads in the kernel? int32_t first = threadIdx . x + blockIdx . x * blockDim . x ; Exercise_01_02 Question 02. Question: host and device functions Give an example of global , device and host-device functions in CMSSW . Can you find an example where host and device code diverge? How is this achieved? Solution 02.a. Give an example of global , device and host-device functions in CMSSW . For example see __global__ kernel in previous exercise . __device__ function in RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu : __device__ pixelgpudetails :: DetIdGPU getRawId ( const SiPixelROCsStatusAndMapping * cablingMap , uint8_t fed , uint32_t link , uint32_t roc ) { uint32_t index = fed * MAX_LINK * MAX_ROC + ( link - 1 ) * MAX_ROC + roc ; pixelgpudetails :: DetIdGPU detId = { cablingMap -> rawId [ index ], cablingMap -> rocInDet [ index ], cablingMap -> moduleId [ index ]}; return detId ; } __host__ __device__ function in HeterogeneousCore/CUDAUtilities/interface/OneToManyAssoc.h : __host__ __device__ __forceinline__ void add ( CountersOnly const & co ) { for ( int32_t i = 0 ; i < totOnes (); ++ i ) { #ifdef __CUDA_ARCH__ atomicAdd ( off . data () + i , co . off [ i ]); #else auto & a = ( std :: atomic < Counter > & )( off [ i ]); a += co . off [ i ]; #endif } } 02.b. Can you find an example where host and device code diverge? How is this achieved? In the CUDA C Programming Guide we can read that: The __device__ and __host__ execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. The __CUDA_ARCH__ macro introduced in Application Compatibility can be used to differentiate code paths between host and device: __host__ __device__ func () { #if __CUDA_ARCH__ >= 800 // Device code path for compute capability 8.x #elif __CUDA_ARCH__ >= 700 // Device code path for compute capability 7.x #elif __CUDA_ARCH__ >= 600 // Device code path for compute capability 6.x #elif __CUDA_ARCH__ >= 500 // Device code path for compute capability 5.x #elif __CUDA_ARCH__ >= 300 // Device code path for compute capability 3.x #elif !defined(__CUDA_ARCH__) // Host code path #endif } Based on this we can see how execution diverges in the previous add function: __host__ __device__ __forceinline__ void add ( CountersOnly const & co ) { for ( int32_t i = 0 ; i < totOnes (); ++ i ) { #ifdef __CUDA_ARCH__ atomicAdd ( off . data () + i , co . off [ i ]); #else auto & a = ( std :: atomic < Counter > & )( off [ i ]); a += co . off [ i ]; #endif } } Exercise_01_03 Exercise 03. Exercise: Write a kernel in which if we're running on the device each thread prints which block and thread it is associated with, for example block 1 thread 3 if we're running on the host each thread just prints host . Test your program! How can you \"hide\" your GPU? Try using CUDA_VISIBLE_DEVICES from the command line. Exercise_01_04 Exercise 04. Exercise: Fine-grained vs coarse-grained parallelism: Give examples in the MatMulKernel kernel of coarse-grained and fine-grained data parallelism (as defined in CUDA abstraction model) as well as sequential execution. MatMulKernel Matrix definition GetElement and SetElement GetSubMatrix // Thread block size #define BLOCK_SIZE 16 __global__ void MatMulKernel ( Matrix A , Matrix B , Matrix C ) { // Block row and column int blockRow = blockIdx . y ; int blockCol = blockIdx . x ; // Each thread block computes one sub-matrix Csub of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol ); // Each thread computes one element of Csub // by accumulating results into Cvalue float Cvalue = 0 ; // Thread row and column within Csub int row = threadIdx . y ; int col = threadIdx . x ; // Loop over all the sub-matrices of A and B that are // required to compute Csub // Multiply each pair of sub-matrices together // and accumulate the results for ( int m = 0 ; m < ( A . width / BLOCK_SIZE ); ++ m ) { // Get sub-matrix Asub of A Matrix Asub = GetSubMatrix ( A , blockRow , m ); // Get sub-matrix Bsub of B Matrix Bsub = GetSubMatrix ( B , m , blockCol ); // Shared memory used to store Asub and Bsub respectively __shared__ float As [ BLOCK_SIZE ][ BLOCK_SIZE ]; __shared__ float Bs [ BLOCK_SIZE ][ BLOCK_SIZE ]; // Load Asub and Bsub from device memory to shared memory // Each thread loads one element of each sub-matrix As [ row ][ col ] = GetElement ( Asub , row , col ); Bs [ row ][ col ] = GetElement ( Bsub , row , col ); // Synchronize to make sure the sub-matrices are loaded // before starting the computation __syncthreads (); // Multiply Asub and Bsub together for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; // Synchronize to make sure that the preceding // computation is done before loading two new // sub-matrices of A and B in the next iteration __syncthreads (); } // Write Csub to device memory // Each thread writes one element SetElement ( Csub , row , col , Cvalue ); } // Matrices are stored in row-major order: // M(row, col) = *(M.elements + row * M.stride + col) typedef struct { int width ; int height ; int stride ; float * elements ; } Matrix ; // Get a matrix element __device__ float GetElement ( const Matrix A , int row , int col ) { return A . elements [ row * A . stride + col ]; } // Set a matrix element __device__ void SetElement ( Matrix A , int row , int col , float value ) { A . elements [ row * A . stride + col ] = value ; } // Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is // located col sub-matrices to the right and row sub-matrices down // from the upper-left corner of A __device__ Matrix GetSubMatrix ( Matrix A , int row , int col ) { Matrix Asub ; Asub . width = BLOCK_SIZE ; Asub . height = BLOCK_SIZE ; Asub . stride = A . stride ; Asub . elements = & A . elements [ A . stride * BLOCK_SIZE * row + BLOCK_SIZE * col ]; return Asub ; } Solution 04.a. Give examples in the MatMulKernel kernel of coarse-grained data parallelism . Coarse-grained data parallel problems in the CUDA programming model are problems that can be solved independently in parallel by blocks of threads. For example in the MatMulKernel : // Block row and column int blockRow = blockIdx . y ; int blockCol = blockIdx . x ; // Each thread block computes one sub-matrix Csub of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol ); The computation of Csub is independent of the computation of other submatrices of C . The work is divided between blocks , no synchronization is performed between computing different submatrices of C . 04.b. Give examples in the MatMulKernel kernel of fine-grained data parallelism . Fine-grained data parallel problems in the CUDA programming model are finer pieces that can be solved cooperatively in parallel by all threads within the block. For example in the MatMulKernel : // Get sub-matrix Asub of A Matrix Asub = GetSubMatrix ( A , blockRow , m ); // Get sub-matrix Bsub of B Matrix Bsub = GetSubMatrix ( B , m , blockCol ); // Shared memory used to store Asub and Bsub respectively __shared__ float As [ BLOCK_SIZE ][ BLOCK_SIZE ]; __shared__ float Bs [ BLOCK_SIZE ][ BLOCK_SIZE ]; // Load Asub and Bsub from device memory to shared memory // Each thread loads one element of each sub-matrix As [ row ][ col ] = GetElement ( Asub , row , col ); Bs [ row ][ col ] = GetElement ( Bsub , row , col ); // Synchronize to make sure the sub-matrices are loaded // before starting the computation __syncthreads (); Loading data into shared memory blocks of matrix A and B is executed parellel by all threads within the block. 04.c. Give examples in the MatMulKernel kernel of sequential execution . // Multiply Asub and Bsub together for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; The computation of Cvalue for each thread is sequential, we execute BLOCK_SIZE additions and multiplications. On the other hand the computation of Cvalue is also a good example of fine-grained data parallelism, since there is one value computed by each thread in the block parallel. To identify fine-grained parallelism one just needs to look for block-level synchronization: for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; // Synchronize to make sure that the preceding // computation is done before loading two new // sub-matrices of A and B in the next iteration __syncthreads ();","title":"Week 01 - Exercises"},{"location":"weeks/week01_exercises/#week-01-exercises","text":"","title":"Week 01 - Exercises"},{"location":"weeks/week01_exercises/#exercise_01_01","text":"","title":"Exercise_01_01"},{"location":"weeks/week01_exercises/#question","text":"01. Question: raw to digi conversion kernel Find the kernel that converts raw data to digis. Where is it launched? What is the execution configuration? How do we access individual threads in the kernel? To search the source code use CMSSW dxr - software cross-reference .","title":"Question"},{"location":"weeks/week01_exercises/#solution","text":"01.a. Find the kernel that converts raw data to digis. // Kernel to perform Raw to Digi conversion __global__ void RawToDigi_kernel(const SiPixelROCsStatusAndMapping *cablingMap, const unsigned char *modToUnp, const uint32_t wordCounter, const uint32_t *word, const uint8_t *fedIds, uint16_t *xx, uint16_t *yy, uint16_t *adc, uint32_t *pdigi, ... 01.b. Where is it launched? It is launched in the makeClustersAsync function: void SiPixelRawToClusterGPUKernel :: makeClustersAsync ( bool isRun2 , const SiPixelClusterThresholds clusterThresholds , const SiPixelROCsStatusAndMapping * cablingMap , const unsigned char * modToUnp , const SiPixelGainForHLTonGPU * gains , const WordFedAppender & wordFed , SiPixelFormatterErrors && errors , const uint32_t wordCounter , ... // Launch rawToDigi kernel RawToDigi_kernel <<< blocks , threadsPerBlock , 0 , stream >>> ( cablingMap , modToUnp , wordCounter , word_d . get (), fedId_d . get (), digis_d . view (). xx (), digis_d . view (). yy (), digis_d . view (). adc (), ... 01.c. What is the execution configuration? For the RawToDigi_kernel he execution configuration is defined as <<< blocks , threadsPerBlock , 0 , stream >>> Where const int threadsPerBlock = 512 ; const int blocks = ( wordCounter + threadsPerBlock - 1 ) / threadsPerBlock ; // fill it all In this case 01.d. How do we access individual threads in the kernel? int32_t first = threadIdx . x + blockIdx . x * blockDim . x ;","title":"Solution"},{"location":"weeks/week01_exercises/#exercise_01_02","text":"","title":"Exercise_01_02"},{"location":"weeks/week01_exercises/#question_1","text":"02. Question: host and device functions Give an example of global , device and host-device functions in CMSSW . Can you find an example where host and device code diverge? How is this achieved?","title":"Question"},{"location":"weeks/week01_exercises/#solution_1","text":"02.a. Give an example of global , device and host-device functions in CMSSW . For example see __global__ kernel in previous exercise . __device__ function in RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu : __device__ pixelgpudetails :: DetIdGPU getRawId ( const SiPixelROCsStatusAndMapping * cablingMap , uint8_t fed , uint32_t link , uint32_t roc ) { uint32_t index = fed * MAX_LINK * MAX_ROC + ( link - 1 ) * MAX_ROC + roc ; pixelgpudetails :: DetIdGPU detId = { cablingMap -> rawId [ index ], cablingMap -> rocInDet [ index ], cablingMap -> moduleId [ index ]}; return detId ; } __host__ __device__ function in HeterogeneousCore/CUDAUtilities/interface/OneToManyAssoc.h : __host__ __device__ __forceinline__ void add ( CountersOnly const & co ) { for ( int32_t i = 0 ; i < totOnes (); ++ i ) { #ifdef __CUDA_ARCH__ atomicAdd ( off . data () + i , co . off [ i ]); #else auto & a = ( std :: atomic < Counter > & )( off [ i ]); a += co . off [ i ]; #endif } } 02.b. Can you find an example where host and device code diverge? How is this achieved? In the CUDA C Programming Guide we can read that: The __device__ and __host__ execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. The __CUDA_ARCH__ macro introduced in Application Compatibility can be used to differentiate code paths between host and device: __host__ __device__ func () { #if __CUDA_ARCH__ >= 800 // Device code path for compute capability 8.x #elif __CUDA_ARCH__ >= 700 // Device code path for compute capability 7.x #elif __CUDA_ARCH__ >= 600 // Device code path for compute capability 6.x #elif __CUDA_ARCH__ >= 500 // Device code path for compute capability 5.x #elif __CUDA_ARCH__ >= 300 // Device code path for compute capability 3.x #elif !defined(__CUDA_ARCH__) // Host code path #endif } Based on this we can see how execution diverges in the previous add function: __host__ __device__ __forceinline__ void add ( CountersOnly const & co ) { for ( int32_t i = 0 ; i < totOnes (); ++ i ) { #ifdef __CUDA_ARCH__ atomicAdd ( off . data () + i , co . off [ i ]); #else auto & a = ( std :: atomic < Counter > & )( off [ i ]); a += co . off [ i ]; #endif } }","title":"Solution"},{"location":"weeks/week01_exercises/#exercise_01_03","text":"","title":"Exercise_01_03"},{"location":"weeks/week01_exercises/#exercise","text":"03. Exercise: Write a kernel in which if we're running on the device each thread prints which block and thread it is associated with, for example block 1 thread 3 if we're running on the host each thread just prints host . Test your program! How can you \"hide\" your GPU? Try using CUDA_VISIBLE_DEVICES from the command line.","title":"Exercise"},{"location":"weeks/week01_exercises/#exercise_01_04","text":"","title":"Exercise_01_04"},{"location":"weeks/week01_exercises/#exercise_1","text":"04. Exercise: Fine-grained vs coarse-grained parallelism: Give examples in the MatMulKernel kernel of coarse-grained and fine-grained data parallelism (as defined in CUDA abstraction model) as well as sequential execution. MatMulKernel Matrix definition GetElement and SetElement GetSubMatrix // Thread block size #define BLOCK_SIZE 16 __global__ void MatMulKernel ( Matrix A , Matrix B , Matrix C ) { // Block row and column int blockRow = blockIdx . y ; int blockCol = blockIdx . x ; // Each thread block computes one sub-matrix Csub of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol ); // Each thread computes one element of Csub // by accumulating results into Cvalue float Cvalue = 0 ; // Thread row and column within Csub int row = threadIdx . y ; int col = threadIdx . x ; // Loop over all the sub-matrices of A and B that are // required to compute Csub // Multiply each pair of sub-matrices together // and accumulate the results for ( int m = 0 ; m < ( A . width / BLOCK_SIZE ); ++ m ) { // Get sub-matrix Asub of A Matrix Asub = GetSubMatrix ( A , blockRow , m ); // Get sub-matrix Bsub of B Matrix Bsub = GetSubMatrix ( B , m , blockCol ); // Shared memory used to store Asub and Bsub respectively __shared__ float As [ BLOCK_SIZE ][ BLOCK_SIZE ]; __shared__ float Bs [ BLOCK_SIZE ][ BLOCK_SIZE ]; // Load Asub and Bsub from device memory to shared memory // Each thread loads one element of each sub-matrix As [ row ][ col ] = GetElement ( Asub , row , col ); Bs [ row ][ col ] = GetElement ( Bsub , row , col ); // Synchronize to make sure the sub-matrices are loaded // before starting the computation __syncthreads (); // Multiply Asub and Bsub together for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; // Synchronize to make sure that the preceding // computation is done before loading two new // sub-matrices of A and B in the next iteration __syncthreads (); } // Write Csub to device memory // Each thread writes one element SetElement ( Csub , row , col , Cvalue ); } // Matrices are stored in row-major order: // M(row, col) = *(M.elements + row * M.stride + col) typedef struct { int width ; int height ; int stride ; float * elements ; } Matrix ; // Get a matrix element __device__ float GetElement ( const Matrix A , int row , int col ) { return A . elements [ row * A . stride + col ]; } // Set a matrix element __device__ void SetElement ( Matrix A , int row , int col , float value ) { A . elements [ row * A . stride + col ] = value ; } // Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is // located col sub-matrices to the right and row sub-matrices down // from the upper-left corner of A __device__ Matrix GetSubMatrix ( Matrix A , int row , int col ) { Matrix Asub ; Asub . width = BLOCK_SIZE ; Asub . height = BLOCK_SIZE ; Asub . stride = A . stride ; Asub . elements = & A . elements [ A . stride * BLOCK_SIZE * row + BLOCK_SIZE * col ]; return Asub ; }","title":"Exercise"},{"location":"weeks/week01_exercises/#solution_2","text":"04.a. Give examples in the MatMulKernel kernel of coarse-grained data parallelism . Coarse-grained data parallel problems in the CUDA programming model are problems that can be solved independently in parallel by blocks of threads. For example in the MatMulKernel : // Block row and column int blockRow = blockIdx . y ; int blockCol = blockIdx . x ; // Each thread block computes one sub-matrix Csub of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol ); The computation of Csub is independent of the computation of other submatrices of C . The work is divided between blocks , no synchronization is performed between computing different submatrices of C . 04.b. Give examples in the MatMulKernel kernel of fine-grained data parallelism . Fine-grained data parallel problems in the CUDA programming model are finer pieces that can be solved cooperatively in parallel by all threads within the block. For example in the MatMulKernel : // Get sub-matrix Asub of A Matrix Asub = GetSubMatrix ( A , blockRow , m ); // Get sub-matrix Bsub of B Matrix Bsub = GetSubMatrix ( B , m , blockCol ); // Shared memory used to store Asub and Bsub respectively __shared__ float As [ BLOCK_SIZE ][ BLOCK_SIZE ]; __shared__ float Bs [ BLOCK_SIZE ][ BLOCK_SIZE ]; // Load Asub and Bsub from device memory to shared memory // Each thread loads one element of each sub-matrix As [ row ][ col ] = GetElement ( Asub , row , col ); Bs [ row ][ col ] = GetElement ( Bsub , row , col ); // Synchronize to make sure the sub-matrices are loaded // before starting the computation __syncthreads (); Loading data into shared memory blocks of matrix A and B is executed parellel by all threads within the block. 04.c. Give examples in the MatMulKernel kernel of sequential execution . // Multiply Asub and Bsub together for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; The computation of Cvalue for each thread is sequential, we execute BLOCK_SIZE additions and multiplications. On the other hand the computation of Cvalue is also a good example of fine-grained data parallelism, since there is one value computed by each thread in the block parallel. To identify fine-grained parallelism one just needs to look for block-level synchronization: for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; // Synchronize to make sure that the preceding // computation is done before loading two new // sub-matrices of A and B in the next iteration __syncthreads ();","title":"Solution"},{"location":"weeks/week02/","text":"Week 02 - 2021.12.13-17. Resources CUDA C++ Programming Guide Introduction to GPUs by New York University NVidia Developer Blog: Using Shared Memory CUDA C/C++ In the previous material in A Scalable Programming Model we've been reading about the three key abstractions in the CUDA programming model: a hierarchy of thread groups shared memories barrier synchronization Thread hierarchy has been previously covered, and in this part shared memory and barrier synchronization follows. 1. Shared memory Memory hierarchy CUDA threads may access data from multiple memory spaces during their execution. Each thread has private local memory . Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. All threads have access to the same global memory . There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces, which we won't cover in detail here. For more information continue reading here. The global, constant, and texture memory spaces are persistent across kernel launches by the same application. __shared__ As detailed in Variable Memory Space Specifiers shared memory is allocated using the __shared__ memory space specifier. Shared memory is expected to be much faster than global memory. Static and dynamic Example from: https://github.com/NVIDIA-developer-blog/code-samples/blob/master/series/cuda-cpp/shared-memory/shared-memory.cu Usage of static and dynamic memory copyright staticReverse dynamicReverse main /* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved. * * Redistribution and use in source and binary forms, with or without * modification, are permitted provided that the following conditions * are met: * * Redistributions of source code must retain the above copyright * notice, this list of conditions and the following disclaimer. * * Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimer in the * documentation and/or other materials provided with the distribution. * * Neither the name of NVIDIA CORPORATION nor the names of its * contributors may be used to endorse or promote products derived * from this software without specific prior written permission. * * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. */ #include <stdio.h> __global__ void staticReverse ( int * d , int n ) { __shared__ int s [ 64 ]; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } __global__ void dynamicReverse ( int * d , int n ) { extern __shared__ int s []; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } int main ( void ) { const int n = 64 ; int a [ n ], r [ n ], d [ n ]; for ( int i = 0 ; i < n ; i ++ ) { a [ i ] = i ; r [ i ] = n - i -1 ; d [ i ] = 0 ; } int * d_d ; cudaMalloc ( & d_d , n * sizeof ( int )); // run version with static shared memory cudaMemcpy ( d_d , a , n * sizeof ( int ), cudaMemcpyHostToDevice ); staticReverse <<< 1 , n >>> ( d_d , n ); cudaMemcpy ( d , d_d , n * sizeof ( int ), cudaMemcpyDeviceToHost ); for ( int i = 0 ; i < n ; i ++ ) if ( d [ i ] != r [ i ]) printf ( \"Error: d[%d]!=r[%d] (%d, %d) \\n \" , i , i , d [ i ], r [ i ]); // run dynamic shared memory version cudaMemcpy ( d_d , a , n * sizeof ( int ), cudaMemcpyHostToDevice ); dynamicReverse <<< 1 , n , n * sizeof ( int ) >>> ( d_d , n ); cudaMemcpy ( d , d_d , n * sizeof ( int ), cudaMemcpyDeviceToHost ); for ( int i = 0 ; i < n ; i ++ ) if ( d [ i ] != r [ i ]) printf ( \"Error: d[%d]!=r[%d] (%d, %d) \\n \" , i , i , d [ i ], r [ i ]); } If the shared memory array size is known at compile time , as in the staticReverse kernel, then we can explicitly declare an array of that size, as we do with the array s . __global__ void staticReverse ( int * d , int n ) { __shared__ int s [ 64 ]; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } The other three kernels in this example use dynamically allocated shared memory, which can be used when the amount of shared memory is not known at compile time . In this case the shared memory allocation size per thread block must be specified (in bytes) using an optional third execution configuration parameter, as in the following excerpt. dynamicReverse <<< 1 , n , n * sizeof ( int ) >>> ( d_d , n ); The dynamic shared memory kernel, dynamicReverse(), declares the shared memory array using an unsized extern array syntax, extern __shared__ int s[] . __global__ void dynamicReverse ( int * d , int n ) { extern __shared__ int s []; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } 2. Barrier synchronization Introduction When sharing data between threads, we need to be careful to avoid race conditions, because while threads in a block run logically in parallel, not all threads can execute physically at the same time. Synchronize block of threads Let\u2019s say that two threads A and B each load a data element from global memory and store it to shared memory. Then, thread A wants to read B\u2019s element from shared memory, and vice versa. Let\u2019s assume that A and B are threads in two different warps. If B has not finished writing its element before A tries to read it, we have a race condition, which can lead to undefined behavior and incorrect results. To ensure correct results when parallel threads cooperate, we must synchronize the threads. CUDA provides a simple barrier synchronization primitive, __syncthreads() . A thread\u2019s execution can only proceed past a __syncthreads() after all threads in its block have executed the __syncthreads() . Thus, we can avoid the race condition described above by calling __syncthreads() after the store to shared memory and before any threads load from shared memory. Deadlocks barrier synchronization deadlock correct barrier int s = threadIdx . x / 2 ; if ( threadIdx . x > s ) { value [ threadIdx . x ] = 2 * s ; __syncthreads (); } else { value [ threadIdx . x ] = s ; __syncthreads (); } int s = threadIdx . x / 2 ; if ( threadIdx . x > s ) { value [ threadIdx . x ] = 2 * s ; } else { value [ threadIdx . x ] = s ; } __syncthreads (); It\u2019s important to be aware that calling __syncthreads() in divergent code is undefined and can lead to deadlock\u2014all threads within a thread block must call __syncthreads() at the same point. 3. Page-Locked Host Memory From the CUDA Programming Guide The runtime provides functions to allow the use of page-locked (also known as pinned) host memory (as opposed to regular pageable host memory allocated by malloc()): cudaHostAlloc() and cudaFreeHost() allocate and free page-locked host memory; cudaHostRegister() page-locks a range of memory allocated by malloc() (see reference manual for limitations). Using page-locked host memory has several benefits: Copies between page-locked host memory and device memory can be performed concurrently with kernel execution for some devices as mentioned in Asynchronous Concurrent Execution. On some devices, page-locked host memory can be mapped into the address space of the device, eliminating the need to copy it to or from device memory as detailed in Mapped Memory. On systems with a front-side bus, bandwidth between host memory and device memory is higher if host memory is allocated as page-locked and even higher if in addition it is allocated as write-combining as described in Write-Combining Memory. Explaining pinned memory : Setting flags for cudaHostAlloc :","title":"Week 02 - 2021.12.13-17."},{"location":"weeks/week02/#week-02-20211213-17","text":"Resources CUDA C++ Programming Guide Introduction to GPUs by New York University NVidia Developer Blog: Using Shared Memory CUDA C/C++ In the previous material in A Scalable Programming Model we've been reading about the three key abstractions in the CUDA programming model: a hierarchy of thread groups shared memories barrier synchronization Thread hierarchy has been previously covered, and in this part shared memory and barrier synchronization follows.","title":"Week 02 - 2021.12.13-17."},{"location":"weeks/week02/#1-shared-memory","text":"","title":"1. Shared memory"},{"location":"weeks/week02/#memory-hierarchy","text":"CUDA threads may access data from multiple memory spaces during their execution. Each thread has private local memory . Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. All threads have access to the same global memory . There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces, which we won't cover in detail here. For more information continue reading here. The global, constant, and texture memory spaces are persistent across kernel launches by the same application.","title":"Memory hierarchy"},{"location":"weeks/week02/#__shared__","text":"As detailed in Variable Memory Space Specifiers shared memory is allocated using the __shared__ memory space specifier. Shared memory is expected to be much faster than global memory.","title":"__shared__"},{"location":"weeks/week02/#static-and-dynamic","text":"Example from: https://github.com/NVIDIA-developer-blog/code-samples/blob/master/series/cuda-cpp/shared-memory/shared-memory.cu Usage of static and dynamic memory copyright staticReverse dynamicReverse main /* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved. * * Redistribution and use in source and binary forms, with or without * modification, are permitted provided that the following conditions * are met: * * Redistributions of source code must retain the above copyright * notice, this list of conditions and the following disclaimer. * * Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimer in the * documentation and/or other materials provided with the distribution. * * Neither the name of NVIDIA CORPORATION nor the names of its * contributors may be used to endorse or promote products derived * from this software without specific prior written permission. * * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. */ #include <stdio.h> __global__ void staticReverse ( int * d , int n ) { __shared__ int s [ 64 ]; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } __global__ void dynamicReverse ( int * d , int n ) { extern __shared__ int s []; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } int main ( void ) { const int n = 64 ; int a [ n ], r [ n ], d [ n ]; for ( int i = 0 ; i < n ; i ++ ) { a [ i ] = i ; r [ i ] = n - i -1 ; d [ i ] = 0 ; } int * d_d ; cudaMalloc ( & d_d , n * sizeof ( int )); // run version with static shared memory cudaMemcpy ( d_d , a , n * sizeof ( int ), cudaMemcpyHostToDevice ); staticReverse <<< 1 , n >>> ( d_d , n ); cudaMemcpy ( d , d_d , n * sizeof ( int ), cudaMemcpyDeviceToHost ); for ( int i = 0 ; i < n ; i ++ ) if ( d [ i ] != r [ i ]) printf ( \"Error: d[%d]!=r[%d] (%d, %d) \\n \" , i , i , d [ i ], r [ i ]); // run dynamic shared memory version cudaMemcpy ( d_d , a , n * sizeof ( int ), cudaMemcpyHostToDevice ); dynamicReverse <<< 1 , n , n * sizeof ( int ) >>> ( d_d , n ); cudaMemcpy ( d , d_d , n * sizeof ( int ), cudaMemcpyDeviceToHost ); for ( int i = 0 ; i < n ; i ++ ) if ( d [ i ] != r [ i ]) printf ( \"Error: d[%d]!=r[%d] (%d, %d) \\n \" , i , i , d [ i ], r [ i ]); } If the shared memory array size is known at compile time , as in the staticReverse kernel, then we can explicitly declare an array of that size, as we do with the array s . __global__ void staticReverse ( int * d , int n ) { __shared__ int s [ 64 ]; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } The other three kernels in this example use dynamically allocated shared memory, which can be used when the amount of shared memory is not known at compile time . In this case the shared memory allocation size per thread block must be specified (in bytes) using an optional third execution configuration parameter, as in the following excerpt. dynamicReverse <<< 1 , n , n * sizeof ( int ) >>> ( d_d , n ); The dynamic shared memory kernel, dynamicReverse(), declares the shared memory array using an unsized extern array syntax, extern __shared__ int s[] . __global__ void dynamicReverse ( int * d , int n ) { extern __shared__ int s []; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; }","title":"Static and dynamic"},{"location":"weeks/week02/#2-barrier-synchronization","text":"","title":"2. Barrier synchronization"},{"location":"weeks/week02/#introduction","text":"When sharing data between threads, we need to be careful to avoid race conditions, because while threads in a block run logically in parallel, not all threads can execute physically at the same time.","title":"Introduction"},{"location":"weeks/week02/#synchronize-block-of-threads","text":"Let\u2019s say that two threads A and B each load a data element from global memory and store it to shared memory. Then, thread A wants to read B\u2019s element from shared memory, and vice versa. Let\u2019s assume that A and B are threads in two different warps. If B has not finished writing its element before A tries to read it, we have a race condition, which can lead to undefined behavior and incorrect results. To ensure correct results when parallel threads cooperate, we must synchronize the threads. CUDA provides a simple barrier synchronization primitive, __syncthreads() . A thread\u2019s execution can only proceed past a __syncthreads() after all threads in its block have executed the __syncthreads() . Thus, we can avoid the race condition described above by calling __syncthreads() after the store to shared memory and before any threads load from shared memory.","title":"Synchronize block of threads"},{"location":"weeks/week02/#deadlocks","text":"barrier synchronization deadlock correct barrier int s = threadIdx . x / 2 ; if ( threadIdx . x > s ) { value [ threadIdx . x ] = 2 * s ; __syncthreads (); } else { value [ threadIdx . x ] = s ; __syncthreads (); } int s = threadIdx . x / 2 ; if ( threadIdx . x > s ) { value [ threadIdx . x ] = 2 * s ; } else { value [ threadIdx . x ] = s ; } __syncthreads (); It\u2019s important to be aware that calling __syncthreads() in divergent code is undefined and can lead to deadlock\u2014all threads within a thread block must call __syncthreads() at the same point.","title":"Deadlocks"},{"location":"weeks/week02/#3-page-locked-host-memory","text":"From the CUDA Programming Guide The runtime provides functions to allow the use of page-locked (also known as pinned) host memory (as opposed to regular pageable host memory allocated by malloc()): cudaHostAlloc() and cudaFreeHost() allocate and free page-locked host memory; cudaHostRegister() page-locks a range of memory allocated by malloc() (see reference manual for limitations). Using page-locked host memory has several benefits: Copies between page-locked host memory and device memory can be performed concurrently with kernel execution for some devices as mentioned in Asynchronous Concurrent Execution. On some devices, page-locked host memory can be mapped into the address space of the device, eliminating the need to copy it to or from device memory as detailed in Mapped Memory. On systems with a front-side bus, bandwidth between host memory and device memory is higher if host memory is allocated as page-locked and even higher if in addition it is allocated as write-combining as described in Write-Combining Memory. Explaining pinned memory : Setting flags for cudaHostAlloc :","title":"3. Page-Locked Host Memory"},{"location":"weeks/week02_exercises/","text":"Week 02 - Exercises Follow exercises from Patatrack Knowledge Transfer part 1","title":"Week 02 - Exercises"},{"location":"weeks/week02_exercises/#week-02-exercises","text":"Follow exercises from Patatrack Knowledge Transfer part 1","title":"Week 02 - Exercises"},{"location":"workflows/profiling/","text":"Profiling Prerequisites Check activity on current GPU node: foo@bar~$ nvidia-smi Thu Oct 21 17:54:04 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:5E:00.0 Off | 0 | | N/A 41C P0 26W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Prepare uncompressed ROOT files It is not evident that reconstruction runs faster on uncompressed input files, it depends on whether our application is computation or memory bound.","title":"Profiling"},{"location":"workflows/profiling/#profiling","text":"","title":"Profiling"},{"location":"workflows/profiling/#prerequisites","text":"Check activity on current GPU node: foo@bar~$ nvidia-smi Thu Oct 21 17:54:04 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:5E:00.0 Off | 0 | | N/A 41C P0 26W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+","title":"Prerequisites"},{"location":"workflows/profiling/#prepare-uncompressed-root-files","text":"It is not evident that reconstruction runs faster on uncompressed input files, it depends on whether our application is computation or memory bound.","title":"Prepare uncompressed ROOT files"}]}