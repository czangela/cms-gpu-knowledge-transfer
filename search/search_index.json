{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CMS Tracker DPG Knowledge Transfer Material Documentation In contrast to some previous approaches (see archived ) this version won't try to cover already existing material. There are some great and readily available resources out there that one can learn to learn CUDA programming, so in this version this won't be covered. Other resources It is recommended and in some cases necessary to follow these courses and read these manuals before browsing the content here: 1. Introduction to GPUs by NYU Introduction to GPUs by New York University 2. Previous Patatrack Knowledge transfer Access here: https://indico.cern.ch/event/863657/ Presentations Direct link to Welcome and Introduction to Parallel Programming Direck link to Introduction to GPU programming using CUDA Direct link to SoA model for Pixel Reconstruction (and beyond?) Direct link to A fully Heterogeneous Pixel Reconstruction Hands-on Solve all exercises here https://patatrack.web.cern.ch/patatrack/wiki/cuda_training_dpg_12_2019/ Part 1 Part 2 Part 3 3. Caltech - GPU Programming [CS 179] http://courses.cms.caltech.edu/cs179/ 4. Patatrack website and Wiki Access Wiki here: https://patatrack.web.cern.ch/patatrack/wiki/ . Access Website here: https://patatrack.web.cern.ch/patatrack/index.html . 5. CUDA algorithms in CMSSW documentation https://github.com/cms-patatrack/cmssw/blob/master/HeterogeneousCore/CUDACore/README.md 6. CUDA Programming Guide Honestly, so many other online resources just borrow from here, one might as well just read the original Access here: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html","title":"Home"},{"location":"#cms-tracker-dpg-knowledge-transfer-material","text":"","title":"CMS Tracker DPG Knowledge Transfer Material"},{"location":"#documentation","text":"In contrast to some previous approaches (see archived ) this version won't try to cover already existing material. There are some great and readily available resources out there that one can learn to learn CUDA programming, so in this version this won't be covered.","title":"Documentation"},{"location":"#other-resources","text":"It is recommended and in some cases necessary to follow these courses and read these manuals before browsing the content here:","title":"Other resources"},{"location":"#1-introduction-to-gpus-by-nyu","text":"Introduction to GPUs by New York University","title":"1. Introduction to GPUs by NYU"},{"location":"#2-previous-patatrack-knowledge-transfer","text":"Access here: https://indico.cern.ch/event/863657/","title":"2. Previous Patatrack Knowledge transfer"},{"location":"#presentations","text":"Direct link to Welcome and Introduction to Parallel Programming Direck link to Introduction to GPU programming using CUDA Direct link to SoA model for Pixel Reconstruction (and beyond?) Direct link to A fully Heterogeneous Pixel Reconstruction","title":"Presentations"},{"location":"#hands-on","text":"Solve all exercises here https://patatrack.web.cern.ch/patatrack/wiki/cuda_training_dpg_12_2019/ Part 1 Part 2 Part 3","title":"Hands-on"},{"location":"#3-caltech-gpu-programming-cs-179","text":"http://courses.cms.caltech.edu/cs179/","title":"3. Caltech - GPU Programming [CS 179]"},{"location":"#4-patatrack-website-and-wiki","text":"Access Wiki here: https://patatrack.web.cern.ch/patatrack/wiki/ . Access Website here: https://patatrack.web.cern.ch/patatrack/index.html .","title":"4. Patatrack website and Wiki"},{"location":"#5-cuda-algorithms-in-cmssw-documentation","text":"https://github.com/cms-patatrack/cmssw/blob/master/HeterogeneousCore/CUDACore/README.md","title":"5. CUDA algorithms in CMSSW documentation"},{"location":"#6-cuda-programming-guide","text":"Honestly, so many other online resources just borrow from here, one might as well just read the original Access here: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html","title":"6. CUDA Programming Guide"},{"location":"archive/","text":"Archived resources I wasn't sure how to start knowledge transfer. In this previous approach I started creating a course to cover CUDA basics and make the reader familiar with CMSSW . week01_material week01_exercises week02_material further_reading","title":"Archived resources"},{"location":"archive/#archived-resources","text":"I wasn't sure how to start knowledge transfer. In this previous approach I started creating a course to cover CUDA basics and make the reader familiar with CMSSW . week01_material week01_exercises week02_material further_reading","title":"Archived resources"},{"location":"archive/weeks/examples/","text":"Practice Example Unordered List Ordered List Example : * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Example : 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Practice"},{"location":"archive/weeks/examples/#practice","text":"Example Unordered List Ordered List Example : * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Example : 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Practice"},{"location":"archive/weeks/further_reading/","text":"Further reading 1. Performance measurement Memory bandwidth (sometimes also referred to as memory throughput) Processing throughput To calculate the memory bandwidth we need to understand two concepts related to it. Memory interface width: It is the physical bit-width of the memory bus. Every clock-cycle data is transferred along a memory bus to and from the on-card memory. The memory interface width is the physical count of the bits of how many can fit down the bus per clock cycle. Memory clock rate: (or memory clock cycle) Measured in the unit of Hz it is the rate of memory bus clock. For example a memory bus operating on a 1GHz rate is capable of issuing memory tranfers with the bus 10^9 times per second. Talking about clock rate it is important to take a detour to SDR , DDR and QDR . Example A simple analogy is of course public transport, where memory interface width would be the number of seats on a bus and the memory clock rate is the frequency of buses in unit time (hour let's say). Multiplying these two numbers we can calculate how many people can be transported along one bus line in a certain time period. 2. Parallel Programming models Suggested reading: SIMD < SIMT < SMT: parallelism in NVIDIA GPUs by yosefk Quote NVIDIA call their parallel programming model SIMT - \"Single Instruction, Multiple Threads\". Two other different, but related parallel programming models are SIMD - \"Single Instruction, Multiple Data\", and SMT - \"Simultaneous Multithreading\". Each model exploits a different source of parallelism: In SIMD, elements of short vectors are processed in parallel. In SMT, instructions of several threads are run in parallel. SIMT is somewhere in between \u2013 an interesting hybrid between vector processing and hardware threading. Other resources https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html https://towardsdatascience.com/the-ai-illustrated-guide-why-are-gpus-so-powerful-99f4ae85a5c3 https://streamhpc.com/blog/2016-07-19/performance-can-be-measured-as-throughput-latency-or-processor-utilisation/ https://www.gamersnexus.net/dictionary/3-memory-interface","title":"Further reading"},{"location":"archive/weeks/further_reading/#further-reading","text":"","title":"Further reading"},{"location":"archive/weeks/further_reading/#1-performance-measurement","text":"Memory bandwidth (sometimes also referred to as memory throughput) Processing throughput To calculate the memory bandwidth we need to understand two concepts related to it. Memory interface width: It is the physical bit-width of the memory bus. Every clock-cycle data is transferred along a memory bus to and from the on-card memory. The memory interface width is the physical count of the bits of how many can fit down the bus per clock cycle. Memory clock rate: (or memory clock cycle) Measured in the unit of Hz it is the rate of memory bus clock. For example a memory bus operating on a 1GHz rate is capable of issuing memory tranfers with the bus 10^9 times per second. Talking about clock rate it is important to take a detour to SDR , DDR and QDR . Example A simple analogy is of course public transport, where memory interface width would be the number of seats on a bus and the memory clock rate is the frequency of buses in unit time (hour let's say). Multiplying these two numbers we can calculate how many people can be transported along one bus line in a certain time period.","title":"1. Performance measurement"},{"location":"archive/weeks/further_reading/#2-parallel-programming-models","text":"Suggested reading: SIMD < SIMT < SMT: parallelism in NVIDIA GPUs by yosefk Quote NVIDIA call their parallel programming model SIMT - \"Single Instruction, Multiple Threads\". Two other different, but related parallel programming models are SIMD - \"Single Instruction, Multiple Data\", and SMT - \"Simultaneous Multithreading\". Each model exploits a different source of parallelism: In SIMD, elements of short vectors are processed in parallel. In SMT, instructions of several threads are run in parallel. SIMT is somewhere in between \u2013 an interesting hybrid between vector processing and hardware threading. Other resources https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html https://towardsdatascience.com/the-ai-illustrated-guide-why-are-gpus-so-powerful-99f4ae85a5c3 https://streamhpc.com/blog/2016-07-19/performance-can-be-measured-as-throughput-latency-or-processor-utilisation/ https://www.gamersnexus.net/dictionary/3-memory-interface","title":"2. Parallel Programming models"},{"location":"archive/weeks/week01/","text":"Week 01 - 2021.12.06-10. Overview What is the CUDA programming model? Hierarchy of thread groups Kernels and other language extensions Resources This material heavily borrows from the following sources: CUDA C++ Programming Guide Introduction to GPUs by New York University Introduction to parallel programming and CUDA by Felice Pantaleo Introduction 1. CUDA\u00ae: A General-Purpose Parallel Computing Platform and Programming Model In November 2006, NVIDIA\u00ae introduced CUDA\u00ae , which originally stood for \u201cCompute Unified Device Architecture\u201d, a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU. CUDA comes with a software environment that allows developers to use C++ as a high-level programming language. Other languages, application programming interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC. 2. A Scalable Programming Model At the core of the CUDA parallel programming model there are three key abstractions: a hierarchy of thread groups shared memories barrier synchronization They are exposed to the programmer as a minimal set of language extensions . These abstractions provide fine-grained data parallelism and thread parallelism , nested within coarse-grained data parallelism and task parallelism . Further reading and material Optional reading and exercise on this topic at abstractions: granularity . Programming model 3. Kernels CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels , that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new <<<...>>> execution configuration syntax. Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables. Kernel and execution configuration example // Kernel definition __global__ void VecAdd ( float * A , float * B , float * C ) { int i = threadIdx . x ; C [ i ] = A [ i ] + B [ i ]; } int main () { ... // Kernel invocation with N threads VecAdd <<< 1 , N >>> ( A , B , C ); ... } 01. Question: raw to digi conversion kernel Find the kernel that converts raw data to digis. Where is it launched? What is the execution configuration? How do we access individual threads in the kernel? Go to exercise 01_01 4. Thread hierarchy A kernel is executed in parallel by an array of threads: All threads run the same code. Each thread has an ID that it uses to compute memory addresses and make control decisions. Threads are arranged as a grid of thread blocks: Different kernels can have different grid/block configuration Threads from the same block have access to a shared memory and their execution can be synchronized Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series. This independence requirement allows thread blocks to be scheduled in any order across any number of cores, enabling programmers to write code that scales with the number of cores. Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. The grid of blocks and the thread blocks can be 1, 2, or 3-dimensional. The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs) as shown below. Each SM has a set of execution units, a set of registers and a chunk of shared memory. 5. Language extensions From CUDA Toolkit Documentation: Language Extensions : __global__ The global execution space specifier declares a function as being a kernel. Such a function is: Executed on the device, Callable from the host, Callable from the device for devices of compute capability 3.2 or higher (see CUDA Dynamic Parallelism for more details). A global function must have void return type, and cannot be a member of a class. Any call to a global function must specify its execution configuration as described in Execution . A call to a global function is asynchronous, meaning it returns before the device has completed its execution. __device__ The device execution space specifier declares a function that is: Executed on the device, Callable from the device only. The global and device execution space specifiers cannot be used together. __host__ The host execution space specifier declares a function that is: Executed on the host, Callable from the host only. It is equivalent to declare a function with only the host execution space specifier or to declare it without any of the host , device , or global execution space specifier; in either case the function is compiled for the host only. The global and host execution space specifiers cannot be used together. The device and host execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. 02. Question: host and device functions Give an example of global , device and host-device functions in CMSSW . Can you find an example where host and device code diverge? How is this achieved? Go to exercise 01_02 03. Exercise: Write a kernel in which if we're running on the device each thread prints which block and thread it is associated with, for example block 1 thread 3 if we're running on the host each thread just prints host . Go to exercise 01_03 6. Execution Configuration From CUDA Toolkit Documentation: Execution Configuration Any call to a global function must specify the execution configuration for that call. The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams). The execution configuration is specified by inserting an expression of the form <<< Dg, Db, Ns, S >>> between the function name and the parenthesized argument list, where: Dg is of type dim3 (see dim3) and specifies the dimension and size of the grid, such that Dg.x * Dg.y * Dg.z equals the number of blocks being launched; Db is of type dim3 (see dim3) and specifies the dimension and size of each block, such that Db.x * Db.y * Db.z equals the number of threads per block; Ns is of type size_t and specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory; this dynamically allocated memory is used by any of the variables declared as an external array as mentioned in shared ; Ns is an optional argument which defaults to 0; S is of type cudaStream_t and specifies the associated stream; S is an optional argument which defaults to 0. Abstractions: Granularity Granularity If is the computation time and denotes the communication time, then the Granularity G of a task can be calculated as Granularity is usually measured in terms of the number of instructions executed in a particular task. Fine-grained parallelism Fine-grained parallelism means individual tasks are relatively small in terms of code size and execution time. The data is transferred among processors frequently in amounts of one or a few memory words. Coarse-grained parallelism Coarse-grained is the opposite in the sense that data is communicated infrequently, after larger amounts of computation. The CUDA abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block . This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors. The following exercise requires knowledge about barrier synchronization and shared memory . Follow-up on __syncthreads() and shared memory . 04. Exercise: Fine-grained vs coarse-grained parallelism Give examples in the MatMulKernel kernel of coarse-grained and fine-grained data parallelism (as defined in CUDA abstraction model) as well as sequential execution. Go to exercise 01_04","title":"Week 01 - 2021.12.06-10."},{"location":"archive/weeks/week01/#week-01-20211206-10","text":"Overview What is the CUDA programming model? Hierarchy of thread groups Kernels and other language extensions Resources This material heavily borrows from the following sources: CUDA C++ Programming Guide Introduction to GPUs by New York University Introduction to parallel programming and CUDA by Felice Pantaleo","title":"Week 01 - 2021.12.06-10."},{"location":"archive/weeks/week01/#introduction","text":"","title":"Introduction"},{"location":"archive/weeks/week01/#1-cuda-a-general-purpose-parallel-computing-platform-and-programming-model","text":"In November 2006, NVIDIA\u00ae introduced CUDA\u00ae , which originally stood for \u201cCompute Unified Device Architecture\u201d, a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU. CUDA comes with a software environment that allows developers to use C++ as a high-level programming language. Other languages, application programming interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC.","title":"1. CUDA\u00ae: A General-Purpose Parallel Computing Platform and Programming Model"},{"location":"archive/weeks/week01/#2-a-scalable-programming-model","text":"At the core of the CUDA parallel programming model there are three key abstractions: a hierarchy of thread groups shared memories barrier synchronization They are exposed to the programmer as a minimal set of language extensions . These abstractions provide fine-grained data parallelism and thread parallelism , nested within coarse-grained data parallelism and task parallelism . Further reading and material Optional reading and exercise on this topic at abstractions: granularity .","title":"2. A Scalable Programming Model"},{"location":"archive/weeks/week01/#programming-model","text":"","title":"Programming model"},{"location":"archive/weeks/week01/#3-kernels","text":"CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels , that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions. A kernel is defined using the __global__ declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new <<<...>>> execution configuration syntax. Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables. Kernel and execution configuration example // Kernel definition __global__ void VecAdd ( float * A , float * B , float * C ) { int i = threadIdx . x ; C [ i ] = A [ i ] + B [ i ]; } int main () { ... // Kernel invocation with N threads VecAdd <<< 1 , N >>> ( A , B , C ); ... } 01. Question: raw to digi conversion kernel Find the kernel that converts raw data to digis. Where is it launched? What is the execution configuration? How do we access individual threads in the kernel? Go to exercise 01_01","title":"3. Kernels"},{"location":"archive/weeks/week01/#4-thread-hierarchy","text":"A kernel is executed in parallel by an array of threads: All threads run the same code. Each thread has an ID that it uses to compute memory addresses and make control decisions. Threads are arranged as a grid of thread blocks: Different kernels can have different grid/block configuration Threads from the same block have access to a shared memory and their execution can be synchronized Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series. This independence requirement allows thread blocks to be scheduled in any order across any number of cores, enabling programmers to write code that scales with the number of cores. Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. The grid of blocks and the thread blocks can be 1, 2, or 3-dimensional. The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs) as shown below. Each SM has a set of execution units, a set of registers and a chunk of shared memory.","title":"4. Thread hierarchy"},{"location":"archive/weeks/week01/#5-language-extensions","text":"From CUDA Toolkit Documentation: Language Extensions :","title":"5. Language extensions"},{"location":"archive/weeks/week01/#__global__","text":"The global execution space specifier declares a function as being a kernel. Such a function is: Executed on the device, Callable from the host, Callable from the device for devices of compute capability 3.2 or higher (see CUDA Dynamic Parallelism for more details). A global function must have void return type, and cannot be a member of a class. Any call to a global function must specify its execution configuration as described in Execution . A call to a global function is asynchronous, meaning it returns before the device has completed its execution.","title":"__global__"},{"location":"archive/weeks/week01/#__device__","text":"The device execution space specifier declares a function that is: Executed on the device, Callable from the device only. The global and device execution space specifiers cannot be used together.","title":"__device__"},{"location":"archive/weeks/week01/#__host__","text":"The host execution space specifier declares a function that is: Executed on the host, Callable from the host only. It is equivalent to declare a function with only the host execution space specifier or to declare it without any of the host , device , or global execution space specifier; in either case the function is compiled for the host only. The global and host execution space specifiers cannot be used together. The device and host execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. 02. Question: host and device functions Give an example of global , device and host-device functions in CMSSW . Can you find an example where host and device code diverge? How is this achieved? Go to exercise 01_02 03. Exercise: Write a kernel in which if we're running on the device each thread prints which block and thread it is associated with, for example block 1 thread 3 if we're running on the host each thread just prints host . Go to exercise 01_03","title":"__host__"},{"location":"archive/weeks/week01/#6-execution-configuration","text":"From CUDA Toolkit Documentation: Execution Configuration Any call to a global function must specify the execution configuration for that call. The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams). The execution configuration is specified by inserting an expression of the form <<< Dg, Db, Ns, S >>> between the function name and the parenthesized argument list, where: Dg is of type dim3 (see dim3) and specifies the dimension and size of the grid, such that Dg.x * Dg.y * Dg.z equals the number of blocks being launched; Db is of type dim3 (see dim3) and specifies the dimension and size of each block, such that Db.x * Db.y * Db.z equals the number of threads per block; Ns is of type size_t and specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory; this dynamically allocated memory is used by any of the variables declared as an external array as mentioned in shared ; Ns is an optional argument which defaults to 0; S is of type cudaStream_t and specifies the associated stream; S is an optional argument which defaults to 0.","title":"6. Execution Configuration"},{"location":"archive/weeks/week01/#abstractions-granularity","text":"Granularity If is the computation time and denotes the communication time, then the Granularity G of a task can be calculated as Granularity is usually measured in terms of the number of instructions executed in a particular task. Fine-grained parallelism Fine-grained parallelism means individual tasks are relatively small in terms of code size and execution time. The data is transferred among processors frequently in amounts of one or a few memory words. Coarse-grained parallelism Coarse-grained is the opposite in the sense that data is communicated infrequently, after larger amounts of computation. The CUDA abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block . This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability. Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors. The following exercise requires knowledge about barrier synchronization and shared memory . Follow-up on __syncthreads() and shared memory . 04. Exercise: Fine-grained vs coarse-grained parallelism Give examples in the MatMulKernel kernel of coarse-grained and fine-grained data parallelism (as defined in CUDA abstraction model) as well as sequential execution. Go to exercise 01_04","title":"Abstractions: Granularity"},{"location":"archive/weeks/week01_exercises/","text":"Week 01 - Exercises Exercise_01_01 Question 01. Question: raw to digi conversion kernel Find the kernel that converts raw data to digis. Where is it launched? What is the execution configuration? How do we access individual threads in the kernel? To search the source code use CMSSW dxr - software cross-reference . Solution 01.a. Find the kernel that converts raw data to digis. // Kernel to perform Raw to Digi conversion __global__ void RawToDigi_kernel(const SiPixelROCsStatusAndMapping *cablingMap, const unsigned char *modToUnp, const uint32_t wordCounter, const uint32_t *word, const uint8_t *fedIds, uint16_t *xx, uint16_t *yy, uint16_t *adc, uint32_t *pdigi, ... 01.b. Where is it launched? It is launched in the makeClustersAsync function: void SiPixelRawToClusterGPUKernel :: makeClustersAsync ( bool isRun2 , const SiPixelClusterThresholds clusterThresholds , const SiPixelROCsStatusAndMapping * cablingMap , const unsigned char * modToUnp , const SiPixelGainForHLTonGPU * gains , const WordFedAppender & wordFed , SiPixelFormatterErrors && errors , const uint32_t wordCounter , ... // Launch rawToDigi kernel RawToDigi_kernel <<< blocks , threadsPerBlock , 0 , stream >>> ( cablingMap , modToUnp , wordCounter , word_d . get (), fedId_d . get (), digis_d . view (). xx (), digis_d . view (). yy (), digis_d . view (). adc (), ... 01.c. What is the execution configuration? For the RawToDigi_kernel he execution configuration is defined as <<< blocks , threadsPerBlock , 0 , stream >>> Where const int threadsPerBlock = 512 ; const int blocks = ( wordCounter + threadsPerBlock - 1 ) / threadsPerBlock ; // fill it all In this case 01.d. How do we access individual threads in the kernel? int32_t first = threadIdx . x + blockIdx . x * blockDim . x ; Exercise_01_02 Question 02. Question: host and device functions Give an example of global , device and host-device functions in CMSSW . Can you find an example where host and device code diverge? How is this achieved? Solution 02.a. Give an example of global , device and host-device functions in CMSSW . For example see __global__ kernel in previous exercise . __device__ function in RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu : __device__ pixelgpudetails :: DetIdGPU getRawId ( const SiPixelROCsStatusAndMapping * cablingMap , uint8_t fed , uint32_t link , uint32_t roc ) { uint32_t index = fed * MAX_LINK * MAX_ROC + ( link - 1 ) * MAX_ROC + roc ; pixelgpudetails :: DetIdGPU detId = { cablingMap -> rawId [ index ], cablingMap -> rocInDet [ index ], cablingMap -> moduleId [ index ]}; return detId ; } __host__ __device__ function in HeterogeneousCore/CUDAUtilities/interface/OneToManyAssoc.h : __host__ __device__ __forceinline__ void add ( CountersOnly const & co ) { for ( int32_t i = 0 ; i < totOnes (); ++ i ) { #ifdef __CUDA_ARCH__ atomicAdd ( off . data () + i , co . off [ i ]); #else auto & a = ( std :: atomic < Counter > & )( off [ i ]); a += co . off [ i ]; #endif } } 02.b. Can you find an example where host and device code diverge? How is this achieved? In the CUDA C Programming Guide we can read that: The __device__ and __host__ execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. The __CUDA_ARCH__ macro introduced in Application Compatibility can be used to differentiate code paths between host and device: __host__ __device__ func () { #if __CUDA_ARCH__ >= 800 // Device code path for compute capability 8.x #elif __CUDA_ARCH__ >= 700 // Device code path for compute capability 7.x #elif __CUDA_ARCH__ >= 600 // Device code path for compute capability 6.x #elif __CUDA_ARCH__ >= 500 // Device code path for compute capability 5.x #elif __CUDA_ARCH__ >= 300 // Device code path for compute capability 3.x #elif !defined(__CUDA_ARCH__) // Host code path #endif } Based on this we can see how execution diverges in the previous add function: __host__ __device__ __forceinline__ void add ( CountersOnly const & co ) { for ( int32_t i = 0 ; i < totOnes (); ++ i ) { #ifdef __CUDA_ARCH__ atomicAdd ( off . data () + i , co . off [ i ]); #else auto & a = ( std :: atomic < Counter > & )( off [ i ]); a += co . off [ i ]; #endif } } Exercise_01_03 Exercise 03. Exercise: Write a kernel in which if we're running on the device each thread prints which block and thread it is associated with, for example block 1 thread 3 if we're running on the host each thread just prints host . Test your program! How can you \"hide\" your GPU? Try using CUDA_VISIBLE_DEVICES from the command line. Exercise_01_04 Exercise 04. Exercise: Fine-grained vs coarse-grained parallelism: Give examples in the MatMulKernel kernel of coarse-grained and fine-grained data parallelism (as defined in CUDA abstraction model) as well as sequential execution. MatMulKernel Matrix definition GetElement and SetElement GetSubMatrix // Thread block size #define BLOCK_SIZE 16 __global__ void MatMulKernel ( Matrix A , Matrix B , Matrix C ) { // Block row and column int blockRow = blockIdx . y ; int blockCol = blockIdx . x ; // Each thread block computes one sub-matrix Csub of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol ); // Each thread computes one element of Csub // by accumulating results into Cvalue float Cvalue = 0 ; // Thread row and column within Csub int row = threadIdx . y ; int col = threadIdx . x ; // Loop over all the sub-matrices of A and B that are // required to compute Csub // Multiply each pair of sub-matrices together // and accumulate the results for ( int m = 0 ; m < ( A . width / BLOCK_SIZE ); ++ m ) { // Get sub-matrix Asub of A Matrix Asub = GetSubMatrix ( A , blockRow , m ); // Get sub-matrix Bsub of B Matrix Bsub = GetSubMatrix ( B , m , blockCol ); // Shared memory used to store Asub and Bsub respectively __shared__ float As [ BLOCK_SIZE ][ BLOCK_SIZE ]; __shared__ float Bs [ BLOCK_SIZE ][ BLOCK_SIZE ]; // Load Asub and Bsub from device memory to shared memory // Each thread loads one element of each sub-matrix As [ row ][ col ] = GetElement ( Asub , row , col ); Bs [ row ][ col ] = GetElement ( Bsub , row , col ); // Synchronize to make sure the sub-matrices are loaded // before starting the computation __syncthreads (); // Multiply Asub and Bsub together for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; // Synchronize to make sure that the preceding // computation is done before loading two new // sub-matrices of A and B in the next iteration __syncthreads (); } // Write Csub to device memory // Each thread writes one element SetElement ( Csub , row , col , Cvalue ); } // Matrices are stored in row-major order: // M(row, col) = *(M.elements + row * M.stride + col) typedef struct { int width ; int height ; int stride ; float * elements ; } Matrix ; // Get a matrix element __device__ float GetElement ( const Matrix A , int row , int col ) { return A . elements [ row * A . stride + col ]; } // Set a matrix element __device__ void SetElement ( Matrix A , int row , int col , float value ) { A . elements [ row * A . stride + col ] = value ; } // Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is // located col sub-matrices to the right and row sub-matrices down // from the upper-left corner of A __device__ Matrix GetSubMatrix ( Matrix A , int row , int col ) { Matrix Asub ; Asub . width = BLOCK_SIZE ; Asub . height = BLOCK_SIZE ; Asub . stride = A . stride ; Asub . elements = & A . elements [ A . stride * BLOCK_SIZE * row + BLOCK_SIZE * col ]; return Asub ; } Solution 04.a. Give examples in the MatMulKernel kernel of coarse-grained data parallelism . Coarse-grained data parallel problems in the CUDA programming model are problems that can be solved independently in parallel by blocks of threads. For example in the MatMulKernel : // Block row and column int blockRow = blockIdx . y ; int blockCol = blockIdx . x ; // Each thread block computes one sub-matrix Csub of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol ); The computation of Csub is independent of the computation of other submatrices of C . The work is divided between blocks , no synchronization is performed between computing different submatrices of C . 04.b. Give examples in the MatMulKernel kernel of fine-grained data parallelism . Fine-grained data parallel problems in the CUDA programming model are finer pieces that can be solved cooperatively in parallel by all threads within the block. For example in the MatMulKernel : // Get sub-matrix Asub of A Matrix Asub = GetSubMatrix ( A , blockRow , m ); // Get sub-matrix Bsub of B Matrix Bsub = GetSubMatrix ( B , m , blockCol ); // Shared memory used to store Asub and Bsub respectively __shared__ float As [ BLOCK_SIZE ][ BLOCK_SIZE ]; __shared__ float Bs [ BLOCK_SIZE ][ BLOCK_SIZE ]; // Load Asub and Bsub from device memory to shared memory // Each thread loads one element of each sub-matrix As [ row ][ col ] = GetElement ( Asub , row , col ); Bs [ row ][ col ] = GetElement ( Bsub , row , col ); // Synchronize to make sure the sub-matrices are loaded // before starting the computation __syncthreads (); Loading data into shared memory blocks of matrix A and B is executed parellel by all threads within the block. 04.c. Give examples in the MatMulKernel kernel of sequential execution . // Multiply Asub and Bsub together for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; The computation of Cvalue for each thread is sequential, we execute BLOCK_SIZE additions and multiplications. On the other hand the computation of Cvalue is also a good example of fine-grained data parallelism, since there is one value computed by each thread in the block parallel. To identify fine-grained parallelism one just needs to look for block-level synchronization: for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; // Synchronize to make sure that the preceding // computation is done before loading two new // sub-matrices of A and B in the next iteration __syncthreads ();","title":"Week 01 - Exercises"},{"location":"archive/weeks/week01_exercises/#week-01-exercises","text":"","title":"Week 01 - Exercises"},{"location":"archive/weeks/week01_exercises/#exercise_01_01","text":"","title":"Exercise_01_01"},{"location":"archive/weeks/week01_exercises/#question","text":"01. Question: raw to digi conversion kernel Find the kernel that converts raw data to digis. Where is it launched? What is the execution configuration? How do we access individual threads in the kernel? To search the source code use CMSSW dxr - software cross-reference .","title":"Question"},{"location":"archive/weeks/week01_exercises/#solution","text":"01.a. Find the kernel that converts raw data to digis. // Kernel to perform Raw to Digi conversion __global__ void RawToDigi_kernel(const SiPixelROCsStatusAndMapping *cablingMap, const unsigned char *modToUnp, const uint32_t wordCounter, const uint32_t *word, const uint8_t *fedIds, uint16_t *xx, uint16_t *yy, uint16_t *adc, uint32_t *pdigi, ... 01.b. Where is it launched? It is launched in the makeClustersAsync function: void SiPixelRawToClusterGPUKernel :: makeClustersAsync ( bool isRun2 , const SiPixelClusterThresholds clusterThresholds , const SiPixelROCsStatusAndMapping * cablingMap , const unsigned char * modToUnp , const SiPixelGainForHLTonGPU * gains , const WordFedAppender & wordFed , SiPixelFormatterErrors && errors , const uint32_t wordCounter , ... // Launch rawToDigi kernel RawToDigi_kernel <<< blocks , threadsPerBlock , 0 , stream >>> ( cablingMap , modToUnp , wordCounter , word_d . get (), fedId_d . get (), digis_d . view (). xx (), digis_d . view (). yy (), digis_d . view (). adc (), ... 01.c. What is the execution configuration? For the RawToDigi_kernel he execution configuration is defined as <<< blocks , threadsPerBlock , 0 , stream >>> Where const int threadsPerBlock = 512 ; const int blocks = ( wordCounter + threadsPerBlock - 1 ) / threadsPerBlock ; // fill it all In this case 01.d. How do we access individual threads in the kernel? int32_t first = threadIdx . x + blockIdx . x * blockDim . x ;","title":"Solution"},{"location":"archive/weeks/week01_exercises/#exercise_01_02","text":"","title":"Exercise_01_02"},{"location":"archive/weeks/week01_exercises/#question_1","text":"02. Question: host and device functions Give an example of global , device and host-device functions in CMSSW . Can you find an example where host and device code diverge? How is this achieved?","title":"Question"},{"location":"archive/weeks/week01_exercises/#solution_1","text":"02.a. Give an example of global , device and host-device functions in CMSSW . For example see __global__ kernel in previous exercise . __device__ function in RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu : __device__ pixelgpudetails :: DetIdGPU getRawId ( const SiPixelROCsStatusAndMapping * cablingMap , uint8_t fed , uint32_t link , uint32_t roc ) { uint32_t index = fed * MAX_LINK * MAX_ROC + ( link - 1 ) * MAX_ROC + roc ; pixelgpudetails :: DetIdGPU detId = { cablingMap -> rawId [ index ], cablingMap -> rocInDet [ index ], cablingMap -> moduleId [ index ]}; return detId ; } __host__ __device__ function in HeterogeneousCore/CUDAUtilities/interface/OneToManyAssoc.h : __host__ __device__ __forceinline__ void add ( CountersOnly const & co ) { for ( int32_t i = 0 ; i < totOnes (); ++ i ) { #ifdef __CUDA_ARCH__ atomicAdd ( off . data () + i , co . off [ i ]); #else auto & a = ( std :: atomic < Counter > & )( off [ i ]); a += co . off [ i ]; #endif } } 02.b. Can you find an example where host and device code diverge? How is this achieved? In the CUDA C Programming Guide we can read that: The __device__ and __host__ execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. The __CUDA_ARCH__ macro introduced in Application Compatibility can be used to differentiate code paths between host and device: __host__ __device__ func () { #if __CUDA_ARCH__ >= 800 // Device code path for compute capability 8.x #elif __CUDA_ARCH__ >= 700 // Device code path for compute capability 7.x #elif __CUDA_ARCH__ >= 600 // Device code path for compute capability 6.x #elif __CUDA_ARCH__ >= 500 // Device code path for compute capability 5.x #elif __CUDA_ARCH__ >= 300 // Device code path for compute capability 3.x #elif !defined(__CUDA_ARCH__) // Host code path #endif } Based on this we can see how execution diverges in the previous add function: __host__ __device__ __forceinline__ void add ( CountersOnly const & co ) { for ( int32_t i = 0 ; i < totOnes (); ++ i ) { #ifdef __CUDA_ARCH__ atomicAdd ( off . data () + i , co . off [ i ]); #else auto & a = ( std :: atomic < Counter > & )( off [ i ]); a += co . off [ i ]; #endif } }","title":"Solution"},{"location":"archive/weeks/week01_exercises/#exercise_01_03","text":"","title":"Exercise_01_03"},{"location":"archive/weeks/week01_exercises/#exercise","text":"03. Exercise: Write a kernel in which if we're running on the device each thread prints which block and thread it is associated with, for example block 1 thread 3 if we're running on the host each thread just prints host . Test your program! How can you \"hide\" your GPU? Try using CUDA_VISIBLE_DEVICES from the command line.","title":"Exercise"},{"location":"archive/weeks/week01_exercises/#exercise_01_04","text":"","title":"Exercise_01_04"},{"location":"archive/weeks/week01_exercises/#exercise_1","text":"04. Exercise: Fine-grained vs coarse-grained parallelism: Give examples in the MatMulKernel kernel of coarse-grained and fine-grained data parallelism (as defined in CUDA abstraction model) as well as sequential execution. MatMulKernel Matrix definition GetElement and SetElement GetSubMatrix // Thread block size #define BLOCK_SIZE 16 __global__ void MatMulKernel ( Matrix A , Matrix B , Matrix C ) { // Block row and column int blockRow = blockIdx . y ; int blockCol = blockIdx . x ; // Each thread block computes one sub-matrix Csub of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol ); // Each thread computes one element of Csub // by accumulating results into Cvalue float Cvalue = 0 ; // Thread row and column within Csub int row = threadIdx . y ; int col = threadIdx . x ; // Loop over all the sub-matrices of A and B that are // required to compute Csub // Multiply each pair of sub-matrices together // and accumulate the results for ( int m = 0 ; m < ( A . width / BLOCK_SIZE ); ++ m ) { // Get sub-matrix Asub of A Matrix Asub = GetSubMatrix ( A , blockRow , m ); // Get sub-matrix Bsub of B Matrix Bsub = GetSubMatrix ( B , m , blockCol ); // Shared memory used to store Asub and Bsub respectively __shared__ float As [ BLOCK_SIZE ][ BLOCK_SIZE ]; __shared__ float Bs [ BLOCK_SIZE ][ BLOCK_SIZE ]; // Load Asub and Bsub from device memory to shared memory // Each thread loads one element of each sub-matrix As [ row ][ col ] = GetElement ( Asub , row , col ); Bs [ row ][ col ] = GetElement ( Bsub , row , col ); // Synchronize to make sure the sub-matrices are loaded // before starting the computation __syncthreads (); // Multiply Asub and Bsub together for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; // Synchronize to make sure that the preceding // computation is done before loading two new // sub-matrices of A and B in the next iteration __syncthreads (); } // Write Csub to device memory // Each thread writes one element SetElement ( Csub , row , col , Cvalue ); } // Matrices are stored in row-major order: // M(row, col) = *(M.elements + row * M.stride + col) typedef struct { int width ; int height ; int stride ; float * elements ; } Matrix ; // Get a matrix element __device__ float GetElement ( const Matrix A , int row , int col ) { return A . elements [ row * A . stride + col ]; } // Set a matrix element __device__ void SetElement ( Matrix A , int row , int col , float value ) { A . elements [ row * A . stride + col ] = value ; } // Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is // located col sub-matrices to the right and row sub-matrices down // from the upper-left corner of A __device__ Matrix GetSubMatrix ( Matrix A , int row , int col ) { Matrix Asub ; Asub . width = BLOCK_SIZE ; Asub . height = BLOCK_SIZE ; Asub . stride = A . stride ; Asub . elements = & A . elements [ A . stride * BLOCK_SIZE * row + BLOCK_SIZE * col ]; return Asub ; }","title":"Exercise"},{"location":"archive/weeks/week01_exercises/#solution_2","text":"04.a. Give examples in the MatMulKernel kernel of coarse-grained data parallelism . Coarse-grained data parallel problems in the CUDA programming model are problems that can be solved independently in parallel by blocks of threads. For example in the MatMulKernel : // Block row and column int blockRow = blockIdx . y ; int blockCol = blockIdx . x ; // Each thread block computes one sub-matrix Csub of C Matrix Csub = GetSubMatrix ( C , blockRow , blockCol ); The computation of Csub is independent of the computation of other submatrices of C . The work is divided between blocks , no synchronization is performed between computing different submatrices of C . 04.b. Give examples in the MatMulKernel kernel of fine-grained data parallelism . Fine-grained data parallel problems in the CUDA programming model are finer pieces that can be solved cooperatively in parallel by all threads within the block. For example in the MatMulKernel : // Get sub-matrix Asub of A Matrix Asub = GetSubMatrix ( A , blockRow , m ); // Get sub-matrix Bsub of B Matrix Bsub = GetSubMatrix ( B , m , blockCol ); // Shared memory used to store Asub and Bsub respectively __shared__ float As [ BLOCK_SIZE ][ BLOCK_SIZE ]; __shared__ float Bs [ BLOCK_SIZE ][ BLOCK_SIZE ]; // Load Asub and Bsub from device memory to shared memory // Each thread loads one element of each sub-matrix As [ row ][ col ] = GetElement ( Asub , row , col ); Bs [ row ][ col ] = GetElement ( Bsub , row , col ); // Synchronize to make sure the sub-matrices are loaded // before starting the computation __syncthreads (); Loading data into shared memory blocks of matrix A and B is executed parellel by all threads within the block. 04.c. Give examples in the MatMulKernel kernel of sequential execution . // Multiply Asub and Bsub together for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; The computation of Cvalue for each thread is sequential, we execute BLOCK_SIZE additions and multiplications. On the other hand the computation of Cvalue is also a good example of fine-grained data parallelism, since there is one value computed by each thread in the block parallel. To identify fine-grained parallelism one just needs to look for block-level synchronization: for ( int e = 0 ; e < BLOCK_SIZE ; ++ e ) Cvalue += As [ row ][ e ] * Bs [ e ][ col ]; // Synchronize to make sure that the preceding // computation is done before loading two new // sub-matrices of A and B in the next iteration __syncthreads ();","title":"Solution"},{"location":"archive/weeks/week02/","text":"Week 02 - 2021.12.13-17. Resources CUDA C++ Programming Guide Introduction to GPUs by New York University NVidia Developer Blog: Using Shared Memory CUDA C/C++ In the previous material in A Scalable Programming Model we've been reading about the three key abstractions in the CUDA programming model: a hierarchy of thread groups shared memories barrier synchronization Thread hierarchy has been previously covered, and in this part shared memory and barrier synchronization follows. 1. Shared memory Memory hierarchy CUDA threads may access data from multiple memory spaces during their execution. Each thread has private local memory . Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. All threads have access to the same global memory . There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces, which we won't cover in detail here. For more information continue reading here. The global, constant, and texture memory spaces are persistent across kernel launches by the same application. __shared__ As detailed in Variable Memory Space Specifiers shared memory is allocated using the __shared__ memory space specifier. Shared memory is expected to be much faster than global memory. Static and dynamic Example from: https://github.com/NVIDIA-developer-blog/code-samples/blob/master/series/cuda-cpp/shared-memory/shared-memory.cu Usage of static and dynamic memory copyright staticReverse dynamicReverse main /* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved. * * Redistribution and use in source and binary forms, with or without * modification, are permitted provided that the following conditions * are met: * * Redistributions of source code must retain the above copyright * notice, this list of conditions and the following disclaimer. * * Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimer in the * documentation and/or other materials provided with the distribution. * * Neither the name of NVIDIA CORPORATION nor the names of its * contributors may be used to endorse or promote products derived * from this software without specific prior written permission. * * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. */ #include <stdio.h> __global__ void staticReverse ( int * d , int n ) { __shared__ int s [ 64 ]; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } __global__ void dynamicReverse ( int * d , int n ) { extern __shared__ int s []; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } int main ( void ) { const int n = 64 ; int a [ n ], r [ n ], d [ n ]; for ( int i = 0 ; i < n ; i ++ ) { a [ i ] = i ; r [ i ] = n - i -1 ; d [ i ] = 0 ; } int * d_d ; cudaMalloc ( & d_d , n * sizeof ( int )); // run version with static shared memory cudaMemcpy ( d_d , a , n * sizeof ( int ), cudaMemcpyHostToDevice ); staticReverse <<< 1 , n >>> ( d_d , n ); cudaMemcpy ( d , d_d , n * sizeof ( int ), cudaMemcpyDeviceToHost ); for ( int i = 0 ; i < n ; i ++ ) if ( d [ i ] != r [ i ]) printf ( \"Error: d[%d]!=r[%d] (%d, %d) \\n \" , i , i , d [ i ], r [ i ]); // run dynamic shared memory version cudaMemcpy ( d_d , a , n * sizeof ( int ), cudaMemcpyHostToDevice ); dynamicReverse <<< 1 , n , n * sizeof ( int ) >>> ( d_d , n ); cudaMemcpy ( d , d_d , n * sizeof ( int ), cudaMemcpyDeviceToHost ); for ( int i = 0 ; i < n ; i ++ ) if ( d [ i ] != r [ i ]) printf ( \"Error: d[%d]!=r[%d] (%d, %d) \\n \" , i , i , d [ i ], r [ i ]); } If the shared memory array size is known at compile time , as in the staticReverse kernel, then we can explicitly declare an array of that size, as we do with the array s . __global__ void staticReverse ( int * d , int n ) { __shared__ int s [ 64 ]; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } The other three kernels in this example use dynamically allocated shared memory, which can be used when the amount of shared memory is not known at compile time . In this case the shared memory allocation size per thread block must be specified (in bytes) using an optional third execution configuration parameter, as in the following excerpt. dynamicReverse <<< 1 , n , n * sizeof ( int ) >>> ( d_d , n ); The dynamic shared memory kernel, dynamicReverse(), declares the shared memory array using an unsized extern array syntax, extern __shared__ int s[] . __global__ void dynamicReverse ( int * d , int n ) { extern __shared__ int s []; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } 2. Barrier synchronization Introduction When sharing data between threads, we need to be careful to avoid race conditions, because while threads in a block run logically in parallel, not all threads can execute physically at the same time. Synchronize block of threads Let\u2019s say that two threads A and B each load a data element from global memory and store it to shared memory. Then, thread A wants to read B\u2019s element from shared memory, and vice versa. Let\u2019s assume that A and B are threads in two different warps. If B has not finished writing its element before A tries to read it, we have a race condition, which can lead to undefined behavior and incorrect results. To ensure correct results when parallel threads cooperate, we must synchronize the threads. CUDA provides a simple barrier synchronization primitive, __syncthreads() . A thread\u2019s execution can only proceed past a __syncthreads() after all threads in its block have executed the __syncthreads() . Thus, we can avoid the race condition described above by calling __syncthreads() after the store to shared memory and before any threads load from shared memory. Deadlocks barrier synchronization deadlock correct barrier int s = threadIdx . x / 2 ; if ( threadIdx . x > s ) { value [ threadIdx . x ] = 2 * s ; __syncthreads (); } else { value [ threadIdx . x ] = s ; __syncthreads (); } int s = threadIdx . x / 2 ; if ( threadIdx . x > s ) { value [ threadIdx . x ] = 2 * s ; } else { value [ threadIdx . x ] = s ; } __syncthreads (); It\u2019s important to be aware that calling __syncthreads() in divergent code is undefined and can lead to deadlock\u2014all threads within a thread block must call __syncthreads() at the same point. 3. Page-Locked Host Memory From the CUDA Programming Guide The runtime provides functions to allow the use of page-locked (also known as pinned) host memory (as opposed to regular pageable host memory allocated by malloc()): cudaHostAlloc() and cudaFreeHost() allocate and free page-locked host memory; cudaHostRegister() page-locks a range of memory allocated by malloc() (see reference manual for limitations). Using page-locked host memory has several benefits: Copies between page-locked host memory and device memory can be performed concurrently with kernel execution for some devices as mentioned in Asynchronous Concurrent Execution. On some devices, page-locked host memory can be mapped into the address space of the device, eliminating the need to copy it to or from device memory as detailed in Mapped Memory. On systems with a front-side bus, bandwidth between host memory and device memory is higher if host memory is allocated as page-locked and even higher if in addition it is allocated as write-combining as described in Write-Combining Memory. Explaining pinned memory : Setting flags for cudaHostAlloc :","title":"Week 02 - 2021.12.13-17."},{"location":"archive/weeks/week02/#week-02-20211213-17","text":"Resources CUDA C++ Programming Guide Introduction to GPUs by New York University NVidia Developer Blog: Using Shared Memory CUDA C/C++ In the previous material in A Scalable Programming Model we've been reading about the three key abstractions in the CUDA programming model: a hierarchy of thread groups shared memories barrier synchronization Thread hierarchy has been previously covered, and in this part shared memory and barrier synchronization follows.","title":"Week 02 - 2021.12.13-17."},{"location":"archive/weeks/week02/#1-shared-memory","text":"","title":"1. Shared memory"},{"location":"archive/weeks/week02/#memory-hierarchy","text":"CUDA threads may access data from multiple memory spaces during their execution. Each thread has private local memory . Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. All threads have access to the same global memory . There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces, which we won't cover in detail here. For more information continue reading here. The global, constant, and texture memory spaces are persistent across kernel launches by the same application.","title":"Memory hierarchy"},{"location":"archive/weeks/week02/#__shared__","text":"As detailed in Variable Memory Space Specifiers shared memory is allocated using the __shared__ memory space specifier. Shared memory is expected to be much faster than global memory.","title":"__shared__"},{"location":"archive/weeks/week02/#static-and-dynamic","text":"Example from: https://github.com/NVIDIA-developer-blog/code-samples/blob/master/series/cuda-cpp/shared-memory/shared-memory.cu Usage of static and dynamic memory copyright staticReverse dynamicReverse main /* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved. * * Redistribution and use in source and binary forms, with or without * modification, are permitted provided that the following conditions * are met: * * Redistributions of source code must retain the above copyright * notice, this list of conditions and the following disclaimer. * * Redistributions in binary form must reproduce the above copyright * notice, this list of conditions and the following disclaimer in the * documentation and/or other materials provided with the distribution. * * Neither the name of NVIDIA CORPORATION nor the names of its * contributors may be used to endorse or promote products derived * from this software without specific prior written permission. * * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. */ #include <stdio.h> __global__ void staticReverse ( int * d , int n ) { __shared__ int s [ 64 ]; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } __global__ void dynamicReverse ( int * d , int n ) { extern __shared__ int s []; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } int main ( void ) { const int n = 64 ; int a [ n ], r [ n ], d [ n ]; for ( int i = 0 ; i < n ; i ++ ) { a [ i ] = i ; r [ i ] = n - i -1 ; d [ i ] = 0 ; } int * d_d ; cudaMalloc ( & d_d , n * sizeof ( int )); // run version with static shared memory cudaMemcpy ( d_d , a , n * sizeof ( int ), cudaMemcpyHostToDevice ); staticReverse <<< 1 , n >>> ( d_d , n ); cudaMemcpy ( d , d_d , n * sizeof ( int ), cudaMemcpyDeviceToHost ); for ( int i = 0 ; i < n ; i ++ ) if ( d [ i ] != r [ i ]) printf ( \"Error: d[%d]!=r[%d] (%d, %d) \\n \" , i , i , d [ i ], r [ i ]); // run dynamic shared memory version cudaMemcpy ( d_d , a , n * sizeof ( int ), cudaMemcpyHostToDevice ); dynamicReverse <<< 1 , n , n * sizeof ( int ) >>> ( d_d , n ); cudaMemcpy ( d , d_d , n * sizeof ( int ), cudaMemcpyDeviceToHost ); for ( int i = 0 ; i < n ; i ++ ) if ( d [ i ] != r [ i ]) printf ( \"Error: d[%d]!=r[%d] (%d, %d) \\n \" , i , i , d [ i ], r [ i ]); } If the shared memory array size is known at compile time , as in the staticReverse kernel, then we can explicitly declare an array of that size, as we do with the array s . __global__ void staticReverse ( int * d , int n ) { __shared__ int s [ 64 ]; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; } The other three kernels in this example use dynamically allocated shared memory, which can be used when the amount of shared memory is not known at compile time . In this case the shared memory allocation size per thread block must be specified (in bytes) using an optional third execution configuration parameter, as in the following excerpt. dynamicReverse <<< 1 , n , n * sizeof ( int ) >>> ( d_d , n ); The dynamic shared memory kernel, dynamicReverse(), declares the shared memory array using an unsized extern array syntax, extern __shared__ int s[] . __global__ void dynamicReverse ( int * d , int n ) { extern __shared__ int s []; int t = threadIdx . x ; int tr = n - t -1 ; s [ t ] = d [ t ]; __syncthreads (); d [ t ] = s [ tr ]; }","title":"Static and dynamic"},{"location":"archive/weeks/week02/#2-barrier-synchronization","text":"","title":"2. Barrier synchronization"},{"location":"archive/weeks/week02/#introduction","text":"When sharing data between threads, we need to be careful to avoid race conditions, because while threads in a block run logically in parallel, not all threads can execute physically at the same time.","title":"Introduction"},{"location":"archive/weeks/week02/#synchronize-block-of-threads","text":"Let\u2019s say that two threads A and B each load a data element from global memory and store it to shared memory. Then, thread A wants to read B\u2019s element from shared memory, and vice versa. Let\u2019s assume that A and B are threads in two different warps. If B has not finished writing its element before A tries to read it, we have a race condition, which can lead to undefined behavior and incorrect results. To ensure correct results when parallel threads cooperate, we must synchronize the threads. CUDA provides a simple barrier synchronization primitive, __syncthreads() . A thread\u2019s execution can only proceed past a __syncthreads() after all threads in its block have executed the __syncthreads() . Thus, we can avoid the race condition described above by calling __syncthreads() after the store to shared memory and before any threads load from shared memory.","title":"Synchronize block of threads"},{"location":"archive/weeks/week02/#deadlocks","text":"barrier synchronization deadlock correct barrier int s = threadIdx . x / 2 ; if ( threadIdx . x > s ) { value [ threadIdx . x ] = 2 * s ; __syncthreads (); } else { value [ threadIdx . x ] = s ; __syncthreads (); } int s = threadIdx . x / 2 ; if ( threadIdx . x > s ) { value [ threadIdx . x ] = 2 * s ; } else { value [ threadIdx . x ] = s ; } __syncthreads (); It\u2019s important to be aware that calling __syncthreads() in divergent code is undefined and can lead to deadlock\u2014all threads within a thread block must call __syncthreads() at the same point.","title":"Deadlocks"},{"location":"archive/weeks/week02/#3-page-locked-host-memory","text":"From the CUDA Programming Guide The runtime provides functions to allow the use of page-locked (also known as pinned) host memory (as opposed to regular pageable host memory allocated by malloc()): cudaHostAlloc() and cudaFreeHost() allocate and free page-locked host memory; cudaHostRegister() page-locks a range of memory allocated by malloc() (see reference manual for limitations). Using page-locked host memory has several benefits: Copies between page-locked host memory and device memory can be performed concurrently with kernel execution for some devices as mentioned in Asynchronous Concurrent Execution. On some devices, page-locked host memory can be mapped into the address space of the device, eliminating the need to copy it to or from device memory as detailed in Mapped Memory. On systems with a front-side bus, bandwidth between host memory and device memory is higher if host memory is allocated as page-locked and even higher if in addition it is allocated as write-combining as described in Write-Combining Memory. Explaining pinned memory : Setting flags for cudaHostAlloc :","title":"3. Page-Locked Host Memory"},{"location":"archive/weeks/week02_exercises/","text":"Week 02 - Exercises Follow exercises from Patatrack Knowledge Transfer part 1","title":"Week 02 - Exercises"},{"location":"archive/weeks/week02_exercises/#week-02-exercises","text":"Follow exercises from Patatrack Knowledge Transfer part 1","title":"Week 02 - Exercises"},{"location":"documentation/calibPixels/","text":"gpuCalibPixel.h - calibPixels The whole kernel: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 template < bool isRun2 > __global__ void calibDigis ( uint16_t * id , uint16_t const * __restrict__ x , uint16_t const * __restrict__ y , uint16_t * adc , SiPixelGainForHLTonGPU const * __restrict__ ped , int numElements , uint32_t * __restrict__ moduleStart , // just to zero first uint32_t * __restrict__ nClustersInModule , // just to zero them uint32_t * __restrict__ clusModuleStart // just to zero first ) { int first = blockDim . x * blockIdx . x + threadIdx . x ; // zero for next kernels... if ( 0 == first ) clusModuleStart [ 0 ] = moduleStart [ 0 ] = 0 ; for ( int i = first ; i < phase1PixelTopology :: numberOfModules ; i += gridDim . x * blockDim . x ) { nClustersInModule [ i ] = 0 ; } for ( int i = first ; i < numElements ; i += gridDim . x * blockDim . x ) { if ( invalidModuleId == id [ i ]) continue ; bool isDeadColumn = false , isNoisyColumn = false ; int row = x [ i ]; int col = y [ i ]; auto ret = ped -> getPedAndGain ( id [ i ], col , row , isDeadColumn , isNoisyColumn ); float pedestal = ret . first ; float gain = ret . second ; // float pedestal = 0; float gain = 1.; if ( isDeadColumn | isNoisyColumn ) { printf ( \"bad pixel at %d in %d \\n \" , i , id [ i ]); id [ i ] = invalidModuleId ; adc [ i ] = 0 ; } else { float vcal = float ( adc [ i ]) * gain - pedestal * gain ; if constexpr ( isRun2 ) { float conversionFactor = id [ i ] < 96 ? VCaltoElectronGain_L1 : VCaltoElectronGain ; float offset = id [ i ] < 96 ? VCaltoElectronOffset_L1 : VCaltoElectronOffset ; vcal = vcal * conversionFactor + offset ; } adc [ i ] = std :: max ( 100 , int ( vcal )); } } } 1. Init This part of the code has nothing to do with calibrating pixels. We're initialising clusModuleStart and moduleStart as well as nClustersInModule . Even if we have effectively zero digis in this event we still need to call this kernel for these initialisations to happen. 14 15 16 17 18 19 // zero for next kernels... if ( 0 == first ) clusModuleStart [ 0 ] = moduleStart [ 0 ] = 0 ; for ( int i = first ; i < phase1PixelTopology :: numberOfModules ; i += gridDim . x * blockDim . x ) { nClustersInModule [ i ] = 0 ; } 2. ADC to VCAL Converting from ADC to VCAL : 29 30 31 auto ret = ped -> getPedAndGain ( id [ i ], col , row , isDeadColumn , isNoisyColumn ); float pedestal = ret . first ; float gain = ret . second ; ... 38 float vcal = float ( adc [ i ]) * gain - pedestal * gain ; Note that to determine the gain and pedestal values the inverse of them is measured. This is done by injecting different VCAL values to the detector and measuring the ADC response. From Offline calibrations and performance of the CMS pixel detector [#1] In the second step of the ADC-to-charge calibration, a polynomial of first degree is fit to the ADC vs charge measurements. The fit is performed in a restricted VCAL range to minimize the influence of the non-linearities at high charge. The resulting two parameters (gain and pedestal) are displayed in Fig. 2 as obtained in a calibration run from October 2009. The gain is the inverse slope and the pedestal is the offset in Fig. 1. The parameters are very stable and are determined about every four months for control purposes. Figure 1. Example ADC response as a function of injected charge in VCAL units (see text for conversion to electrons). The red line is a first degree polynomial fit to the data in a restricted VCAL range https://doi.org/10.1016/j.nima.2010.11.188 Figure 2. Distributions of the gain and pedestal constants for each pixel as obtained in a dedicated calibration run in October 2009. https://doi.org/10.1016/j.nima.2010.11.188 Some more recent slides from https://indico.cern.ch/event/914013/#10-gain-calibration-for-run3-p : 3. VCAL to electrons (charge) VCAL to charge conversion (up to Run2) 39 40 41 42 43 if constexpr ( isRun2 ) { float conversionFactor = id [ i ] < 96 ? VCaltoElectronGain_L1 : VCaltoElectronGain ; float offset = id [ i ] < 96 ? VCaltoElectronOffset_L1 : VCaltoElectronOffset ; vcal = vcal * conversionFactor + offset ; } From Offline calibrations and performance of the CMS pixel detector [#1] The ADC-to-charge calibration proceeds in two steps. First, in a dedicated standalone calibration run (3\u20136 hour duration, depending on the setup) of the pixel detector, all pixels are subject to charge injection from around 4000 electrons into the saturation regime (> 50000 electrons). ... The charge injection is controlled by a digital-to-analog-converter called VCAL. The relation between VCAL and injected charge Q in electrons, Q = 65.5\u00d7VCAL\u2212414, has been obtained from dedicated x-ray source calibrations with variable x-ray energies (17.44 keV from Mo, 22.10 keV from Ag, and 32.06 keV from Ba, excited from a primary Am source). To us, the linear relationship is relevant here Q = 65.5\u00d7VCAL\u2212414 . Note the difference between Run2 and afterwards Gain calibration has changed from Run3, for more information read the following resources: PRs: pixel mc gain calibration scheme: new GTs for MC Run3, modified Clusterizer conditions for Run3 #29333 Add SiPixelVCal DB object for pixel gain calibration #29829 Presentations: https://indico.cern.ch/event/879470/contributions/3796405/attachments/2009273/3356603/pix_off_25_3_gain_calibration_mc.pdf From this presentation: For the phase1 pixels the MC gain constants used to be: gains = 3.17 (0.055) pedestal = 16.3 (5.4) same for bpix & fpix. After the inclusion of the vcal calibration they become: gains = 149(2.6) for L1 158.6(2.8) pedestal = 16.7 (5.4) for L1 20.5(5.4) from Configuration.Eras.Modifier_run3_common_cff import run3_common run3_common . toModify ( siPixelClusters , VCaltoElectronGain = 1 , # all gains=1, pedestals=0 VCaltoElectronGain_L1 = 1 , # VCaltoElectronOffset = 0 , # VCaltoElectronOffset_L1 = 0 # ) Practically speaking, we're combining the two linear models, ADC to VCAL and VCAL to #electrons into 1 linear model. So afterwards, our VCAL to electron conversion is only defined for backwards compatibility, it is not used/executed, it doesn't do anything. ( slope=1, offset=0 ) 4. min electron cut minumum electron value becomes 100 44 adc [ i ] = std :: max ( 100 , int ( vcal )); This is also present in the legacy code. 5. Conclusion From the high level overview, we calculate the charge (#electrons) for some pixel hit. We receive an ADC value at a specific x , y coordinate in a specific module id[i] , perform the gain calibration ADC->VCAL and VCAL->electrons (or ADC->electrons ). Output #electrons in adc array We store the output, #electrons in the same storage that we had for input ADCs , the adc array. Citations [#1] Urs Langenegger, Offline calibrations and performance of the CMS pixel detector, Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, Volume 650, Issue 1, 2011, Pages 25-29, ISSN 0168-9002, https://doi.org/10.1016/j.nima.2010.11.188. (https://www.sciencedirect.com/science/article/pii/S0168900210027385) Abstract: The CMS pixel detector, divided into barrel and endcap subdetectors, has 66 million pixels. We present the offline algorithms and results for the gain/pedestal and Lorentz angle calibrations. The determination of the optimal clock delay settings with initial data is described. The expected detector performance from Monte Carlo simulations is compared to the real performance in 7TeV proton\u2013proton collisions. Keywords: Pixel detectors; Calibrations; CMS experiment; LHC detectors","title":"gpuCalibPixel.h - calibPixels"},{"location":"documentation/calibPixels/#gpucalibpixelh-calibpixels","text":"The whole kernel: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 template < bool isRun2 > __global__ void calibDigis ( uint16_t * id , uint16_t const * __restrict__ x , uint16_t const * __restrict__ y , uint16_t * adc , SiPixelGainForHLTonGPU const * __restrict__ ped , int numElements , uint32_t * __restrict__ moduleStart , // just to zero first uint32_t * __restrict__ nClustersInModule , // just to zero them uint32_t * __restrict__ clusModuleStart // just to zero first ) { int first = blockDim . x * blockIdx . x + threadIdx . x ; // zero for next kernels... if ( 0 == first ) clusModuleStart [ 0 ] = moduleStart [ 0 ] = 0 ; for ( int i = first ; i < phase1PixelTopology :: numberOfModules ; i += gridDim . x * blockDim . x ) { nClustersInModule [ i ] = 0 ; } for ( int i = first ; i < numElements ; i += gridDim . x * blockDim . x ) { if ( invalidModuleId == id [ i ]) continue ; bool isDeadColumn = false , isNoisyColumn = false ; int row = x [ i ]; int col = y [ i ]; auto ret = ped -> getPedAndGain ( id [ i ], col , row , isDeadColumn , isNoisyColumn ); float pedestal = ret . first ; float gain = ret . second ; // float pedestal = 0; float gain = 1.; if ( isDeadColumn | isNoisyColumn ) { printf ( \"bad pixel at %d in %d \\n \" , i , id [ i ]); id [ i ] = invalidModuleId ; adc [ i ] = 0 ; } else { float vcal = float ( adc [ i ]) * gain - pedestal * gain ; if constexpr ( isRun2 ) { float conversionFactor = id [ i ] < 96 ? VCaltoElectronGain_L1 : VCaltoElectronGain ; float offset = id [ i ] < 96 ? VCaltoElectronOffset_L1 : VCaltoElectronOffset ; vcal = vcal * conversionFactor + offset ; } adc [ i ] = std :: max ( 100 , int ( vcal )); } } }","title":"gpuCalibPixel.h - calibPixels"},{"location":"documentation/calibPixels/#1-init","text":"This part of the code has nothing to do with calibrating pixels. We're initialising clusModuleStart and moduleStart as well as nClustersInModule . Even if we have effectively zero digis in this event we still need to call this kernel for these initialisations to happen. 14 15 16 17 18 19 // zero for next kernels... if ( 0 == first ) clusModuleStart [ 0 ] = moduleStart [ 0 ] = 0 ; for ( int i = first ; i < phase1PixelTopology :: numberOfModules ; i += gridDim . x * blockDim . x ) { nClustersInModule [ i ] = 0 ; }","title":"1. Init"},{"location":"documentation/calibPixels/#2-adc-to-vcal","text":"Converting from ADC to VCAL : 29 30 31 auto ret = ped -> getPedAndGain ( id [ i ], col , row , isDeadColumn , isNoisyColumn ); float pedestal = ret . first ; float gain = ret . second ; ... 38 float vcal = float ( adc [ i ]) * gain - pedestal * gain ; Note that to determine the gain and pedestal values the inverse of them is measured. This is done by injecting different VCAL values to the detector and measuring the ADC response. From Offline calibrations and performance of the CMS pixel detector [#1] In the second step of the ADC-to-charge calibration, a polynomial of first degree is fit to the ADC vs charge measurements. The fit is performed in a restricted VCAL range to minimize the influence of the non-linearities at high charge. The resulting two parameters (gain and pedestal) are displayed in Fig. 2 as obtained in a calibration run from October 2009. The gain is the inverse slope and the pedestal is the offset in Fig. 1. The parameters are very stable and are determined about every four months for control purposes. Figure 1. Example ADC response as a function of injected charge in VCAL units (see text for conversion to electrons). The red line is a first degree polynomial fit to the data in a restricted VCAL range https://doi.org/10.1016/j.nima.2010.11.188 Figure 2. Distributions of the gain and pedestal constants for each pixel as obtained in a dedicated calibration run in October 2009. https://doi.org/10.1016/j.nima.2010.11.188 Some more recent slides from https://indico.cern.ch/event/914013/#10-gain-calibration-for-run3-p :","title":"2. ADC to VCAL"},{"location":"documentation/calibPixels/#3-vcal-to-electrons-charge","text":"VCAL to charge conversion (up to Run2) 39 40 41 42 43 if constexpr ( isRun2 ) { float conversionFactor = id [ i ] < 96 ? VCaltoElectronGain_L1 : VCaltoElectronGain ; float offset = id [ i ] < 96 ? VCaltoElectronOffset_L1 : VCaltoElectronOffset ; vcal = vcal * conversionFactor + offset ; } From Offline calibrations and performance of the CMS pixel detector [#1] The ADC-to-charge calibration proceeds in two steps. First, in a dedicated standalone calibration run (3\u20136 hour duration, depending on the setup) of the pixel detector, all pixels are subject to charge injection from around 4000 electrons into the saturation regime (> 50000 electrons). ... The charge injection is controlled by a digital-to-analog-converter called VCAL. The relation between VCAL and injected charge Q in electrons, Q = 65.5\u00d7VCAL\u2212414, has been obtained from dedicated x-ray source calibrations with variable x-ray energies (17.44 keV from Mo, 22.10 keV from Ag, and 32.06 keV from Ba, excited from a primary Am source). To us, the linear relationship is relevant here Q = 65.5\u00d7VCAL\u2212414 . Note the difference between Run2 and afterwards Gain calibration has changed from Run3, for more information read the following resources: PRs: pixel mc gain calibration scheme: new GTs for MC Run3, modified Clusterizer conditions for Run3 #29333 Add SiPixelVCal DB object for pixel gain calibration #29829 Presentations: https://indico.cern.ch/event/879470/contributions/3796405/attachments/2009273/3356603/pix_off_25_3_gain_calibration_mc.pdf From this presentation: For the phase1 pixels the MC gain constants used to be: gains = 3.17 (0.055) pedestal = 16.3 (5.4) same for bpix & fpix. After the inclusion of the vcal calibration they become: gains = 149(2.6) for L1 158.6(2.8) pedestal = 16.7 (5.4) for L1 20.5(5.4) from Configuration.Eras.Modifier_run3_common_cff import run3_common run3_common . toModify ( siPixelClusters , VCaltoElectronGain = 1 , # all gains=1, pedestals=0 VCaltoElectronGain_L1 = 1 , # VCaltoElectronOffset = 0 , # VCaltoElectronOffset_L1 = 0 # ) Practically speaking, we're combining the two linear models, ADC to VCAL and VCAL to #electrons into 1 linear model. So afterwards, our VCAL to electron conversion is only defined for backwards compatibility, it is not used/executed, it doesn't do anything. ( slope=1, offset=0 )","title":"3. VCAL to electrons (charge)"},{"location":"documentation/calibPixels/#4-min-electron-cut","text":"minumum electron value becomes 100 44 adc [ i ] = std :: max ( 100 , int ( vcal )); This is also present in the legacy code.","title":"4. min electron cut"},{"location":"documentation/calibPixels/#5-conclusion","text":"From the high level overview, we calculate the charge (#electrons) for some pixel hit. We receive an ADC value at a specific x , y coordinate in a specific module id[i] , perform the gain calibration ADC->VCAL and VCAL->electrons (or ADC->electrons ). Output #electrons in adc array We store the output, #electrons in the same storage that we had for input ADCs , the adc array.","title":"5. Conclusion"},{"location":"documentation/calibPixels/#citations","text":"[#1] Urs Langenegger, Offline calibrations and performance of the CMS pixel detector, Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, Volume 650, Issue 1, 2011, Pages 25-29, ISSN 0168-9002, https://doi.org/10.1016/j.nima.2010.11.188. (https://www.sciencedirect.com/science/article/pii/S0168900210027385) Abstract: The CMS pixel detector, divided into barrel and endcap subdetectors, has 66 million pixels. We present the offline algorithms and results for the gain/pedestal and Lorentz angle calibrations. The determination of the optimal clock delay settings with initial data is described. The expected detector performance from Monte Carlo simulations is compared to the real performance in 7TeV proton\u2013proton collisions. Keywords: Pixel detectors; Calibrations; CMS experiment; LHC detectors","title":"Citations"},{"location":"documentation/gpuClustering/","text":"gpuClustering.h - findClus Let's look at the histogram filling without the debug messages which we might explain later. // fill histo for ( int i = first ; i < msize ; i += blockDim . x ) { if ( id [ i ] == invalidModuleId ) // skip invalid pixels continue ; hist . count ( y [ i ]); } __syncthreads (); if ( threadIdx . x < 32 ) ws [ threadIdx . x ] = 0 ; // used by prefix scan... __syncthreads (); hist . finalize ( ws ); __syncthreads (); for ( int i = first ; i < msize ; i += blockDim . x ) { if ( id [ i ] == invalidModuleId ) // skip invalid pixels continue ; hist . fill ( y [ i ], i - firstPixel ); } Competitive filling We will fill our histogram hist with the values i-firstPixel . We make sure that each pixel get's in the right bin, corresponding to their column in the module map. hist . fill ( y [ i ], i - firstPixel ); Here as we know, y is the column value array of the digis. If we iterate through the histogram, pixels in the first column will be processed sooner to pixels in the next column, and so on. What we don't know however is what order we are going to process our pixels in one column/bin. Filling the histogram is competitive between the threads, the following image illustrates this. Figure 1 - Order in histogram Not to misunderstand, we don't fill our histogram in this order, this is the iteration order of cms::cuda::HistoContainer . This iteration order can change, and most probably will change from reconstruction to reconstruction. We see the relative positions of pixels in our cluster: Figure 2 - Our HistoContainer Our ordering will be defined as top to bottom inside bins and left to right between bins. At least for this example, it doesn't really matter if one imagines the order in one bin the other way around. Figure 3 - What we store in the HistoContainer On the left, we see how we will later iterate through the histogram. In the middle and on the right we see what we are actually storing in the histogram. We are storing i-firstPixel . This is the relative position of our digis in the digi view/array . We don't need all data about digis stored there, just their position in the digi array or digi view . Actually, not even that. We only need their relative position and not the absolute . That is because all digis belonging to the same module are consecutive in the digi array . This way we can save precious space, because we would need 32 bits to store the absolute position of a digi, however, this way we can use only 16 bits . Hmm, 16 bits means 2^16 = 65536 maximum possible relative positions. How do we know there are no more digis in the module? On one hand, it is very unlikely, since in phase 1 our module dimensions are 80*2*52*8 = 66560 . Module occupancy is much lower than 0.98 in any layer, so we're good. Still, we're making constraints on the maximum number of digis in the mdodule. Currently, this is //6000 max pixels required for HI operations with no measurable impact on pp performance constexpr uint32_t maxPixInModule = 6000 ; We actually don't use this here. We will use this for something else, namely iterating through the digis in the module. Why we will need this will be uncovered soon. This example only contained one cluster, but in reality we will likely have some consecutive clusters. Figure 4 - Multiple clusters in one module Again, in reality our cluster will be more spread out, we are only drawing them this close together for the sake of the example. Adding multiple clusters to the game the iteration order will change. Nearest neighbours and counting them A crucial part of the code is the following: // allocate space for duplicate pixels: a pixel can appear more than once // with different charge in the same event constexpr int maxNeighbours = 10 ; assert (( hist . size () / blockDim . x ) <= maxiter ); // nearest neighbour uint16_t nn [ maxiter ][ maxNeighbours ]; uint8_t nnn [ maxiter ]; // number of nn for ( uint32_t k = 0 ; k < maxiter ; ++ k ) nnn [ k ] = 0 ; __syncthreads (); // for hit filling! nn or nearest neighbours We want to store the relative position of the nearest neighbours of every digi. Basically, that's why we created the histogram in the first place. With the histogram the job is half-done, we know the neighbours of every digi columnwise. We will create local arrays to store the neighbours. We consider neighbours to be 8-connected . This would max out the number of possible neighbours to be, well, 8. But as the comment above (and below) explains, due to some read-out inconsistencies or whatnot we can have duplicate digis. So we allocate some extra space for them, but we'll also use some assertions later on to make sure we don't exceed our self-made limit. // allocate space for duplicate pixels: a pixel can appear more than once // with different charge in the same event constexpr int maxNeighbours = 10 ; How many digis? We finally get to answer why we have an upper limit on the number of digis that we get to check rigorously in our kernel. //6000 max pixels required for HI operations with no measurable impact on pp performance constexpr uint32_t maxPixInModule = 6000 ; It is connected to this: assert (( hist . size () / blockDim . x ) <= maxiter ); We want to store nearest neighbours for every digi/pixel, but we don't know in advance how many there are. But we do need to fix the number of threads in advance, that is compile time constant. nn is thread local, so what we will do, is make it two dimensional and let the threads iterate through the module digis, always increasing the position by blockDim.x , and store nearest neighbours of the next digi in the next row of the nn array. // nearest neighbour uint16_t nn [ maxiter ][ maxNeighbours ]; uint8_t nnn [ maxiter ]; // number of nn For example, if blockDim.x = 16 and we have 120 digis/hits in our event, then thread 3 will process the following digis: digi id nn place 3 -> nn[0] 19 -> nn[1] 35 -> nn[2] 51 -> nn[3] 67 -> nn[4] 83 -> nn[5] 99 -> nn[6] 115 -> nn[7] We must decide (or do we) the size of nn in compile time too, so that's why we have maxiter and this dangereous looking message: #ifdef __CUDA_ARCH__ // assume that we can cover the whole module with up to 16 blockDim.x-wide iterations constexpr int maxiter = 16 ; if ( threadIdx . x == 0 && ( hist . size () / blockDim . x ) >= maxiter ) printf ( \"THIS IS NOT SUPPOSED TO HAPPEN too many hits in module %d: %d for block size %d \\n \" , thisModuleId , hist . size (), blockDim . x ); #else auto maxiter = hist . size (); #endif It really isn't supposed to happen. Why? Why? What would happen in our code if this were true? threadIdx . x == 0 && ( hist . size () / blockDim . x ) >= maxiter Let's say hist.size() = 300 . Well, then thread 3 would try to put the nearest neighbours of the following digis in the following non-existing places: digi id nn place 259 -> nn[16] 275 -> nn[17] 291 -> nn[18] We really don't want to have out of bounds indexing errors. It could happen in theory, that's why we run simulations and try to find out are expected (max) occupancy in advance and set maxiter accordingly. nnn or number of nearest neighbours We will keep track of the number of nearest neighbours as well in a separate array. uint8_t nnn [ maxiter ]; // number of nn We could actually get rid of this and follow a different approach, can you find out how? How? No, but really, think about it I'm serious Ok, well. Technically, when you create nn // nearest neighbour uint16_t nn [ maxiter ][ maxNeighbours ]; You could do this: // nearest neighbour uint16_t nn [ maxiter ][ maxNeighbours + 1 ]; And save one field at the end of each row for a special value e.g. and initialize all values to numeric_limits<uint16_t>::max()-1 and later only iterate until we reach this value. This solution actually uses a bit more space 16 vs 8 bits and requires us to do some initialization. Filling nn Let's look at how we actually fill our nearest neighbours arrays: // fill NN for ( auto j = threadIdx . x , k = 0U ; j < hist . size (); j += blockDim . x , ++ k ) { assert ( k < maxiter ); auto p = hist . begin () + j ; auto i = * p + firstPixel ; assert ( id [ i ] != invalidModuleId ); assert ( id [ i ] == thisModuleId ); // same module int be = Hist :: bin ( y [ i ] + 1 ); auto e = hist . end ( be ); ++ p ; assert ( 0 == nnn [ k ]); for (; p < e ; ++ p ) { auto m = ( * p ) + firstPixel ; assert ( m != i ); assert ( int ( y [ m ]) - int ( y [ i ]) >= 0 ); assert ( int ( y [ m ]) - int ( y [ i ]) <= 1 ); if ( std :: abs ( int ( x [ m ]) - int ( x [ i ])) > 1 ) continue ; auto l = nnn [ k ] ++ ; assert ( l < maxNeighbours ); nn [ k ][ l ] = * p ; } } Current iteration, keeping track of k We will use k to keep track of which iteration we are currently in: for ( auto j = threadIdx . x , k = 0U ; j < hist . size (); j += blockDim . x , ++ k ) It shall not overflow maxiter assert ( k < maxiter ); There hasn't been any nearest neighbours added to nn[k] so our counter of them nnn[k] should be zero, we check this here: assert ( 0 == nnn [ k ]); When we find a neighbour, we add it to nn[k] : auto l = nnn [ k ] ++ ; assert ( l < maxNeighbours ); nn [ k ][ l ] = * p ; We also check that our index l is within bounds. Pointer to hist element p We we look at the j th element in the histogram and set the pointer p to this element, i will be the absolute position of our digi. auto p = hist . begin () + j ; auto i = * p + firstPixel ; We made sure of these conditions when we created and filled the histogram assert ( id [ i ] != invalidModuleId ); assert ( id [ i ] == thisModuleId ); // same module Let's find the pointer to the last element in the next bin, this will be e , probably short for end . Also, we increase p by one so we only start considering digis that come after p int be = Hist :: bin ( y [ i ] + 1 ); auto e = hist . end ( be ); ++ p ; m , or possible neighbours Finally we iterate over elements from p++ until e for (; p < e ; ++ p ) { auto m = ( * p ) + firstPixel ; assert ( m != i ); ... } We know that our column is correct: assert ( int ( y [ m ]) - int ( y [ i ]) >= 0 ); assert ( int ( y [ m ]) - int ( y [ i ]) <= 1 ); So we only need to check whether our row value is <=1 if ( std :: abs ( int ( x [ m ]) - int ( x [ i ])) > 1 ) continue ; If our row is within bounds, we add m to nn . Example In this example: For m we consider the following values i - firstPixel *(++p) m 4 -> 1 9 13 12 5 2 16 1 -> 9 13 12 5 2 26 9 -> 13 12 5 2 16 13 -> 12 5 2 16 And for the nearest neighbours we get: i - firstPixel nn values 4 -> 9 12 5 1 -> 9 9 -> 12 13 -> 2 Assign same clusterId to clusters Essentially, the following piece of code assigns the same clusterId to all pixels/digis in a cluster. These clusterId s won't be ordered, start from 0 , but they will be the same in for neighbouring pixels. They will also be in the range 0 to numElements , which is the maximum number of digis for this particular event. bool more = true ; int nloops = 0 ; while ( __syncthreads_or ( more )) { if ( 1 == nloops % 2 ) { for ( auto j = threadIdx . x , k = 0U ; j < hist . size (); j += blockDim . x , ++ k ) { auto p = hist . begin () + j ; auto i = * p + firstPixel ; auto m = clusterId [ i ]; while ( m != clusterId [ m ]) m = clusterId [ m ]; clusterId [ i ] = m ; } } else { more = false ; for ( auto j = threadIdx . x , k = 0U ; j < hist . size (); j += blockDim . x , ++ k ) { auto p = hist . begin () + j ; auto i = * p + firstPixel ; for ( int kk = 0 ; kk < nnn [ k ]; ++ kk ) { auto l = nn [ k ][ kk ]; auto m = l + firstPixel ; assert ( m != i ); auto old = atomicMin_block ( & clusterId [ m ], clusterId [ i ]); // do we need memory fence? if ( old != clusterId [ i ]) { // end the loop only if no changes were applied more = true ; } atomicMin_block ( & clusterId [ i ], old ); } // nnloop } // pixel loop } ++ nloops ; } // end while We also get a bit of explanation, namely // for each pixel, look at all the pixels until the end of the module; // when two valid pixels within +/- 1 in x or y are found, set their id to the minimum; // after the loop, all the pixel in each cluster should have the id equeal to the lowest // pixel in the cluster ( clus[i] == i ). This is all true, but we don't see actually why though. We need to understand what nn and nnn and the hist is, and how everything is connected, and why our while loop is divided into two parts. So let's dig in. bool more = true ; int nloops = 0 ; more will be set to true every loop if we updated the cluterId for the current pixel, and so it will tell us to terminate our loop or not nloops or number of loops while ( __syncthreads_or ( more )) { One can reason intuitavely what this does, or can consult the CUDA C Programming Guide . int __syncthreads_or(int predicate); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them. So more , our local variable is scanned for every thread in the block. We terminate the loop if we didn't update any cluterId s in the previous iteration.","title":"gpuClustering.h - findClus"},{"location":"documentation/gpuClustering/#gpuclusteringh-findclus","text":"Let's look at the histogram filling without the debug messages which we might explain later. // fill histo for ( int i = first ; i < msize ; i += blockDim . x ) { if ( id [ i ] == invalidModuleId ) // skip invalid pixels continue ; hist . count ( y [ i ]); } __syncthreads (); if ( threadIdx . x < 32 ) ws [ threadIdx . x ] = 0 ; // used by prefix scan... __syncthreads (); hist . finalize ( ws ); __syncthreads (); for ( int i = first ; i < msize ; i += blockDim . x ) { if ( id [ i ] == invalidModuleId ) // skip invalid pixels continue ; hist . fill ( y [ i ], i - firstPixel ); } Competitive filling We will fill our histogram hist with the values i-firstPixel . We make sure that each pixel get's in the right bin, corresponding to their column in the module map. hist . fill ( y [ i ], i - firstPixel ); Here as we know, y is the column value array of the digis. If we iterate through the histogram, pixels in the first column will be processed sooner to pixels in the next column, and so on. What we don't know however is what order we are going to process our pixels in one column/bin. Filling the histogram is competitive between the threads, the following image illustrates this. Figure 1 - Order in histogram Not to misunderstand, we don't fill our histogram in this order, this is the iteration order of cms::cuda::HistoContainer . This iteration order can change, and most probably will change from reconstruction to reconstruction. We see the relative positions of pixels in our cluster: Figure 2 - Our HistoContainer Our ordering will be defined as top to bottom inside bins and left to right between bins. At least for this example, it doesn't really matter if one imagines the order in one bin the other way around. Figure 3 - What we store in the HistoContainer On the left, we see how we will later iterate through the histogram. In the middle and on the right we see what we are actually storing in the histogram. We are storing i-firstPixel . This is the relative position of our digis in the digi view/array . We don't need all data about digis stored there, just their position in the digi array or digi view . Actually, not even that. We only need their relative position and not the absolute . That is because all digis belonging to the same module are consecutive in the digi array . This way we can save precious space, because we would need 32 bits to store the absolute position of a digi, however, this way we can use only 16 bits . Hmm, 16 bits means 2^16 = 65536 maximum possible relative positions. How do we know there are no more digis in the module? On one hand, it is very unlikely, since in phase 1 our module dimensions are 80*2*52*8 = 66560 . Module occupancy is much lower than 0.98 in any layer, so we're good. Still, we're making constraints on the maximum number of digis in the mdodule. Currently, this is //6000 max pixels required for HI operations with no measurable impact on pp performance constexpr uint32_t maxPixInModule = 6000 ; We actually don't use this here. We will use this for something else, namely iterating through the digis in the module. Why we will need this will be uncovered soon. This example only contained one cluster, but in reality we will likely have some consecutive clusters. Figure 4 - Multiple clusters in one module Again, in reality our cluster will be more spread out, we are only drawing them this close together for the sake of the example. Adding multiple clusters to the game the iteration order will change.","title":"gpuClustering.h - findClus"},{"location":"documentation/gpuClustering/#nearest-neighbours-and-counting-them","text":"A crucial part of the code is the following: // allocate space for duplicate pixels: a pixel can appear more than once // with different charge in the same event constexpr int maxNeighbours = 10 ; assert (( hist . size () / blockDim . x ) <= maxiter ); // nearest neighbour uint16_t nn [ maxiter ][ maxNeighbours ]; uint8_t nnn [ maxiter ]; // number of nn for ( uint32_t k = 0 ; k < maxiter ; ++ k ) nnn [ k ] = 0 ; __syncthreads (); // for hit filling! nn or nearest neighbours We want to store the relative position of the nearest neighbours of every digi. Basically, that's why we created the histogram in the first place. With the histogram the job is half-done, we know the neighbours of every digi columnwise. We will create local arrays to store the neighbours. We consider neighbours to be 8-connected . This would max out the number of possible neighbours to be, well, 8. But as the comment above (and below) explains, due to some read-out inconsistencies or whatnot we can have duplicate digis. So we allocate some extra space for them, but we'll also use some assertions later on to make sure we don't exceed our self-made limit. // allocate space for duplicate pixels: a pixel can appear more than once // with different charge in the same event constexpr int maxNeighbours = 10 ; How many digis? We finally get to answer why we have an upper limit on the number of digis that we get to check rigorously in our kernel. //6000 max pixels required for HI operations with no measurable impact on pp performance constexpr uint32_t maxPixInModule = 6000 ; It is connected to this: assert (( hist . size () / blockDim . x ) <= maxiter ); We want to store nearest neighbours for every digi/pixel, but we don't know in advance how many there are. But we do need to fix the number of threads in advance, that is compile time constant. nn is thread local, so what we will do, is make it two dimensional and let the threads iterate through the module digis, always increasing the position by blockDim.x , and store nearest neighbours of the next digi in the next row of the nn array. // nearest neighbour uint16_t nn [ maxiter ][ maxNeighbours ]; uint8_t nnn [ maxiter ]; // number of nn For example, if blockDim.x = 16 and we have 120 digis/hits in our event, then thread 3 will process the following digis: digi id nn place 3 -> nn[0] 19 -> nn[1] 35 -> nn[2] 51 -> nn[3] 67 -> nn[4] 83 -> nn[5] 99 -> nn[6] 115 -> nn[7] We must decide (or do we) the size of nn in compile time too, so that's why we have maxiter and this dangereous looking message: #ifdef __CUDA_ARCH__ // assume that we can cover the whole module with up to 16 blockDim.x-wide iterations constexpr int maxiter = 16 ; if ( threadIdx . x == 0 && ( hist . size () / blockDim . x ) >= maxiter ) printf ( \"THIS IS NOT SUPPOSED TO HAPPEN too many hits in module %d: %d for block size %d \\n \" , thisModuleId , hist . size (), blockDim . x ); #else auto maxiter = hist . size (); #endif It really isn't supposed to happen. Why? Why? What would happen in our code if this were true? threadIdx . x == 0 && ( hist . size () / blockDim . x ) >= maxiter Let's say hist.size() = 300 . Well, then thread 3 would try to put the nearest neighbours of the following digis in the following non-existing places: digi id nn place 259 -> nn[16] 275 -> nn[17] 291 -> nn[18] We really don't want to have out of bounds indexing errors. It could happen in theory, that's why we run simulations and try to find out are expected (max) occupancy in advance and set maxiter accordingly. nnn or number of nearest neighbours We will keep track of the number of nearest neighbours as well in a separate array. uint8_t nnn [ maxiter ]; // number of nn We could actually get rid of this and follow a different approach, can you find out how? How? No, but really, think about it I'm serious Ok, well. Technically, when you create nn // nearest neighbour uint16_t nn [ maxiter ][ maxNeighbours ]; You could do this: // nearest neighbour uint16_t nn [ maxiter ][ maxNeighbours + 1 ]; And save one field at the end of each row for a special value e.g. and initialize all values to numeric_limits<uint16_t>::max()-1 and later only iterate until we reach this value. This solution actually uses a bit more space 16 vs 8 bits and requires us to do some initialization.","title":"Nearest neighbours and counting them"},{"location":"documentation/gpuClustering/#filling-nn","text":"Let's look at how we actually fill our nearest neighbours arrays: // fill NN for ( auto j = threadIdx . x , k = 0U ; j < hist . size (); j += blockDim . x , ++ k ) { assert ( k < maxiter ); auto p = hist . begin () + j ; auto i = * p + firstPixel ; assert ( id [ i ] != invalidModuleId ); assert ( id [ i ] == thisModuleId ); // same module int be = Hist :: bin ( y [ i ] + 1 ); auto e = hist . end ( be ); ++ p ; assert ( 0 == nnn [ k ]); for (; p < e ; ++ p ) { auto m = ( * p ) + firstPixel ; assert ( m != i ); assert ( int ( y [ m ]) - int ( y [ i ]) >= 0 ); assert ( int ( y [ m ]) - int ( y [ i ]) <= 1 ); if ( std :: abs ( int ( x [ m ]) - int ( x [ i ])) > 1 ) continue ; auto l = nnn [ k ] ++ ; assert ( l < maxNeighbours ); nn [ k ][ l ] = * p ; } } Current iteration, keeping track of k We will use k to keep track of which iteration we are currently in: for ( auto j = threadIdx . x , k = 0U ; j < hist . size (); j += blockDim . x , ++ k ) It shall not overflow maxiter assert ( k < maxiter ); There hasn't been any nearest neighbours added to nn[k] so our counter of them nnn[k] should be zero, we check this here: assert ( 0 == nnn [ k ]); When we find a neighbour, we add it to nn[k] : auto l = nnn [ k ] ++ ; assert ( l < maxNeighbours ); nn [ k ][ l ] = * p ; We also check that our index l is within bounds. Pointer to hist element p We we look at the j th element in the histogram and set the pointer p to this element, i will be the absolute position of our digi. auto p = hist . begin () + j ; auto i = * p + firstPixel ; We made sure of these conditions when we created and filled the histogram assert ( id [ i ] != invalidModuleId ); assert ( id [ i ] == thisModuleId ); // same module Let's find the pointer to the last element in the next bin, this will be e , probably short for end . Also, we increase p by one so we only start considering digis that come after p int be = Hist :: bin ( y [ i ] + 1 ); auto e = hist . end ( be ); ++ p ; m , or possible neighbours Finally we iterate over elements from p++ until e for (; p < e ; ++ p ) { auto m = ( * p ) + firstPixel ; assert ( m != i ); ... } We know that our column is correct: assert ( int ( y [ m ]) - int ( y [ i ]) >= 0 ); assert ( int ( y [ m ]) - int ( y [ i ]) <= 1 ); So we only need to check whether our row value is <=1 if ( std :: abs ( int ( x [ m ]) - int ( x [ i ])) > 1 ) continue ; If our row is within bounds, we add m to nn . Example In this example: For m we consider the following values i - firstPixel *(++p) m 4 -> 1 9 13 12 5 2 16 1 -> 9 13 12 5 2 26 9 -> 13 12 5 2 16 13 -> 12 5 2 16 And for the nearest neighbours we get: i - firstPixel nn values 4 -> 9 12 5 1 -> 9 9 -> 12 13 -> 2","title":"Filling nn"},{"location":"documentation/gpuClustering/#assign-same-clusterid-to-clusters","text":"Essentially, the following piece of code assigns the same clusterId to all pixels/digis in a cluster. These clusterId s won't be ordered, start from 0 , but they will be the same in for neighbouring pixels. They will also be in the range 0 to numElements , which is the maximum number of digis for this particular event. bool more = true ; int nloops = 0 ; while ( __syncthreads_or ( more )) { if ( 1 == nloops % 2 ) { for ( auto j = threadIdx . x , k = 0U ; j < hist . size (); j += blockDim . x , ++ k ) { auto p = hist . begin () + j ; auto i = * p + firstPixel ; auto m = clusterId [ i ]; while ( m != clusterId [ m ]) m = clusterId [ m ]; clusterId [ i ] = m ; } } else { more = false ; for ( auto j = threadIdx . x , k = 0U ; j < hist . size (); j += blockDim . x , ++ k ) { auto p = hist . begin () + j ; auto i = * p + firstPixel ; for ( int kk = 0 ; kk < nnn [ k ]; ++ kk ) { auto l = nn [ k ][ kk ]; auto m = l + firstPixel ; assert ( m != i ); auto old = atomicMin_block ( & clusterId [ m ], clusterId [ i ]); // do we need memory fence? if ( old != clusterId [ i ]) { // end the loop only if no changes were applied more = true ; } atomicMin_block ( & clusterId [ i ], old ); } // nnloop } // pixel loop } ++ nloops ; } // end while We also get a bit of explanation, namely // for each pixel, look at all the pixels until the end of the module; // when two valid pixels within +/- 1 in x or y are found, set their id to the minimum; // after the loop, all the pixel in each cluster should have the id equeal to the lowest // pixel in the cluster ( clus[i] == i ). This is all true, but we don't see actually why though. We need to understand what nn and nnn and the hist is, and how everything is connected, and why our while loop is divided into two parts. So let's dig in. bool more = true ; int nloops = 0 ; more will be set to true every loop if we updated the cluterId for the current pixel, and so it will tell us to terminate our loop or not nloops or number of loops while ( __syncthreads_or ( more )) { One can reason intuitavely what this does, or can consult the CUDA C Programming Guide . int __syncthreads_or(int predicate); is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them. So more , our local variable is scanned for every thread in the block. We terminate the loop if we didn't update any cluterId s in the previous iteration.","title":"Assign same clusterId to clusters"},{"location":"documentation/gpuClustering02/","text":"gpuClustering.h - countModules Our kernel function is the following: template < bool isPhase2 > __global__ void countModules ( uint16_t const * __restrict__ id , uint32_t * __restrict__ moduleStart , int32_t * __restrict__ clusterId , int numElements ) { int first = blockDim . x * blockIdx . x + threadIdx . x ; constexpr int nMaxModules = isPhase2 ? phase2PixelTopology :: numberOfModules : phase1PixelTopology :: numberOfModules ; assert ( nMaxModules < maxNumModules ); for ( int i = first ; i < numElements ; i += gridDim . x * blockDim . x ) { clusterId [ i ] = i ; if ( invalidModuleId == id [ i ]) continue ; auto j = i - 1 ; while ( j >= 0 and id [ j ] == invalidModuleId ) -- j ; if ( j < 0 or id [ j ] != id [ i ]) { // boundary... auto loc = atomicInc ( moduleStart , nMaxModules ); moduleStart [ loc + 1 ] = i ; } } }","title":"gpuClustering.h - countModules"},{"location":"documentation/gpuClustering02/#gpuclusteringh-countmodules","text":"Our kernel function is the following: template < bool isPhase2 > __global__ void countModules ( uint16_t const * __restrict__ id , uint32_t * __restrict__ moduleStart , int32_t * __restrict__ clusterId , int numElements ) { int first = blockDim . x * blockIdx . x + threadIdx . x ; constexpr int nMaxModules = isPhase2 ? phase2PixelTopology :: numberOfModules : phase1PixelTopology :: numberOfModules ; assert ( nMaxModules < maxNumModules ); for ( int i = first ; i < numElements ; i += gridDim . x * blockDim . x ) { clusterId [ i ] = i ; if ( invalidModuleId == id [ i ]) continue ; auto j = i - 1 ; while ( j >= 0 and id [ j ] == invalidModuleId ) -- j ; if ( j < 0 or id [ j ] != id [ i ]) { // boundary... auto loc = atomicInc ( moduleStart , nMaxModules ); moduleStart [ loc + 1 ] = i ; } } }","title":"gpuClustering.h - countModules"},{"location":"getting-started/","text":"Connecting to GPU machines A step-by-step guide on how to access GPU equipped machines at CERN, CMS or how to develop on your machine. Prerequisites CERN computing account Access machines at CERN See the CERN cloud insfrastructure resources guide on how to request GPU resources. lxplus The lxplus service offers lxplus-gpu.cern.ch for shared GPU instances - with limited isolation and performance. One can connect similary as would do to the lxplus.cern.ch host domain. ssh <username>@lxplus-gpu.cern.ch [-X] Access machines at CMS P5 This section is taken from the CMS TWiki TriggerDevelopmentWithGPUs page. Dedicated machines for the development of the online reconstruction There are 6 machines available for general development and validation of the online reconstruction on GPUs: gpu-c2a02-37-03.cms gpu-c2a02-37-04.cms gpu-c2a02-39-01.cms gpu-c2a02-39-02.cms gpu-c2a02-39-03.cms gpu-c2a02-39-04.cms All machines are equipped with two Intel \"Skylake\" Xeon Gold 6130 processors (for a total of 2x16=32 physical cores and 2x2x16 = 64 logical cores or hardware threads); 96 GB of RAM; one NVIDIA Tesla T4 GPU. How to connect To connect to these machines you need to have an online account and be in the gpudev group. To request access, please subscribe to the cms-hlt-gpu@cern.ch e-group and send an email to andrea.bocci@cern.ch , indicating whether you already have an online account; your online or lxplus username; your full name and email. Miscellaneous - or special GPU nodes This section is more or less taken from the Patatrack website systems subpage. cmg-gpu1080 System information Topology of the machine Getting access to the machine In order to get access to the machine you should send a request to subscribe to the CERN e-group: cms-gpu You should also send an email to Felice Pantaleo motivating the reason for the requested access. Usage Policy Normally, no more than 1 GPU per users should be used. To limit visible devices use export CUDA_VISIBLE_DEVICES=<list of numbers> Where <list of numbers> can be e.g. 0 , 0,4 , 1,2,3 . Use nvidia-smi to check available resources. Usage for ML studies If you need to use the machine for training DNNs you could accidentally occupy all the GPUs, making them unavailable for other users. For this reason you're kindly asked to use import setGPU before any import that will use a GPU (e.g. tensorflow). This will assign to you the least loaded GPU on the system. It is strictly forbidden to use GPUs from within your jupyter notebook. Please export your notebook to a python program and execute it. The access to the machine will be revoked when failing to comply to this rule.","title":"Connecting to GPU machines"},{"location":"getting-started/#connecting-to-gpu-machines","text":"A step-by-step guide on how to access GPU equipped machines at CERN, CMS or how to develop on your machine.","title":"Connecting to GPU machines"},{"location":"getting-started/#prerequisites","text":"CERN computing account","title":"Prerequisites"},{"location":"getting-started/#access-machines-at-cern","text":"See the CERN cloud insfrastructure resources guide on how to request GPU resources. lxplus The lxplus service offers lxplus-gpu.cern.ch for shared GPU instances - with limited isolation and performance. One can connect similary as would do to the lxplus.cern.ch host domain. ssh <username>@lxplus-gpu.cern.ch [-X]","title":"Access machines at CERN"},{"location":"getting-started/#access-machines-at-cms-p5","text":"This section is taken from the CMS TWiki TriggerDevelopmentWithGPUs page.","title":"Access machines at CMS P5"},{"location":"getting-started/#dedicated-machines-for-the-development-of-the-online-reconstruction","text":"There are 6 machines available for general development and validation of the online reconstruction on GPUs: gpu-c2a02-37-03.cms gpu-c2a02-37-04.cms gpu-c2a02-39-01.cms gpu-c2a02-39-02.cms gpu-c2a02-39-03.cms gpu-c2a02-39-04.cms","title":"Dedicated machines for the development of the online reconstruction"},{"location":"getting-started/#all-machines-are-equipped-with","text":"two Intel \"Skylake\" Xeon Gold 6130 processors (for a total of 2x16=32 physical cores and 2x2x16 = 64 logical cores or hardware threads); 96 GB of RAM; one NVIDIA Tesla T4 GPU.","title":"All machines are equipped with"},{"location":"getting-started/#how-to-connect","text":"To connect to these machines you need to have an online account and be in the gpudev group. To request access, please subscribe to the cms-hlt-gpu@cern.ch e-group and send an email to andrea.bocci@cern.ch , indicating whether you already have an online account; your online or lxplus username; your full name and email.","title":"How to connect"},{"location":"getting-started/#miscellaneous-or-special-gpu-nodes","text":"This section is more or less taken from the Patatrack website systems subpage.","title":"Miscellaneous - or special GPU nodes"},{"location":"getting-started/#cmg-gpu1080","text":"","title":"cmg-gpu1080"},{"location":"getting-started/#system-information","text":"Topology of the machine","title":"System information"},{"location":"getting-started/#getting-access-to-the-machine","text":"In order to get access to the machine you should send a request to subscribe to the CERN e-group: cms-gpu You should also send an email to Felice Pantaleo motivating the reason for the requested access.","title":"Getting access to the machine"},{"location":"getting-started/#usage-policy","text":"Normally, no more than 1 GPU per users should be used. To limit visible devices use export CUDA_VISIBLE_DEVICES=<list of numbers> Where <list of numbers> can be e.g. 0 , 0,4 , 1,2,3 . Use nvidia-smi to check available resources.","title":"Usage Policy"},{"location":"getting-started/#usage-for-ml-studies","text":"If you need to use the machine for training DNNs you could accidentally occupy all the GPUs, making them unavailable for other users. For this reason you're kindly asked to use import setGPU before any import that will use a GPU (e.g. tensorflow). This will assign to you the least loaded GPU on the system. It is strictly forbidden to use GPUs from within your jupyter notebook. Please export your notebook to a python program and execute it. The access to the machine will be revoked when failing to comply to this rule.","title":"Usage for ML studies"},{"location":"getting-started/software/","text":"General CMSSW developer guide Tutorial: proposing changes to CMSSW Based on http://cms-sw.github.io/tutorial.html Before you start Please make sure you registered to GitHub and that you have provided them a ssh public key to access your private repository. Search for available releases Before setting up a release search for available ones: scram list CMSSW This should result in some output like: Listing installed projects available for platform >> slc7_amd64_gcc10 << -------------------------------------------------------------------------------- | Project Name | Project Version | Project Location | -------------------------------------------------------------------------------- CMSSW CMSSW_12_0_0_pre3 --> /cvmfs/cms.cern.ch/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_0_0_pre3 CMSSW CMSSW_12_0_0_pre4 --> /cvmfs/cms.cern.ch/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_0_0_pre4 CMSSW CMSSW_12_0_0_pre5 --> /cvmfs/cms.cern.ch/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_0_0_pre5 ... CMSSW CMSSW_12_3_DBG_X_2022-02-10-2300 --> /cvmfs/cms-ib.cern.ch/week1/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_3_DBG_X_2022-02-10-2300 CMSSW CMSSW_12_3_SKYLAKEAVX512_X_2022-02-10-2300 --> /cvmfs/cms-ib.cern.ch/week1/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_3_SKYLAKEAVX512_X_2022-02-10-2300 CMSSW CMSSW_12_3_X_2022-02-11-1100 --> /cvmfs/cms-ib.cern.ch/week1/slc7_amd64_gcc10/cms/cmssw-patch/CMSSW_12_3_X_2022-02-11-1100 Create a CMSSW area Set up the work area just like you used to. cmsrel CMSSW_12_3_X_2022-02-11-1100 cd CMSSW_12_3_X_2022-02-11-1100/src cmsenv init git area git cms-init --upstream-only Checkout a few packages using git cms-addpkg Some useful packages for developing the pixel local reconstruction in CUDA : CUDADataFormats/SiPixelCluster/ CUDADataFormats/SiPixelDigi/ RecoLocalTracker/SiPixelClusterizer/ You can add these packages with cms-addpkg : (You need to be in the CMSSW_12_3_X_2022-02-11-1100/src repository) git cms-addpkg CUDADataFormats/SiPixelCluster/ git cms-addpkg CUDADataFormats/SiPixelDigi/ git cms-addpkg RecoLocalTracker/SiPixelClusterizer/ See the checked out packages in .git/info/sparse-checkout Output of cat .git/info/sparse-checkout /.clang-format /.clang-tidy /.gitignore /CUDADataFormats/SiPixelCluster/ /CUDADataFormats/SiPixelDigi/ /RecoLocalTracker/SiPixelClusterizer/ Only build the desired packages, add/remove unwanted ones: http://cms-sw.github.io/git-cms-addpkg.html Fetch updates from remote git checkout master git fetch [ official-cmssw ] git merge official-cmssw/master master Where official-cmssw is the remote name configured for the CMSSW offline software repository . You can list your remote repositories with (for even more verbose output, you can stack the v-s): git remote -v Conflict after PR To resolve a conflict that appeared after proposing the changes in a PR, one should prefer rebase to merge as it keeps the commit history clean. For a rebase, do: git checkout <development_branch_name> git fetch git rebase -i <CMSSW_current_release> It might be, that the tags for branches are only considered for your default remote , eg. my-cmssw, but the current release of CMSSW is not included in that. To resolve this, you can also try specifying the remote for the official cmssw (which is not a fork): git rebase -i <official_cmssw_name>/<CMSSW_current_release> Build release scram b -j Doing a DEBUG build for GPU development: USER_CXXFLAGS = \"-g -DGPU_DEBUG -DEDM_ML_DEBUG\" scram b -j If your debug build is not working, you might need to clean your development area: scram b clean Before making a PR scram b code-format # run clang-format scram b code-checks # run clang-tidy Build types: Stable and pre-releases Find out what's new in a particular release: https://cmssdt.cern.ch/SDT/ReleaseNotes/index.html . For example for CMSSW_12_3_0_pre2 Changes since CMSSW_12_3_0_pre1: compare to previous 36515 from @cms-tsg-storm: Revert EI removal in TSG tests hlt created: 2021-12-16 07:48:39 merged: 2021-12-16 09:48:40 36506 from @bsunanda: Run3-TB64 Make some simple changes to the classes in SimG4CMS/HcalTestBeam simulation created: 2021-12-15 15:29:55 merged: 2021-12-16 07:05:18 36505 from @kpedro88: restrict number of events for 0T PU workflows pdmv upgrade created: 2021-12-15 10:28:32 merged: 2021-12-15 14:50:12 etc. https://cmssdt.cern.ch/SDT/ReleaseNotes/CMSSW_12/CMSSW_12_3_0_pre2.html Integration builds Browse available integration builds, profiling and other test results: https://cmssdt.cern.ch/SDT/html/cmssdt-ib/ . Important features include the ability to filter IBs based on Flavor OS CPU architecture Compiler Look at latest commits, PRs added for a particluar build","title":"General CMSSW developer guide"},{"location":"getting-started/software/#general-cmssw-developer-guide","text":"","title":"General CMSSW developer guide"},{"location":"getting-started/software/#tutorial-proposing-changes-to-cmssw","text":"Based on http://cms-sw.github.io/tutorial.html","title":"Tutorial: proposing changes to CMSSW"},{"location":"getting-started/software/#before-you-start","text":"Please make sure you registered to GitHub and that you have provided them a ssh public key to access your private repository.","title":"Before you start"},{"location":"getting-started/software/#search-for-available-releases","text":"Before setting up a release search for available ones: scram list CMSSW This should result in some output like: Listing installed projects available for platform >> slc7_amd64_gcc10 << -------------------------------------------------------------------------------- | Project Name | Project Version | Project Location | -------------------------------------------------------------------------------- CMSSW CMSSW_12_0_0_pre3 --> /cvmfs/cms.cern.ch/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_0_0_pre3 CMSSW CMSSW_12_0_0_pre4 --> /cvmfs/cms.cern.ch/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_0_0_pre4 CMSSW CMSSW_12_0_0_pre5 --> /cvmfs/cms.cern.ch/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_0_0_pre5 ... CMSSW CMSSW_12_3_DBG_X_2022-02-10-2300 --> /cvmfs/cms-ib.cern.ch/week1/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_3_DBG_X_2022-02-10-2300 CMSSW CMSSW_12_3_SKYLAKEAVX512_X_2022-02-10-2300 --> /cvmfs/cms-ib.cern.ch/week1/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_3_SKYLAKEAVX512_X_2022-02-10-2300 CMSSW CMSSW_12_3_X_2022-02-11-1100 --> /cvmfs/cms-ib.cern.ch/week1/slc7_amd64_gcc10/cms/cmssw-patch/CMSSW_12_3_X_2022-02-11-1100","title":"Search for available releases"},{"location":"getting-started/software/#create-a-cmssw-area","text":"Set up the work area just like you used to. cmsrel CMSSW_12_3_X_2022-02-11-1100 cd CMSSW_12_3_X_2022-02-11-1100/src cmsenv","title":"Create a CMSSW area"},{"location":"getting-started/software/#init-git-area","text":"git cms-init --upstream-only","title":"init git area"},{"location":"getting-started/software/#checkout-a-few-packages-using-git-cms-addpkg","text":"Some useful packages for developing the pixel local reconstruction in CUDA : CUDADataFormats/SiPixelCluster/ CUDADataFormats/SiPixelDigi/ RecoLocalTracker/SiPixelClusterizer/ You can add these packages with cms-addpkg : (You need to be in the CMSSW_12_3_X_2022-02-11-1100/src repository) git cms-addpkg CUDADataFormats/SiPixelCluster/ git cms-addpkg CUDADataFormats/SiPixelDigi/ git cms-addpkg RecoLocalTracker/SiPixelClusterizer/ See the checked out packages in .git/info/sparse-checkout Output of cat .git/info/sparse-checkout /.clang-format /.clang-tidy /.gitignore /CUDADataFormats/SiPixelCluster/ /CUDADataFormats/SiPixelDigi/ /RecoLocalTracker/SiPixelClusterizer/ Only build the desired packages, add/remove unwanted ones: http://cms-sw.github.io/git-cms-addpkg.html Fetch updates from remote git checkout master git fetch [ official-cmssw ] git merge official-cmssw/master master Where official-cmssw is the remote name configured for the CMSSW offline software repository . You can list your remote repositories with (for even more verbose output, you can stack the v-s): git remote -v Conflict after PR To resolve a conflict that appeared after proposing the changes in a PR, one should prefer rebase to merge as it keeps the commit history clean. For a rebase, do: git checkout <development_branch_name> git fetch git rebase -i <CMSSW_current_release> It might be, that the tags for branches are only considered for your default remote , eg. my-cmssw, but the current release of CMSSW is not included in that. To resolve this, you can also try specifying the remote for the official cmssw (which is not a fork): git rebase -i <official_cmssw_name>/<CMSSW_current_release>","title":"Checkout a few packages using git cms-addpkg"},{"location":"getting-started/software/#build-release","text":"scram b -j Doing a DEBUG build for GPU development: USER_CXXFLAGS = \"-g -DGPU_DEBUG -DEDM_ML_DEBUG\" scram b -j If your debug build is not working, you might need to clean your development area: scram b clean","title":"Build release"},{"location":"getting-started/software/#before-making-a-pr","text":"scram b code-format # run clang-format scram b code-checks # run clang-tidy","title":"Before making a PR"},{"location":"getting-started/software/#build-types","text":"Stable and pre-releases Find out what's new in a particular release: https://cmssdt.cern.ch/SDT/ReleaseNotes/index.html . For example for CMSSW_12_3_0_pre2 Changes since CMSSW_12_3_0_pre1: compare to previous 36515 from @cms-tsg-storm: Revert EI removal in TSG tests hlt created: 2021-12-16 07:48:39 merged: 2021-12-16 09:48:40 36506 from @bsunanda: Run3-TB64 Make some simple changes to the classes in SimG4CMS/HcalTestBeam simulation created: 2021-12-15 15:29:55 merged: 2021-12-16 07:05:18 36505 from @kpedro88: restrict number of events for 0T PU workflows pdmv upgrade created: 2021-12-15 10:28:32 merged: 2021-12-15 14:50:12 etc. https://cmssdt.cern.ch/SDT/ReleaseNotes/CMSSW_12/CMSSW_12_3_0_pre2.html Integration builds Browse available integration builds, profiling and other test results: https://cmssdt.cern.ch/SDT/html/cmssdt-ib/ . Important features include the ability to filter IBs based on Flavor OS CPU architecture Compiler Look at latest commits, PRs added for a particluar build","title":"Build types:"},{"location":"getting-started/validation/","text":"Workflows Following is the evolution of GPU workflows in CMSSW with events ( PR s, presentations) more or less in chronological order. Early days Very early spots and historical versions can be discovered in the Patatrack fork of CMSSW . Some of the PR s concerned here are, but are not limited to (based on #31854 description ): Add workflows for Riemann fit and GPU cms-patatrack/cmssw#20 Add workflows for Riemann fit and GPU Add a DQM sequence for pixel-only tracking cms-patatrack/cmssw#23 Add a DQM sequence for pixel-only tracking Riemann fit gpu cms-patatrack/cmssw#60 Port the Riemann fit to CUDA Add pixel tracking workflows for data cms-patatrack/cmssw#144 Add pixel tracking workflows for data Renumber GPU workflows cms-patatrack/cmssw#259 Change GPU workflow numbering: .7->.51, .8->.52, .9->.53 Tracking developments for review and merging cms-patatrack/cmssw#338 Rework the Riemann fit and broken line fit Updating RelVal WF & customisation on CPU cms-patatrack/cmssw#549 Update the RelVal workflows and the CPU Towards integration in CMSSW 1. Add Patatrack process modifiers and workflows #28522 fwyzard commented on Dec 2, 2019 PR description: Add the pixelNtupleFit process modifier, that will be used to customise the Pixel-only tracks to use the ntuplet fit developed as part of the Patatrack workflows. Add the \"gpu\" process modifer, that can be used to enable offloading of available modules to run on GPUs. Add the first Patatrack workflows (currently just a placeholder). 2. Patatrack integration in CMSSW, A. Bocci, Reconstruction meeting, 2020.03.20. Relevant slide: Begin integration in CMSSW Patatrack integration - Pixel workflows (12/N) #31854 Merged silviodonato merged 54 commits into cms-sw:master from cms-patatrack:patatrack_integration_12_N_pixel_workflows on Apr 12, 2021 fwyzard commented on Oct 19, 2020 PR description: Update the runTheMatrix.py workflows for pixel-only tracking: ###.501 : pixel-only quadruplets on CPU ###.502 : pixel-only quadruplets on GPU ###.505 : pixel-only triplets on CPU ###.506 : pixel-only triplets on GPU Redesign Once the main PR originating from cms-patatrack/cms-sw has been merged, different redesigns happened: 1. Redesign all GPU workflows to detect if a GPU is present, and fall back to CPU otherwise #33428 Merged cmsbuild merged 11 commits into cms-sw:master from fwyzard:auto_gpu_workflows on May 11, 2021 fwyzard commented on Apr 14, 2021 PR description: Redesign the GPU workflows: the CPU (*e.g. ###.501) and GPU (###.502) workflows should now be as close as possible; the implementation of the CPU and GPU workflows has been simplified; all GPU workflows use the SwitchProducerCUDA mechanism to detect if a GPU is available and offload a module or task to the GPU; if not, they automatically fall back to the equivalent CPU modules and tasks; when the \"gpu\" modifier is used, the pixel local reconstruction workflow used the \"HLT\" payload type both on the CPU and on the GPU, for better consistency of the results; the \"Patatrack\" pixel tracks reconstruction on CPU is based on a modifier (pixelNtupletFit) instead of a customisation, in line with the other workflows; the HCAL-only workflows should follow more closely the implementation of the general reconstruction sequence, both for Run 2 (2018) and Run 3 scenarios. Some changes to the relevant EDProducers have made the definition of the workflows easier: the SoA-to-legacy HCAL rechit producer has been updated to make the production of the SoA and/or legacy collections optional; the legacy ECAL unpacker has been updated to declare only the event products it will actually produce; the default labels used in many modules have been updated to reflect the labels used in the configuration. Some other general changes and code clean up: remove some no-longer-used files as well as some commented-out code always clone() a module used in a SwitchProducerCUDA move the implementation of the gpuVertexFinder kernels from gpuVertexFinderImpl.h to gpuVertexFinder.cc The update has been presented here: https://indico.cern.ch/event/1033022/#47-gpu-workflows . 2. updated GPU workflows in CMSSW, A. Bocci, Reconstruction and Analysis Tools meeting, 2021.04.30. Relevant slides: 3. Update GPU workflows #35331 Merged cmsbuild merged 4 commits into cms-sw:master from fwyzard:update_GPU_workflows_121x on Sep 24, 2021 fwyzard commented on Sep 18, 2021 PR description: Add new GPU workflows, that run the Patatrack pixel local and pixel-only track reconstruction in addition to the full reconstruction, with the possibility of offloading to GPUs also the ECAL and HCAL local reconstruction. PR validation: Validated with runTheMatrix.py -w upgrade -j 4 -t 8 -l 11634 .591,11634.592,11634.595,11634.596 ... Running up to 4 concurrent jobs, each with 8 threads per process ... 11634 .591_TTbar_14TeV+2021_Patatrack_CPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST Step0-PASSED Step1-PASSED Step2-PASSED Step3-PASSED - time date Sat Sep 18 12 :26:05 2021 -date Sat Sep 18 12 :21:24 2021 ; exit: 0 0 0 0 11634 .592_TTbar_14TeV+2021_Patatrack_GPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST Step0-PASSED Step1-PASSED Step2-PASSED Step3-PASSED - time date Sat Sep 18 12 :26:12 2021 -date Sat Sep 18 12 :21:25 2021 ; exit: 0 0 0 0 11634 .595_TTbar_14TeV+2021_Patatrack_TripletsCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST Step0-PASSED Step1-PASSED Step2-PASSED Step3-PASSED - time date Sat Sep 18 12 :26:15 2021 -date Sat Sep 18 12 :21:25 2021 ; exit: 0 0 0 0 11634 .596_TTbar_14TeV+2021_Patatrack_TripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST Step0-PASSED Step1-PASSED Step2-PASSED Step3-PASSED - time date Sat Sep 18 12 :26:10 2021 -date Sat Sep 18 12 :21:26 2021 ; exit: 0 0 0 0 4 4 4 4 tests passed, 0 0 0 0 failed 4. Add workflows for profiling the GPU code #35540 Merged cmsbuild merged 2 commits into cms-sw:master from fwyzard:add_GPU_profiling_workflows on Oct 8, 2021 fwyzard commented on Oct 5, 2021 PR description: Add four workflows for profiling the GPU code: .504 Pixel-only local reconstruction and quadruplets .508 Pixel-only local reconstruction and triplets .514 ECAL-only local reconstruction .524 ECAL-only local reconstruction The workflows explicitly consume the GPU products, so they can only run on a GPU-equipped machine. The transfer to the host and the conversion to the legacy format is not run. PR validation: Used to profile the various workflows on top of CMSSW_12_1_0_pre3, runing over that release's TTbar relvals with pileup: measurement CMSSW_12_1_0_pre3 I/O throughput ~ 2 kev/s 11634.504 1071 \u00b1 3 ev/s 11634.508 560 \u00b1 2 ev/s 11634.514 1391 \u00b1 5 ev/s 11634.524 1354 \u00b1 10 ev/s Fixes 1. Fix the Patatrack pixel local reconstruction running on CPU #35915 Merged cmsbuild merged 1 commit into cms-sw:master from fwyzard:fix_hltSiPixelRecHitSoA_121x on Nov 1, 2021 fwyzard commented on Oct 29, 2021 PR description: Use the hltSiPixelRecHitSoA producer for the pixel rechits in legacy and SoA format, instead of running the legacy producer. PR validation: Successfully run the GPU workflow 11634.506. Current status runTheMatrix.py List available workflows for gpu: runTheMatrix.py -n --what gpu Example output: ignoring non-requested file relval_standard ignoring non-requested file relval_highstats ignoring non-requested file relval_pileup ignoring non-requested file relval_generator ignoring non-requested file relval_extendedgen ignoring non-requested file relval_production ignoring non-requested file relval_ged ignoring non-requested file relval_upgrade ignoring non-requested file relval_cleanedupgrade processing relval_gpu ignoring non-requested file relval_2017 ignoring non-requested file relval_2026 ignoring non-requested file relval_identity ignoring non-requested file relval_machine ignoring non-requested file relval_premix found a total of 22 workflows: 136 .885502 RunHLTPhy2018D+HLTDR2_2018+RECODR2_2018reHLT_Patatrack_PixelOnlyGPU+HARVEST2018_pixelTrackingOnly 136 .885512 RunHLTPhy2018D+HLTDR2_2018+RECODR2_2018reHLT_ECALOnlyGPU+HARVEST2018_ECALOnly 136 .885522 RunHLTPhy2018D+HLTDR2_2018+RECODR2_2018reHLT_HCALOnlyGPU+HARVEST2018_HCALOnly 136 .888502 RunJetHT2018D+HLTDR2_2018+RECODR2_2018reHLT_Patatrack_PixelOnlyGPU+HARVEST2018_pixelTrackingOnly 136 .888512 RunJetHT2018D+HLTDR2_2018+RECODR2_2018reHLT_ECALOnlyGPU+HARVEST2018_ECALOnly 136 .888522 RunJetHT2018D+HLTDR2_2018+RECODR2_2018reHLT_HCALOnlyGPU+HARVEST2018_HCALOnly 10824 .502 2018_Patatrack_PixelOnlyGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10824 .506 2018_Patatrack_PixelOnlyTripletsGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10824 .512 2018_Patatrack_ECALOnlyGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10824 .522 2018_Patatrack_HCALOnlyGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10824 .592 2018_Patatrack_GPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10824 .596 2018_Patatrack_TripletsGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10842 .502 2018_Patatrack_PixelOnlyGPU+ZMM_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10842 .506 2018_Patatrack_PixelOnlyTripletsGPU+ZMM_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 11634 .502 2021_Patatrack_PixelOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .506 2021_Patatrack_PixelOnlyTripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .512 2021_Patatrack_ECALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .522 2021_Patatrack_HCALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .592 2021_Patatrack_GPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .596 2021_Patatrack_TripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11650 .502 2021_Patatrack_PixelOnlyGPU+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11650 .506 2021_Patatrack_PixelOnlyTripletsGPU+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 22 workflows with 4 steps -------------------------------------------------------------------------------- Some of the workflows mentioned here, for example the profiling ones One can try finding these workflows by: runTheMatrix.py -n --what upgrade | grep Patatrack An example output may be: ... 11442 .595 2018DesignPU_Patatrack_TripletsCPU+ZMM_13TeV_TuneCUETP8M1_GenSim+DigiPU+RecoFakeHLTPU+HARVESTFakeHLTPU 11442 .596 2018DesignPU_Patatrack_TripletsGPU+ZMM_13TeV_TuneCUETP8M1_GenSim+DigiPU+RecoFakeHLTPU+HARVESTFakeHLTPU 11634 .501 2021_Patatrack_PixelOnlyCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .502 2021_Patatrack_PixelOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .504 2021_Patatrack_PixelOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco 11634 .505 2021_Patatrack_PixelOnlyTripletsCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .506 2021_Patatrack_PixelOnlyTripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .508 2021_Patatrack_PixelOnlyTripletsGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco 11634 .511 2021_Patatrack_ECALOnlyCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .512 2021_Patatrack_ECALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .514 2021_Patatrack_ECALOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco 11634 .521 2021_Patatrack_HCALOnlyCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .522 2021_Patatrack_HCALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .524 2021_Patatrack_HCALOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco 11634 .591 2021_Patatrack_CPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .592 2021_Patatrack_GPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .595 2021_Patatrack_TripletsCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .596 2021_Patatrack_TripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11650 .501 2021_Patatrack_PixelOnlyCPU+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11650 .502 2021_Patatrack_PixelOnlyGPU+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11650 .504 2021_Patatrack_PixelOnlyGPU_Profiling+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco ...","title":"Workflows"},{"location":"getting-started/validation/#workflows","text":"Following is the evolution of GPU workflows in CMSSW with events ( PR s, presentations) more or less in chronological order.","title":"Workflows"},{"location":"getting-started/validation/#early-days","text":"Very early spots and historical versions can be discovered in the Patatrack fork of CMSSW . Some of the PR s concerned here are, but are not limited to (based on #31854 description ): Add workflows for Riemann fit and GPU cms-patatrack/cmssw#20 Add workflows for Riemann fit and GPU Add a DQM sequence for pixel-only tracking cms-patatrack/cmssw#23 Add a DQM sequence for pixel-only tracking Riemann fit gpu cms-patatrack/cmssw#60 Port the Riemann fit to CUDA Add pixel tracking workflows for data cms-patatrack/cmssw#144 Add pixel tracking workflows for data Renumber GPU workflows cms-patatrack/cmssw#259 Change GPU workflow numbering: .7->.51, .8->.52, .9->.53 Tracking developments for review and merging cms-patatrack/cmssw#338 Rework the Riemann fit and broken line fit Updating RelVal WF & customisation on CPU cms-patatrack/cmssw#549 Update the RelVal workflows and the CPU","title":"Early days"},{"location":"getting-started/validation/#towards-integration-in-cmssw","text":"","title":"Towards integration in CMSSW"},{"location":"getting-started/validation/#1-add-patatrack-process-modifiers-and-workflows-28522","text":"fwyzard commented on Dec 2, 2019 PR description: Add the pixelNtupleFit process modifier, that will be used to customise the Pixel-only tracks to use the ntuplet fit developed as part of the Patatrack workflows. Add the \"gpu\" process modifer, that can be used to enable offloading of available modules to run on GPUs. Add the first Patatrack workflows (currently just a placeholder).","title":"1. Add Patatrack process modifiers and workflows #28522"},{"location":"getting-started/validation/#2-patatrack-integration-in-cmssw-a-bocci-reconstruction-meeting-20200320","text":"Relevant slide:","title":"2. Patatrack integration in CMSSW, A. Bocci, Reconstruction meeting, 2020.03.20."},{"location":"getting-started/validation/#begin-integration-in-cmssw","text":"","title":"Begin integration in CMSSW"},{"location":"getting-started/validation/#patatrack-integration-pixel-workflows-12n-31854","text":"Merged silviodonato merged 54 commits into cms-sw:master from cms-patatrack:patatrack_integration_12_N_pixel_workflows on Apr 12, 2021 fwyzard commented on Oct 19, 2020 PR description: Update the runTheMatrix.py workflows for pixel-only tracking: ###.501 : pixel-only quadruplets on CPU ###.502 : pixel-only quadruplets on GPU ###.505 : pixel-only triplets on CPU ###.506 : pixel-only triplets on GPU","title":"Patatrack integration - Pixel workflows (12/N) #31854"},{"location":"getting-started/validation/#redesign","text":"Once the main PR originating from cms-patatrack/cms-sw has been merged, different redesigns happened:","title":"Redesign"},{"location":"getting-started/validation/#1-redesign-all-gpu-workflows-to-detect-if-a-gpu-is-present-and-fall-back-to-cpu-otherwise-33428","text":"Merged cmsbuild merged 11 commits into cms-sw:master from fwyzard:auto_gpu_workflows on May 11, 2021 fwyzard commented on Apr 14, 2021 PR description: Redesign the GPU workflows: the CPU (*e.g. ###.501) and GPU (###.502) workflows should now be as close as possible; the implementation of the CPU and GPU workflows has been simplified; all GPU workflows use the SwitchProducerCUDA mechanism to detect if a GPU is available and offload a module or task to the GPU; if not, they automatically fall back to the equivalent CPU modules and tasks; when the \"gpu\" modifier is used, the pixel local reconstruction workflow used the \"HLT\" payload type both on the CPU and on the GPU, for better consistency of the results; the \"Patatrack\" pixel tracks reconstruction on CPU is based on a modifier (pixelNtupletFit) instead of a customisation, in line with the other workflows; the HCAL-only workflows should follow more closely the implementation of the general reconstruction sequence, both for Run 2 (2018) and Run 3 scenarios. Some changes to the relevant EDProducers have made the definition of the workflows easier: the SoA-to-legacy HCAL rechit producer has been updated to make the production of the SoA and/or legacy collections optional; the legacy ECAL unpacker has been updated to declare only the event products it will actually produce; the default labels used in many modules have been updated to reflect the labels used in the configuration. Some other general changes and code clean up: remove some no-longer-used files as well as some commented-out code always clone() a module used in a SwitchProducerCUDA move the implementation of the gpuVertexFinder kernels from gpuVertexFinderImpl.h to gpuVertexFinder.cc The update has been presented here: https://indico.cern.ch/event/1033022/#47-gpu-workflows .","title":"1. Redesign all GPU workflows to detect if a GPU is present, and fall back to CPU otherwise #33428"},{"location":"getting-started/validation/#2-updated-gpu-workflows-in-cmssw-a-bocci-reconstruction-and-analysis-tools-meeting-20210430","text":"Relevant slides:","title":"2. updated GPU workflows in CMSSW, A. Bocci, Reconstruction and Analysis Tools meeting, 2021.04.30."},{"location":"getting-started/validation/#3-update-gpu-workflows-35331","text":"Merged cmsbuild merged 4 commits into cms-sw:master from fwyzard:update_GPU_workflows_121x on Sep 24, 2021 fwyzard commented on Sep 18, 2021 PR description: Add new GPU workflows, that run the Patatrack pixel local and pixel-only track reconstruction in addition to the full reconstruction, with the possibility of offloading to GPUs also the ECAL and HCAL local reconstruction. PR validation: Validated with runTheMatrix.py -w upgrade -j 4 -t 8 -l 11634 .591,11634.592,11634.595,11634.596 ... Running up to 4 concurrent jobs, each with 8 threads per process ... 11634 .591_TTbar_14TeV+2021_Patatrack_CPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST Step0-PASSED Step1-PASSED Step2-PASSED Step3-PASSED - time date Sat Sep 18 12 :26:05 2021 -date Sat Sep 18 12 :21:24 2021 ; exit: 0 0 0 0 11634 .592_TTbar_14TeV+2021_Patatrack_GPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST Step0-PASSED Step1-PASSED Step2-PASSED Step3-PASSED - time date Sat Sep 18 12 :26:12 2021 -date Sat Sep 18 12 :21:25 2021 ; exit: 0 0 0 0 11634 .595_TTbar_14TeV+2021_Patatrack_TripletsCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST Step0-PASSED Step1-PASSED Step2-PASSED Step3-PASSED - time date Sat Sep 18 12 :26:15 2021 -date Sat Sep 18 12 :21:25 2021 ; exit: 0 0 0 0 11634 .596_TTbar_14TeV+2021_Patatrack_TripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST Step0-PASSED Step1-PASSED Step2-PASSED Step3-PASSED - time date Sat Sep 18 12 :26:10 2021 -date Sat Sep 18 12 :21:26 2021 ; exit: 0 0 0 0 4 4 4 4 tests passed, 0 0 0 0 failed","title":"3. Update GPU workflows #35331"},{"location":"getting-started/validation/#4-add-workflows-for-profiling-the-gpu-code-35540","text":"Merged cmsbuild merged 2 commits into cms-sw:master from fwyzard:add_GPU_profiling_workflows on Oct 8, 2021 fwyzard commented on Oct 5, 2021 PR description: Add four workflows for profiling the GPU code: .504 Pixel-only local reconstruction and quadruplets .508 Pixel-only local reconstruction and triplets .514 ECAL-only local reconstruction .524 ECAL-only local reconstruction The workflows explicitly consume the GPU products, so they can only run on a GPU-equipped machine. The transfer to the host and the conversion to the legacy format is not run. PR validation: Used to profile the various workflows on top of CMSSW_12_1_0_pre3, runing over that release's TTbar relvals with pileup: measurement CMSSW_12_1_0_pre3 I/O throughput ~ 2 kev/s 11634.504 1071 \u00b1 3 ev/s 11634.508 560 \u00b1 2 ev/s 11634.514 1391 \u00b1 5 ev/s 11634.524 1354 \u00b1 10 ev/s","title":"4. Add workflows for profiling the GPU code #35540"},{"location":"getting-started/validation/#fixes","text":"","title":"Fixes"},{"location":"getting-started/validation/#1-fix-the-patatrack-pixel-local-reconstruction-running-on-cpu-35915","text":"Merged cmsbuild merged 1 commit into cms-sw:master from fwyzard:fix_hltSiPixelRecHitSoA_121x on Nov 1, 2021 fwyzard commented on Oct 29, 2021 PR description: Use the hltSiPixelRecHitSoA producer for the pixel rechits in legacy and SoA format, instead of running the legacy producer. PR validation: Successfully run the GPU workflow 11634.506.","title":"1. Fix the Patatrack pixel local reconstruction running on CPU #35915"},{"location":"getting-started/validation/#current-status","text":"","title":"Current status"},{"location":"getting-started/validation/#runthematrixpy","text":"List available workflows for gpu: runTheMatrix.py -n --what gpu Example output: ignoring non-requested file relval_standard ignoring non-requested file relval_highstats ignoring non-requested file relval_pileup ignoring non-requested file relval_generator ignoring non-requested file relval_extendedgen ignoring non-requested file relval_production ignoring non-requested file relval_ged ignoring non-requested file relval_upgrade ignoring non-requested file relval_cleanedupgrade processing relval_gpu ignoring non-requested file relval_2017 ignoring non-requested file relval_2026 ignoring non-requested file relval_identity ignoring non-requested file relval_machine ignoring non-requested file relval_premix found a total of 22 workflows: 136 .885502 RunHLTPhy2018D+HLTDR2_2018+RECODR2_2018reHLT_Patatrack_PixelOnlyGPU+HARVEST2018_pixelTrackingOnly 136 .885512 RunHLTPhy2018D+HLTDR2_2018+RECODR2_2018reHLT_ECALOnlyGPU+HARVEST2018_ECALOnly 136 .885522 RunHLTPhy2018D+HLTDR2_2018+RECODR2_2018reHLT_HCALOnlyGPU+HARVEST2018_HCALOnly 136 .888502 RunJetHT2018D+HLTDR2_2018+RECODR2_2018reHLT_Patatrack_PixelOnlyGPU+HARVEST2018_pixelTrackingOnly 136 .888512 RunJetHT2018D+HLTDR2_2018+RECODR2_2018reHLT_ECALOnlyGPU+HARVEST2018_ECALOnly 136 .888522 RunJetHT2018D+HLTDR2_2018+RECODR2_2018reHLT_HCALOnlyGPU+HARVEST2018_HCALOnly 10824 .502 2018_Patatrack_PixelOnlyGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10824 .506 2018_Patatrack_PixelOnlyTripletsGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10824 .512 2018_Patatrack_ECALOnlyGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10824 .522 2018_Patatrack_HCALOnlyGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10824 .592 2018_Patatrack_GPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10824 .596 2018_Patatrack_TripletsGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10842 .502 2018_Patatrack_PixelOnlyGPU+ZMM_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 10842 .506 2018_Patatrack_PixelOnlyTripletsGPU+ZMM_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT 11634 .502 2021_Patatrack_PixelOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .506 2021_Patatrack_PixelOnlyTripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .512 2021_Patatrack_ECALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .522 2021_Patatrack_HCALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .592 2021_Patatrack_GPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .596 2021_Patatrack_TripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11650 .502 2021_Patatrack_PixelOnlyGPU+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11650 .506 2021_Patatrack_PixelOnlyTripletsGPU+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 22 workflows with 4 steps -------------------------------------------------------------------------------- Some of the workflows mentioned here, for example the profiling ones One can try finding these workflows by: runTheMatrix.py -n --what upgrade | grep Patatrack An example output may be: ... 11442 .595 2018DesignPU_Patatrack_TripletsCPU+ZMM_13TeV_TuneCUETP8M1_GenSim+DigiPU+RecoFakeHLTPU+HARVESTFakeHLTPU 11442 .596 2018DesignPU_Patatrack_TripletsGPU+ZMM_13TeV_TuneCUETP8M1_GenSim+DigiPU+RecoFakeHLTPU+HARVESTFakeHLTPU 11634 .501 2021_Patatrack_PixelOnlyCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .502 2021_Patatrack_PixelOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .504 2021_Patatrack_PixelOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco 11634 .505 2021_Patatrack_PixelOnlyTripletsCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .506 2021_Patatrack_PixelOnlyTripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .508 2021_Patatrack_PixelOnlyTripletsGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco 11634 .511 2021_Patatrack_ECALOnlyCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .512 2021_Patatrack_ECALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .514 2021_Patatrack_ECALOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco 11634 .521 2021_Patatrack_HCALOnlyCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .522 2021_Patatrack_HCALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .524 2021_Patatrack_HCALOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco 11634 .591 2021_Patatrack_CPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .592 2021_Patatrack_GPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .595 2021_Patatrack_TripletsCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11634 .596 2021_Patatrack_TripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11650 .501 2021_Patatrack_PixelOnlyCPU+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11650 .502 2021_Patatrack_PixelOnlyGPU+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST 11650 .504 2021_Patatrack_PixelOnlyGPU_Profiling+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco ...","title":"runTheMatrix.py"},{"location":"rand/hm/","text":"","title":"Hm"},{"location":"workflows/profiling/","text":"Profiling Prerequisites Check activity on current GPU node: foo@bar~$ nvidia-smi Thu Oct 21 17:54:04 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:5E:00.0 Off | 0 | | N/A 41C P0 26W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Prepare uncompressed ROOT files It is not evident that reconstruction runs faster on uncompressed input files, it depends on whether our application is computation or memory bound.","title":"Profiling"},{"location":"workflows/profiling/#profiling","text":"","title":"Profiling"},{"location":"workflows/profiling/#prerequisites","text":"Check activity on current GPU node: foo@bar~$ nvidia-smi Thu Oct 21 17:54:04 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:5E:00.0 Off | 0 | | N/A 41C P0 26W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+","title":"Prerequisites"},{"location":"workflows/profiling/#prepare-uncompressed-root-files","text":"It is not evident that reconstruction runs faster on uncompressed input files, it depends on whether our application is computation or memory bound.","title":"Prepare uncompressed ROOT files"}]}