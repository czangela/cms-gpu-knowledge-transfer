{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CMS Tracker DPG Knowledge Transfer Material","title":"Home"},{"location":"#cms-tracker-dpg-knowledge-transfer-material","text":"","title":"CMS Tracker DPG Knowledge Transfer Material"},{"location":"daily/day01-02/","text":"Week 01 - 2021.12.06-10. Overview What is the CUDA programming model? What are some key language extensions? How to launch kernels? Resources based on Introduction to parallel programming and CUDA by Felice Pantaleo and https://nyu-cds.github.io/python-gpu/02-cuda/ Introduction This section is based on https://nyu-cds.github.io/python-gpu/02-cuda/ . In November 2006, NVIDIA introduced CUDA, which originally stood for \u201cCompute Unified Device Architecture\u201d, a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU. The CUDA parallel programming model has three key abstractions at its core: a hierarchy of thread groups shared memories barrier synchronization There are exposed to the programmer as a minimal set of language extensions. In parallel programming, granularity means the amount of computation in relation to communication (or transfer) of data. Fine-grained parallelism means individual tasks are relatively small in terms of code size and execution time. The data is transferred among processors frequently in amounts of one or a few memory words. Coarse-grained is the opposite in that data is communicated infrequently, after larger amounts of computation. The CUDA abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block. Performance measurement Memory throughput Processing throughput Memory throughput is also referred to as memory bandwidth. To calculate the memory bandwidth we need to understand two concepts related to it. Memory interface width: It is the physical bit-width of the memory bus. Every clock-cycle data is transferred along a memory bus to and from the on-card memory. The memory interface width is the physical count of the bits of how many can fit down the bus per clock cycle. Memory clock rate: (or memory clock cycle) Measured in the unit of Hz it is the rate of memory bus clock. For example a memory bus operating on a 1GHz rate is capable of issuing memory tranfers with the bus 10^9 times per second. Talking about clock rate it is important to take a detour to SDR , DDR and QDR . Example A simple analogy is of course public transport, where memory interface width would be the number of seats on a bus and the memory clock rate is the frequency of buses in unit time (hour let's say). Multiplying these two numbers we can calculate how many people can be transported along one bus line in a certain time period. SPMD Single Program Multiple Data Language extensions From CUDA Toolkit Documentation: Language Extensions : __global__ : The global execution space specifier declares a function as being a kernel. Such a function is: Executed on the device, Callable from the host, Callable from the device for devices of compute capability 3.2 or higher (see CUDA Dynamic Parallelism for more details). A global function must have void return type, and cannot be a member of a class. Any call to a global function must specify its execution configuration as described in Execution . A call to a global function is asynchronous, meaning it returns before the device has completed its execution. __device__ : The device execution space specifier declares a function that is: Executed on the device, Callable from the device only. The global and device execution space specifiers cannot be used together. __host__ : The host execution space specifier declares a function that is: Executed on the host, Callable from the host only. It is equivalent to declare a function with only the host execution space specifier or to declare it without any of the host , device , or global execution space specifier; in either case the function is compiled for the host only. The global and host execution space specifiers cannot be used together. The device and host execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. Question Give an example of global , device and host-device functions in CMSSW . Can you find an example where host and device code diverge? How is this achieved? Answer Execution Configuration From CUDA Toolkit Documentation: Execution Configuration Any call to a global function must specify the execution configuration for that call. The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams). The execution configuration is specified by inserting an expression of the form <<< Dg, Db, Ns, S >>> between the function name and the parenthesized argument list, where: Dg is of type dim3 (see dim3) and specifies the dimension and size of the grid, such that Dg.x * Dg.y * Dg.z equals the number of blocks being launched; Db is of type dim3 (see dim3) and specifies the dimension and size of each block, such that Db.x * Db.y * Db.z equals the number of threads per block; Ns is of type size_t and specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory; this dynamically allocated memory is used by any of the variables declared as an external array as mentioned in shared ; Ns is an optional argument which defaults to 0; S is of type cudaStream_t and specifies the associated stream; S is an optional argument which defaults to 0. Other resources https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html https://towardsdatascience.com/the-ai-illustrated-guide-why-are-gpus-so-powerful-99f4ae85a5c3 https://streamhpc.com/blog/2016-07-19/performance-can-be-measured-as-throughput-latency-or-processor-utilisation/ https://www.gamersnexus.net/dictionary/3-memory-interface","title":"Week 01 - 2021.12.06-10."},{"location":"daily/day01-02/#week-01-20211206-10","text":"Overview What is the CUDA programming model? What are some key language extensions? How to launch kernels? Resources based on Introduction to parallel programming and CUDA by Felice Pantaleo and https://nyu-cds.github.io/python-gpu/02-cuda/","title":"Week 01 - 2021.12.06-10."},{"location":"daily/day01-02/#introduction","text":"This section is based on https://nyu-cds.github.io/python-gpu/02-cuda/ . In November 2006, NVIDIA introduced CUDA, which originally stood for \u201cCompute Unified Device Architecture\u201d, a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU. The CUDA parallel programming model has three key abstractions at its core: a hierarchy of thread groups shared memories barrier synchronization There are exposed to the programmer as a minimal set of language extensions. In parallel programming, granularity means the amount of computation in relation to communication (or transfer) of data. Fine-grained parallelism means individual tasks are relatively small in terms of code size and execution time. The data is transferred among processors frequently in amounts of one or a few memory words. Coarse-grained is the opposite in that data is communicated infrequently, after larger amounts of computation. The CUDA abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block.","title":"Introduction"},{"location":"daily/day01-02/#performance-measurement","text":"Memory throughput Processing throughput Memory throughput is also referred to as memory bandwidth. To calculate the memory bandwidth we need to understand two concepts related to it. Memory interface width: It is the physical bit-width of the memory bus. Every clock-cycle data is transferred along a memory bus to and from the on-card memory. The memory interface width is the physical count of the bits of how many can fit down the bus per clock cycle. Memory clock rate: (or memory clock cycle) Measured in the unit of Hz it is the rate of memory bus clock. For example a memory bus operating on a 1GHz rate is capable of issuing memory tranfers with the bus 10^9 times per second. Talking about clock rate it is important to take a detour to SDR , DDR and QDR . Example A simple analogy is of course public transport, where memory interface width would be the number of seats on a bus and the memory clock rate is the frequency of buses in unit time (hour let's say). Multiplying these two numbers we can calculate how many people can be transported along one bus line in a certain time period.","title":"Performance measurement"},{"location":"daily/day01-02/#spmd","text":"Single Program Multiple Data","title":"SPMD"},{"location":"daily/day01-02/#language-extensions","text":"From CUDA Toolkit Documentation: Language Extensions :","title":"Language extensions"},{"location":"daily/day01-02/#__global__","text":"The global execution space specifier declares a function as being a kernel. Such a function is: Executed on the device, Callable from the host, Callable from the device for devices of compute capability 3.2 or higher (see CUDA Dynamic Parallelism for more details). A global function must have void return type, and cannot be a member of a class. Any call to a global function must specify its execution configuration as described in Execution . A call to a global function is asynchronous, meaning it returns before the device has completed its execution.","title":"__global__ :"},{"location":"daily/day01-02/#__device__","text":"The device execution space specifier declares a function that is: Executed on the device, Callable from the device only. The global and device execution space specifiers cannot be used together.","title":"__device__ :"},{"location":"daily/day01-02/#__host__","text":"The host execution space specifier declares a function that is: Executed on the host, Callable from the host only. It is equivalent to declare a function with only the host execution space specifier or to declare it without any of the host , device , or global execution space specifier; in either case the function is compiled for the host only. The global and host execution space specifiers cannot be used together. The device and host execution space specifiers can be used together however, in which case the function is compiled for both the host and the device. Question Give an example of global , device and host-device functions in CMSSW . Can you find an example where host and device code diverge? How is this achieved? Answer","title":"__host__ :"},{"location":"daily/day01-02/#execution-configuration","text":"From CUDA Toolkit Documentation: Execution Configuration Any call to a global function must specify the execution configuration for that call. The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams). The execution configuration is specified by inserting an expression of the form <<< Dg, Db, Ns, S >>> between the function name and the parenthesized argument list, where: Dg is of type dim3 (see dim3) and specifies the dimension and size of the grid, such that Dg.x * Dg.y * Dg.z equals the number of blocks being launched; Db is of type dim3 (see dim3) and specifies the dimension and size of each block, such that Db.x * Db.y * Db.z equals the number of threads per block; Ns is of type size_t and specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory; this dynamically allocated memory is used by any of the variables declared as an external array as mentioned in shared ; Ns is an optional argument which defaults to 0; S is of type cudaStream_t and specifies the associated stream; S is an optional argument which defaults to 0. Other resources https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html https://towardsdatascience.com/the-ai-illustrated-guide-why-are-gpus-so-powerful-99f4ae85a5c3 https://streamhpc.com/blog/2016-07-19/performance-can-be-measured-as-throughput-latency-or-processor-utilisation/ https://www.gamersnexus.net/dictionary/3-memory-interface","title":"Execution Configuration"},{"location":"daily/examples/","text":"Practice Example Unordered List Ordered List Example : * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Example : 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Practice"},{"location":"daily/examples/#practice","text":"Example Unordered List Ordered List Example : * Sed sagittis eleifend rutrum * Donec vitae suscipit est * Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Example : 1. Sed sagittis eleifend rutrum 2. Donec vitae suscipit est 3. Nulla tempor lobortis orci Result : Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Question Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Practice"},{"location":"getting-started/","text":"Access to machines A step-by-step guide on how to access GPU equipped machines at CERN, CMS or how to develop on your machine. Prerequisites CERN computing account Access machines at CERN See the CERN cloud insfrastructure resources guide on how to request GPU resources. lxplus The lxplus service offers lxplus-gpu.cern.ch for shared GPU instances - with limited isolation and performance. One can connect similary as would do to the lxplus.cern.ch host domain. ssh <username>@lxplus-gpu.cern.ch [-X] Access machines at CMS P5 This section is taken from the CMS TWiki TriggerDevelopmentWithGPUs page. Dedicated machines for the development of the online reconstruction There are 6 machines available for general development and validation of the online reconstruction on GPUs: gpu-c2a02-37-03.cms gpu-c2a02-37-04.cms gpu-c2a02-39-01.cms gpu-c2a02-39-02.cms gpu-c2a02-39-03.cms gpu-c2a02-39-04.cms All machines are equipped with two Intel \"Skylake\" Xeon Gold 6130 processors (for a total of 2x16=32 physical cores and 2x2x16 = 64 logical cores or hardware threads); 96 GB of RAM; one NVIDIA Tesla T4 GPU. How to connect To connect to these machines you need to have an online account and be in the gpudev group. To request access, please subscribe to the cms-hlt-gpu@cern.ch e-group and send an email to andrea.bocci@cern.ch , indicating whether you already have an online account; your online or lxplus username; your full name and email. Miscellaneous - or special GPU nodes This section is more or less taken from the Patatrack website systems subpage. cmg-gpu1080 System information Topology of the machine Getting access to the machine In order to get access to the machine you should send a request to subscribe to the CERN e-group: cms-gpu You should also send an email to Felice Pantaleo motivating the reason for the requested access. Usage Policy Normally, no more than 1 GPU per users should be used. To limit visible devices use export CUDA_VISIBLE_DEVICES=<list of numbers> Where <list of numbers> can be e.g. 0 , 0,4 , 1,2,3 . Use nvidia-smi to check available resources. Usage for ML studies If you need to use the machine for training DNNs you could accidentally occupy all the GPUs, making them unavailable for other users. For this reason you're kindly asked to use import setGPU before any import that will use a GPU (e.g. tensorflow). This will assign to you the least loaded GPU on the system. It is strictly forbidden to use GPUs from within your jupyter notebook. Please export your notebook to a python program and execute it. The access to the machine will be revoked when failing to comply to this rule.","title":"Access to machines"},{"location":"getting-started/#access-to-machines","text":"A step-by-step guide on how to access GPU equipped machines at CERN, CMS or how to develop on your machine.","title":"Access to machines"},{"location":"getting-started/#prerequisites","text":"CERN computing account","title":"Prerequisites"},{"location":"getting-started/#access-machines-at-cern","text":"See the CERN cloud insfrastructure resources guide on how to request GPU resources. lxplus The lxplus service offers lxplus-gpu.cern.ch for shared GPU instances - with limited isolation and performance. One can connect similary as would do to the lxplus.cern.ch host domain. ssh <username>@lxplus-gpu.cern.ch [-X]","title":"Access machines at CERN"},{"location":"getting-started/#access-machines-at-cms-p5","text":"This section is taken from the CMS TWiki TriggerDevelopmentWithGPUs page.","title":"Access machines at CMS P5"},{"location":"getting-started/#dedicated-machines-for-the-development-of-the-online-reconstruction","text":"There are 6 machines available for general development and validation of the online reconstruction on GPUs: gpu-c2a02-37-03.cms gpu-c2a02-37-04.cms gpu-c2a02-39-01.cms gpu-c2a02-39-02.cms gpu-c2a02-39-03.cms gpu-c2a02-39-04.cms","title":"Dedicated machines for the development of the online reconstruction"},{"location":"getting-started/#all-machines-are-equipped-with","text":"two Intel \"Skylake\" Xeon Gold 6130 processors (for a total of 2x16=32 physical cores and 2x2x16 = 64 logical cores or hardware threads); 96 GB of RAM; one NVIDIA Tesla T4 GPU.","title":"All machines are equipped with"},{"location":"getting-started/#how-to-connect","text":"To connect to these machines you need to have an online account and be in the gpudev group. To request access, please subscribe to the cms-hlt-gpu@cern.ch e-group and send an email to andrea.bocci@cern.ch , indicating whether you already have an online account; your online or lxplus username; your full name and email.","title":"How to connect"},{"location":"getting-started/#miscellaneous-or-special-gpu-nodes","text":"This section is more or less taken from the Patatrack website systems subpage.","title":"Miscellaneous - or special GPU nodes"},{"location":"getting-started/#cmg-gpu1080","text":"","title":"cmg-gpu1080"},{"location":"getting-started/#system-information","text":"Topology of the machine","title":"System information"},{"location":"getting-started/#getting-access-to-the-machine","text":"In order to get access to the machine you should send a request to subscribe to the CERN e-group: cms-gpu You should also send an email to Felice Pantaleo motivating the reason for the requested access.","title":"Getting access to the machine"},{"location":"getting-started/#usage-policy","text":"Normally, no more than 1 GPU per users should be used. To limit visible devices use export CUDA_VISIBLE_DEVICES=<list of numbers> Where <list of numbers> can be e.g. 0 , 0,4 , 1,2,3 . Use nvidia-smi to check available resources.","title":"Usage Policy"},{"location":"getting-started/#usage-for-ml-studies","text":"If you need to use the machine for training DNNs you could accidentally occupy all the GPUs, making them unavailable for other users. For this reason you're kindly asked to use import setGPU before any import that will use a GPU (e.g. tensorflow). This will assign to you the least loaded GPU on the system. It is strictly forbidden to use GPUs from within your jupyter notebook. Please export your notebook to a python program and execute it. The access to the machine will be revoked when failing to comply to this rule.","title":"Usage for ML studies"},{"location":"workflows/profiling/","text":"Profiling Prerequisites Check activity on current GPU node: foo@bar~$ nvidia-smi Thu Oct 21 17:54:04 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:5E:00.0 Off | 0 | | N/A 41C P0 26W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Prepare uncompressed ROOT files It is not evident that reconstruction runs faster on uncompressed input files, it depends on whether our application is computation or memory bound.","title":"Profiling"},{"location":"workflows/profiling/#profiling","text":"","title":"Profiling"},{"location":"workflows/profiling/#prerequisites","text":"Check activity on current GPU node: foo@bar~$ nvidia-smi Thu Oct 21 17:54:04 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 Off | 00000000:5E:00.0 Off | 0 | | N/A 41C P0 26W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+","title":"Prerequisites"},{"location":"workflows/profiling/#prepare-uncompressed-root-files","text":"It is not evident that reconstruction runs faster on uncompressed input files, it depends on whether our application is computation or memory bound.","title":"Prepare uncompressed ROOT files"}]}